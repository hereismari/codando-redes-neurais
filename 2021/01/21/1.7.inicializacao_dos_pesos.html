<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>1.7 Inicialização dos Pesos | Codando Deep Learning</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="1.7 Inicialização dos Pesos" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Curso de Deep Learning para programdoras." />
<meta property="og:description" content="Curso de Deep Learning para programdoras." />
<link rel="canonical" href="https://hereismari.github.io/codando-deep-learning-jax/2021/01/21/1.7.inicializacao_dos_pesos.html" />
<meta property="og:url" content="https://hereismari.github.io/codando-deep-learning-jax/2021/01/21/1.7.inicializacao_dos_pesos.html" />
<meta property="og:site_name" content="Codando Deep Learning" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-21T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://hereismari.github.io/codando-deep-learning-jax/2021/01/21/1.7.inicializacao_dos_pesos.html","@type":"BlogPosting","headline":"1.7 Inicialização dos Pesos","dateModified":"2021-01-21T00:00:00-06:00","datePublished":"2021-01-21T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://hereismari.github.io/codando-deep-learning-jax/2021/01/21/1.7.inicializacao_dos_pesos.html"},"description":"Curso de Deep Learning para programdoras.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/codando-deep-learning-jax/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://hereismari.github.io/codando-deep-learning-jax/feed.xml" title="Codando Deep Learning" /><link rel="shortcut icon" type="image/x-icon" href="/codando-deep-learning-jax/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/codando-deep-learning-jax/">Codando Deep Learning</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/codando-deep-learning-jax/about/">About Me</a><a class="page-link" href="/codando-deep-learning-jax/search/">Search</a><a class="page-link" href="/codando-deep-learning-jax/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">1.7 Inicialização dos Pesos</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-01-21T00:00:00-06:00" itemprop="datePublished">
        Jan 21, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/hereismari/codando-deep-learning-jax/tree/master/_notebooks/2021-01-21-1.7.inicializacao_dos_pesos.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/codando-deep-learning-jax/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/hereismari/codando-deep-learning-jax/master?filepath=_notebooks%2F2021-01-21-1.7.inicializacao_dos_pesos.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/codando-deep-learning-jax/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/hereismari/codando-deep-learning-jax/blob/master/_notebooks/2021-01-21-1.7.inicializacao_dos_pesos.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/codando-deep-learning-jax/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-01-21-1.7.inicializacao_dos_pesos.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Bem-vind@s-ao-mundo-estoc&#225;stico!">Bem vind@s ao mundo estoc&#225;stico!<a class="anchor-link" href="#Bem-vind@s-ao-mundo-estoc&#225;stico!"> </a></h2><p>Vimos que redes neurais são um formadas por um conjunto de parâmetros chamados de <strong>pesos</strong> e <strong>bias</strong>.</p>
<blockquote><p>Não confunda <strong>parâmetros</strong> com <strong>hiperparâmetros</strong>. Parâmetros são os valores que sua rede aprende (pesos e bias). Hiperparâmetros são os valores relacionados ao treinamento da sua rede:<em>learning rate</em>, número de camadas/neurônios, função de ativação de cada camada, etc...
Nós também vimos que os pesos e bias são, em geral, inicializados com valores aleatórios. Por conta disso, é muito díficil que duas redes distintas chegem no mesmo resultado. Além disso, nós nunca teremos certeza que tal resultado é o melhor possível (mínimo global) e, dependendo dos valores iniciais, uma rede pode nunca convergir.</p>
</blockquote>
<p>Claro que, para evitar alguns desses problemas, nós podemos mexer nos hiperparâmetros como <em>learning rate</em>, o otimizador, adicionar momentum, entre outros. Se você não conhece sobre eles ainda, não se preocupe! Nós vamos falar sobre eles mais a frente nesse curso. Outra alternativa é inicializar os pesos de diferentes maneiras. E aí, vamos aprender sobre elas?</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Inicializa&#231;&#227;o-com-Zeros">Inicializa&#231;&#227;o com Zeros<a class="anchor-link" href="#Inicializa&#231;&#227;o-com-Zeros"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote><p>"Eu prefiro lutar em 0 graus, pois não é nem quente, nem frio!" <br /> Maguila (ex-lutador de boxe)</p>
</blockquote>
<p>Baseado nesse pensamento, muitas pessoas já pensaram em inicializar os pesos com zero. Dessa forma, todos os pesos começarão com valores iguais, nenhum sendo mais importante que o outro, nem dando mais importância para um determinado atributo dos seus dados. Além disso, é muito simples fazer isso com numpy:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">pesos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">pesos</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.]])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Parece uma boa ideia, né? <font color="red">**Errado!**</font> <strong>Essa é pior coisa que se pode fazer ao inicializar os pesos de uma rede neural!</strong> Parando para pensar um pouco melhor, fazendo isso nós vamos zerar todos os neurônios bem como os gradientes de aprendizagem. <strong>Se os gradientes são zeros, como a rede vai aprender alguma coisa, então?</strong> 
Vale salientar que seus gradientes podem tender a zero por outras razões (que nós veremos depois). Mas, quando isso acontece, chamamos esse fenômeno de <strong>desaparecimento dos gradientes</strong> (<em>gradient vanishing</em>).</p>
<p>Sem contar que você será mal visto pela sua família e amigos se inicializar seus pesos com zeros. Em outras palavras, não pense como o Maguila e não faça isso nunca!</p>
<p>Por outro lado é uma inicialização bem comum para os biases</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Inicializa&#231;&#227;o-com-1s">Inicializa&#231;&#227;o com 1s<a class="anchor-link" href="#Inicializa&#231;&#227;o-com-1s"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Nesse momento, você deve estar pensando: "Tá bom, tá bom. Eu já entendi que inicializar todos os pesos com zero é ruim. Mas, e se inicializamos todos os pesos com 1s, então?"</p>
<p>A resposta é: <strong>é tão ruim quanto!</strong> Pensando um pouquinho, lembra que na primeira camada todos os neurônios recebem as mesmas entradas? Então, se todos os neurônios receberam as mesmas entradas e todos eles tem o mesmo valor de peso (=1), o que que vai acontecer? Isso mesmo: <strong>todos os neurônios vão dar o mesmo resultado!</strong></p>
<p>Ainda nessa linha de pensamento, como todos os neurônios vão da próxima camada vão receber o resultado de cada neurônio dessa camada (que são todos iguais), a mesma coisa vai se repetir. No fim das contas, você pode até ter um pouquinho de gradiente, mas os gradientes para os neurônios de uma mesma camada vão ser iguais. Assim, todos os seus pesos se manterão iguais para sempre! Nada adianta ter uma camada com milhões de pesos se todos eles são idênticos.</p>
<blockquote><p>Resumindo até aqui:<strong>não inicialize os seus neurônios com o mesmo valor, pior ainda se esse valor for igual a zero!</strong>
(Novamente vale ressaltar que inicializar os viéses dessa maneira não é um problema)</p>
</blockquote>
<p>Se você entendeu isso, deve ter percebido que a melhor maneira de inicializar pesos é dando valores aleatórios para cada um deles. A pergunta agora é: será que é melhor uma variância baixa, alta? Será que a média deve ser zero? É melhor uma distribuição normal ou uniforme?</p>
<p>É isso que vamos ver nas próximas seções.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Inicializa&#231;&#227;o-aleat&#243;ria-uniforme">Inicializa&#231;&#227;o aleat&#243;ria uniforme<a class="anchor-link" href="#Inicializa&#231;&#227;o-aleat&#243;ria-uniforme"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Inicializar os pesos aleatórios de forma uniforme (mesma probabilidade para todo mundo) é fácil com numpy:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">pesos_uniforme</span> <span class="o">=</span> <span class="mi">10</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span> <span class="o">-</span> <span class="mi">5</span> <span class="c1"># pesos entre -5 e 5</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">pesos_uniforme</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Repare que como os valores dos pesos têm praticamente uma mesma probabilidade de ocorrência (distribuição uniforme). <strong>Pesos como essa distribuição são bons para quebrar a simetria da rede e vão fazer sua rede aprender alguma coisa</strong>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Inicializa&#231;&#227;o-aleat&#243;ria-normal">Inicializa&#231;&#227;o aleat&#243;ria normal<a class="anchor-link" href="#Inicializa&#231;&#227;o-aleat&#243;ria-normal"> </a></h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pesos_normal</span> <span class="o">=</span> <span class="mi">5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span> <span class="o">+</span> <span class="mi">3</span> <span class="c1"># média = 3 e desvio-padrão = 5</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">pesos_normal</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Diferente da distribuição uniforme, os pesos agora são inicializados por uma distribuição normal, isto é, com certa média e desvio padrão. Ou seja, pesos mais próximos da média são mais comuns e quanto mais "desvios-padrões" além da média, menor a probabilidade. Da mesma forma que a distribuição uniforme, <strong>pesos com essa distribuição também são bons para quebrar a simetria da rede</strong>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Inicializa&#231;&#227;o-Glorot-Uniforme">Inicializa&#231;&#227;o Glorot Uniforme<a class="anchor-link" href="#Inicializa&#231;&#227;o-Glorot-Uniforme"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A gente viu que inicilizar os pesos com uma distribuição normal ou uniforme é uma boa ideia. Entretanto, elas apresentam um defeito: <strong>como definir a faixa de valores que meus pesos vão assumir?</strong> Em outras palavras, qual deve ser a variância dos meus pesos? Quanto menor sua variância, maiores são as chances de acontecer o <em>vanishing</em> dos gradientes - e a gente já viu que isso não é bom. Por outro lado, quando a variância é muito grande, o contrário acontece. Isto é, seus gradientes vão tender ao infinito. Nesse caso, damos o nome de <strong>explosão dos gradientes</strong> (<em>exploding gradients</em>). Portanto, tenha em mente o seguinte:</p>
<blockquote><p>O que buscamos na inicialização de pesos são <strong>valores pequenos com variância ok</strong> (pequena, mas nem tanto)</p>
</blockquote>
<p>Porém, não existe um número mágico para variância dos pesos. Ela vai depender principalmente da quantidade de pesos que a sua rede tem, ou seja, depende da arquitetura da sua rede. O ideal seria que a variância dos pesos fosse definida automaticamente. Ahhh, ia ser tão bom se existisse um método que fizesse isso para gente, né? E existe!!!</p>
<p>A <strong>Glorot Uniforme</strong>, também conhecida como <strong>Xavier Uniforme</strong>, é uma distribuição uniforme que leva em consideração a quantidade de neurônios da camada atual e anterior da sua rede. Na prática, os pesos são inicializados baseados na seguinte fórmula:$$w = 2*\sigma*rand() - \sigma$$
onde,</p>
<p>
$$\sigma = \sqrt{\frac{6}{in + out}}$$
</p>
<p>Supondo que a camada atual tem 5 neurônios e a camada anterior tinha 3, a implementação em Python ficaria dessa forma:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span> <span class="c1"># para que o seu resultado seja igual ao meu :)</span>

<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">6</span> <span class="o">/</span> <span class="p">(</span><span class="mi">5</span> <span class="o">+</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">pesos</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="o">-</span> <span class="n">sigma</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pesos</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[[-0.21730289  0.78066008  0.40182529]
 [ 0.17088151 -0.59579319 -0.59583497]
 [-0.76542164  0.63423569  0.17513634]
 [ 0.36039228 -0.83037201  0.81390774]
 [ 0.57580754 -0.49824328 -0.55109532]]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Inicializa&#231;&#227;o-Glorot-Normal">Inicializa&#231;&#227;o Glorot Normal<a class="anchor-link" href="#Inicializa&#231;&#227;o-Glorot-Normal"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A <strong>Glorot Normal</strong>, também conhecida como <strong>Xavier Normal</strong>, é bem parecida com a sua irmã uniforme. A diferença, na verdade, é que agora vamos fazer a amostragem dos pesos baseado numa distribuição normal <del>(não diga!)</del>, também levando em consideração a quantidade de neurônios da camada atual e anterior:</p>
<p>
$$w = \sigma * randN()$$
</p>
<p>também teremos uma pequena diferençazinha no $\sigma$:</p>
<p>
$$\sigma = \sqrt{\frac{2}{in + out}}$$
</p>
<p>Em Python, considerando a mesma rede com 5 e 3 neurônios:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span> <span class="c1"># mesma semente do exemplo anterior para gente comparar os pesos</span>

<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">5</span> <span class="o">+</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">pesos</span> <span class="o">=</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pesos</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[[ 0.24835708 -0.06913215  0.32384427]
 [ 0.76151493 -0.11707669 -0.11706848]
 [ 0.78960641  0.38371736 -0.23473719]
 [ 0.27128002 -0.23170885 -0.23286488]
 [ 0.12098114 -0.95664012 -0.86245892]]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Pronto! Agora você sabe tudo sobre inicialização de pesos em redes neurais. Provavelmente, a sua única dúvida na cabeça agora é: <strong>da onde vem o $2$ e $6$ nas fórmulas do $\sigma$ das distribuições Glorot?</strong> No <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">artigo original</a>, os autores assumem uma função de ativação simétrica (na época era comum usar sigmóides e tangentes hiperbólicas) e concluem que a variância ótima para a geração de saídas é $1/in$, enquanto a ideal para a backpropagation é $1/out$, como um compromisso eles tomam a variância como sendo o inverso da média $2/(in + out)$. 
O 6 basicamente vem do fato que a variância de uma distribuição uniforme é $(max - min)^2/12$, como a média é 0, min = -max, $\frac{4max^2}{12} = 2/(in + out) \implies max = \sqrt(\frac{6}{in + out})$</p>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/codando-deep-learning-jax/2021/01/21/1.7.inicializacao_dos_pesos.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/codando-deep-learning-jax/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/codando-deep-learning-jax/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/codando-deep-learning-jax/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Curso de Deep Learning para programdoras.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
