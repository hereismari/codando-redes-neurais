{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução ao JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[JAX](https://github.com/google/jax) é uma nova biblioteca de Python da Google com foco em pesquisa de alta performance em Aprendizado de Máquina e seguindo o paradigma de programação funcional. \n",
    "\n",
    "Mais especificamente JAX nos dá acesso a:  e , as principais sendo grad, jit, vmap e pmap(que vai ter seu próprio post no futuro).\n",
    "- uma API compatível com numpy e scipy\n",
    "- uma própria API de números aleatório manejados manualmente\n",
    "- transformações de função composicionais (você pode aplicar elas em conjunto sem muito problema)\n",
    "- transformações para trivialmente computar derivadas de funções\n",
    "- transformações para usar aceleradores (CPU, GPU e TPU)\n",
    "- transformações que permitem paralelizar seu código facilmente\n",
    "\n",
    "Para usar o JAX é recomendado checar as instruções de [instalação](https://github.com/google/jax#installation), e a seguir importá-lo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enquanto jax é pequeno, seria meio chato ficar digitando\n",
    "# jax.numpy toda hora, então fazemos um alias dele para jnp\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos ver cada uma dessas features da biblioteca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O Wrapper de Numpy: jax.numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaogui/miniconda3/envs/jax/lib/python3.7/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "# Vamos comparar o numpy e o jax.numpy\n",
    "import numpy as np\n",
    "\n",
    "a = np.array([1., 2., 3.])\n",
    "b = np.array([1., 1., -1.])\n",
    "print(np.dot(a, b), jnp.dot(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse warning que aparece é ele dizendo que sou pobre e não tenho nem GPU nem TPU :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 4., 9.]), DeviceArray([1., 4., 9.], dtype=float32))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.square(a), jnp.square(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que JAX tem seu próprio tipo de array, o DeviceArray, em geral as funções vão transformar arrays de numpy em DeviceArrays, então se você quiser boa performance é melhor fazer essa transformação manualmente antes de passar os dados para várias funções."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Números aleatórios jax.random\n",
    "Uma das partes mais peculiares de JAX, para faciliar implementações usando paralelismo não existe uma semente global para geradores de números aleatórios, em vez disso em JAX você passa explicitamente a seed para cada função que envolve aleatoriedade, e cabe a você atualizá-la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.18471184 -0.18471184\n",
      "True\n",
      "0.13790314 1.3694694\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(42) #cria um semente aleatória\n",
    "a = jax.random.normal(key, ())\n",
    "b = jax.random.normal(key, ())\n",
    "print(a, b)\n",
    "print(a == b) #como usamos a mesma semente para a mesma função temos valores iguais\n",
    "k1, k2 = jax.random.split(key, 2) #vamos criar duas novas seeds a partir da primeira\n",
    "a = jax.random.normal(k1, ())\n",
    "b = jax.random.normal(k2, ())\n",
    "print(a, b) #Agora são diferentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformações\n",
    "O principal diferencial de JAX são suas tranformações de funções, que nos permitem modificar facilmente funções definidas a partir de outras funções do JAX e algumas primitivas de Python. \n",
    "\n",
    "Algo muito útil e legal delas é que podem ser utilizadas em conjunto (são \"composicionais\"), nos permitindo por exemplo compilar a derivada de uma função vetorizada apenas aplicando 3 transformações uma seguida da outra a função original. \n",
    "\n",
    "Porém existem alguns cuidados que dever ser tomados ao se usar esses transformações, para entender esse cuidados melhor cheque esse [link](https://github.com/google/jax#current-gotchas) e abra o notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferenciação Automática: jax.grad\n",
    "Em aprendizado de máquina, principalmente quando estamos tratando de redes neurais, lidamos com muitas derivadas, gradientes e afins: Para treinar uma regressão linear ou logística, precisamos computar um hessiano, para treinar uma rede neural usamos descida de gradiente, que requer o cálculo de um gradiente, dentre outros exemplos. \n",
    "\n",
    "Computar essas derivadas na mão é muitas vezes extremamente trabalhoso, ou até mesmo impossível dado o tempo disponível, assim temos algoritmos como o backpropagation para redes neurais, porém se sempre tivessemos que implementar nós mesmos esse algoritmo, e implementar a derivada de cada uma das funções que vamos usar, terminaríamos com uma quatidade imensa de código duplicado, além de uma imensa chance de errarmos algo na implementação e terminarmos sem conseguir bons resultados ou com resultados que não correspodem a realidade. \n",
    "\n",
    "Para lidar com isso temos diferenciação automática, transformações que recebem uma função e retornam algum tipo de derivada dela. Simplesmente ter diferenciação automática para as funções de Numpy já é o bastante para uma biblioteca mostrar seu valor, tanto que existe uma biblioteca que é exatamente isso, chamada de Autograd, em muitos sentidos JAX é um sucessor dessa biblioteca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "2.0\n",
      "-2.5066283 -2.5066282746310002\n"
     ]
    }
   ],
   "source": [
    "from jax import grad\n",
    "from math import pi, sqrt\n",
    "dup = grad(jnp.square)\n",
    "print(dup(3.0)) #A derivada de x² é 2x\n",
    "print(grad(dup)(3.0)) #Podemos aplicar várias vezes a grad\n",
    "\n",
    "@grad #Podemos usar as transformações como decoradores\n",
    "def composite_func(x):\n",
    "    y = x**2\n",
    "    return jnp.cos(y)\n",
    "# Pela regra da cadeia, dcos(x²)/dx = -2xsen(x²)\n",
    "print(composite_func(jnp.sqrt(pi/2)), -2*sqrt(pi/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para funções com várias variáveis de entrada a grad por padrão nos dá a derivada em função do primeiro parâmetro, mas podemos mudar isso com o argumento argnums. Também vale ressaltar que os argumentos não precisam ser apenas números e podem ser vetores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.0\n",
      "(DeviceArray(16., dtype=float32), DeviceArray(24., dtype=float32))\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def f(x, y):\n",
    "    return x*(y**2)\n",
    "dfdy = grad(f, argnums=(1))\n",
    "print(dfdy(3.0, 4.0))\n",
    "gradient = grad(f, argnums=(0, 1))\n",
    "print(gradient(3.0, 4.0))\n",
    "\n",
    "def g(v):\n",
    "    return jnp.linalg.norm(v)\n",
    "print(grad(g)(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilação com XLA: jit\n",
    "Mas as vantagens de jax não param em diferenciação automática, se não seria apenas um clone do autograd, jax também tem a habilidade de compilar funções usando o XLA (accelerated linear algebra) da Google, tornando-as bem mais rápidas, além de permitir o uso de aceleradores como GPUs e TPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "a = 1 + jax.random.normal(k1, (2024, 2024))\n",
    "b = 1 + jax.random.normal(k2, (2024, 2024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vantagem se torna maior (> 20x mais rápido) quando usamos aceleradores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit #equivalente a definir jcos e escrever jcos = jit(jcos)\n",
    "def jcos(a, b):\n",
    "    return jnp.dot(a, b)/jnp.sqrt(jnp.dot(a, a)*jnp.dot(b, b))\n",
    "def npcos(a, b):\n",
    "    return np.dot(a, b)/np.sqrt(np.dot(a, a)*np.dot(b, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241 ms ± 60.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "npcos(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[1.0336248 , 1.028297  , 1.0310295 , ..., 0.96507126,\n",
       "              1.0339284 , 1.0405782 ],\n",
       "             [0.94755113, 1.0175968 , 0.9993737 , ..., 1.0073111 ,\n",
       "              1.0127649 , 1.0145004 ],\n",
       "             [0.99184996, 1.0617024 , 1.0004048 , ..., 1.0069526 ,\n",
       "              1.0510893 , 1.0679886 ],\n",
       "             ...,\n",
       "             [0.965744  , 1.0246744 , 1.0708025 , ..., 1.0135127 ,\n",
       "              1.0477784 , 0.98690724],\n",
       "             [0.9184314 , 0.99969995, 0.9819234 , ..., 0.9972953 ,\n",
       "              0.9442775 , 0.9897808 ],\n",
       "             [0.99618256, 1.0751995 , 1.0236498 , ..., 1.0285234 ,\n",
       "              1.0351353 , 1.0573723 ]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jcos(a, b) #Rodamos uma vez fora para compilar a função"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 ms ± 4.32 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "jcos(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vetorização Automática: vmap\n",
    "Vmap é uma transformação muito interessante, usando ela é possível vetorizar automaticamente nossas funções, ou seja, em vez de ter que fazer uma função que lida com um batch de dados, podemos fazer uma função que recebe um único dado e depois usar a trasformação para ganhar a versão que lida com o batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  3. -2.]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1., 2., 3.])\n",
    "b = np.array([1., 1., -1.])\n",
    "c = np.array([[1., 2., 3.], [4., 5., 6.]])\n",
    "\n",
    "@jax.vmap #Podemos usar as transformações como decoradores\n",
    "def f(x, y):\n",
    "    return x/y + 1.\n",
    "print(f(a, b))\n",
    "\n",
    "def prod(x, y):\n",
    "    return x@y\n",
    "print(prod(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 3)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    prod(a, c) #a e c não têm dimensões compatíveis\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([14., 32.], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_prod = jax.vmap(prod, in_axes=(None, 0)) #vamos multiplica a por cada linha de c\n",
    "batch_prod(a, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa primeira parte vimos qual o propósito da biblioteca e suas principais funções, nos próximos posts vamos explorar como criar redes neurais com jax, suas bibliotecas experimentais, o ecossistema de bibliotecas escritas usando jax."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
