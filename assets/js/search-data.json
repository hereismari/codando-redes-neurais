{
  
    
        "post0": {
            "title": "Algoritmos de Otimização",
            "content": "No Deep Learning temos como propósito que nossas redes neurais aprendam a aproximar uma função de interesse, como o preço de casas numa regressão, ou a função que classifica objetos numa foto, no caso da classificação. . No último notebook, nós programos nossa primeira rede neural. Além disso, vimos também a fórmula de atualização dos pesos. Se você não lembra, os pesos e os bias foram atualizados da seguinte forma: . $$w_i = w_i - lambda * partial w $$ . $$b_i = b_i - lambda * partial b$$ . Mas, você já parou pra pensar da onde vêm essas fórmulas? Além disso, será que existem melhores formas de atualizar os pesos? É isso que vamos ver nesse notebook. . Descida de Gradiente Estoc&#225;stica (SGD) . Na descida de gradiente estocástica separamos os nossos dados de treino em vários subconjuntos, que chamamos de mini-batches. No começo eles serão pequenos, como 32-128 exemplos, para aplicações mais avançadas eles tendem a ser muito maiores, na ordem de 1024 e até mesmo 8192 exemplos por mini-batch. . Como na descida de gradiente padrão, computamos o gradiente da função de custo em relação aos exemplos, e subtraímos o gradiente vezes uma taxa de apredizado dos parâmetros da rede. Podemos ver o SGD como tomando um passo pequeno na direção de maior redução do valor da loss. . Equa&#231;&#227;o . $w_{t+1} = w_t - eta cdot nabla L$ . C&#243;digo . import jax def sgd(weights, gradients, eta): return jax.tree_util.tree_multimap(lambda w, g: w - eta*g, weights, gradients) . Vamos usar o SGD para otimizar uma função simples . x0 = 9.0 f_g = jax.value_and_grad(f) x_ = [] y_ = [] for i in range(10): y0, grads = f_g(x0) x_.append(x0) y_.append(y0) x0 = sgd(x0, grads, 0.9) plt.plot(x, y) plt.plot(x_, y_, color=&#39;red&#39;, marker=&#39;o&#39;); . WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) . Momentum . Um problema com o uso de mini-batches é que agora estamos estimando a direção que diminui a função de perda no conjunto de treino, e quão menor o mini-batch mais ruidosa é a nossa estimativa. Para consertar esse problema do ruído nós introduzimos a noção de momentum. O momentm faz sua otimização agir como uma bola pesada descendo uma montanha, então mesmo que o caminho seja cheio de montes e buracos a direção da bola não é muito afetada. De um ponto de vista mais matemático as nossas atualizações dos pesos vão ser uma combinação entre os gradientes desse passo e os gradientes anteriores, estabilizando o treino. . Equa&#231;&#227;o . $v_{t} = gamma v_{t-1} + nabla L quad text{o gamma serve como um coeficiente ponderando entre usar os updates anteriores e o novo gradiente} w_{t+1} = w_t - eta v_t $ . C&#243;digo . def momentum(weights, gradients, eta, mom, gamma): mom = jax.tree_util.tree_multimap( lambda v, g: gamma*v + (1 - gamma)*g, weights, gradients) weights = jax.tree_util.tree_multimap( lambda w, v: w - eta*mom, weights, mom) return weights, mom . x0 = 9.0 mom = 0.0 x_ = [] y_ = [] for i in range(10): y0, grads = f_g(x0) x_.append(x0) y_.append(y0) x0, mom = momentum(x0, grads, 0.9, mom, 0.99) plt.plot(x, y) plt.plot(x_, y_, color=&#39;red&#39;, marker=&#39;o&#39;); . RMSProp . Criado por Geoffrey Hinton durante uma aula, esse método é o primeiro método adaptivo que estamos vendo. O que isso quer dizer é que o método tenta automaticamente computar uma taxa de aprendizado diferente para cada um dos pesos da nossa rede neural, usando taxas pequenas para parâmetros que sofrem atualização frequentemente e taxas maiores para parâmetros que são atualizados mais raramente, permitindo uma otimização mais rápida. . Mais especificamente, o RMSProp divide o update normal do SGD pela raiz da soma dos quadrados dos gradientes anteriores (por isso seu nome Root-Mean-Square Proportional), assim reduzindo a magnitude da atualização de acordo com as magnitudes anteriores. . Equa&#231;&#227;o . $ nu_{t} = gamma nu_{t-1} + (1 - gamma) ( nabla L)^2 w_{t+1} = w_t - frac{ eta nabla L}{ sqrt{ nu_t + epsilon}} $ . C&#243;digo . def computa_momento(updates, moments, decay, order): return jax.tree_multimap( lambda g, t: (1 - decay) * (g ** order) + decay * t, updates, moments) def rmsprop(weights, gradients, eta, nu, gamma): nu = computa_momento(gradients, nu, gamma, 2) updates = jax.tree_multimap( lambda g, n: g * jax.lax.rsqrt(n + 1e-8), gradients, nu) weights = jax.tree_util.tree_multimap(lambda w, g: w - eta*g, weights, updates) return weights, nu . x0 = 9.0 nu = 0.0 x_ = [] y_ = [] for i in range(10): y0, grads = f_g(x0) x_.append(x0) y_.append(y0) x0, nu = rmsprop(x0, grads, 0.9, nu, 0.99) plt.plot(x, y) plt.plot(x_, y_, color=&#39;red&#39;, marker=&#39;o&#39;); . Adam . Por fim o Adam usa ideias semelhantes ao Momentum e ao RMSProp, mantendo médias exponenciais tanto dos gradientes passados, quanto dos seus quadrados. . Equa&#231;&#227;o . $ m_t = beta_1 m_{t-1} + (1 - beta_1) nabla L v_t = beta_2 v_{t-1} + (1 - beta_2) ( nabla L)^2 w_{t+1} = w_t - frac{ eta m_t}{ sqrt{v_t} epsilon} $ . C&#243;digo . import jax.numpy as jnp def adam(weights, gradients, eta, mu, nu, b1, b2): mu = computa_momento(gradients, mu, b1, 1) nu = computa_momento(gradients, nu, b2, 2) updates = jax.tree_multimap( lambda m, v: m / (jnp.sqrt(v + 1e-6) + 1e-8), mu, nu) weights = jax.tree_util.tree_multimap(lambda w, g: w - eta*g, weights, updates) return weights, mu, nu . x0 = 9.0 mu = 0.0 nu = 0.0 x_ = [] y_ = [] for i in range(10): y0, grads = f_g(x0) x_.append(x0) y_.append(y0) x0, mu, nu = adam(x0, grads, 0.8, mu, nu, 0.9, 0.999) plt.plot(x, y) plt.plot(x_, y_, color=&#39;red&#39;, marker=&#39;o&#39;); .",
            "url": "https://hereismari.github.io/codando-deep-learning-jax/2021/04/02/otimizacao.html",
            "relUrl": "/2021/04/02/otimizacao.html",
            "date": " • Apr 2, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "1.5 Introdução a Redes Neurais: minha primeira Rede Neural em JAX :D",
            "content": "Depend&#234;ncias . import matplotlib.pyplot as plt # Matemática + manipulação de vetores import math import numpy as np # JAX import jax from jax import nn import jax.numpy as jnp # # &quot;Fixar&quot; números aleatórios a serem gerados np.random.seed(0) # Trabalhar com os dados import pandas as pd from sklearn.datasets import load_iris, fetch_openml from sklearn.preprocessing import OneHotEncoder from sklearn.model_selection import train_test_split # Utilidades import utils # Recarregar automaticamente dependências caso elas mudem %load_ext autoreload %autoreload 2 . Gerando dados sint&#233;ticos . $Y = 7 * X + 15$ . SYNT_TRAIN_SIZE = 200 # controla o quão espalhados são os dados STD_DEV = 0.7 def random_error(size, mu=0, std_dev=0.5): return np.random.normal(mu, std_dev, size) def add_batch_dim(tensor): if len(tensor.shape) == 1: return jnp.expand_dims(tensor, axis=1) else: return tensor def remove_batch_dim(tensor): return jnp.squeeze(tensor, axis=1) def generate_x(size, use_batch_dim=True): x = np.random.rand(size) if use_batch_dim: x = add_batch_dim(x) return x def plot_line(x, y, style=&#39;-b&#39;): x, y = remove_batch_dim(x), remove_batch_dim(y) return plt.plot([min(x), max(x)], [min(y), max(y)], style) def generate_f(x, a=7, b=15, error_std_dev=0.5, use_batch_dim=True): y = a * x + b + random_error(x.shape, std_dev=error_std_dev) if use_batch_dim: y = add_batch_dim(y) return y def identity(x): return x def _accuracy(pred_y, real_y): p = np.argmax(jax.nn.softmax(pred_y), axis=1) if len(real_y.shape): #Se tiver usando one_hot encoding real_y = np.argmax(real_y, axis=1) return np.sum(p == real_y) / len(pred_y) # gera valores aleatórios para x synt_x = generate_x(SYNT_TRAIN_SIZE) # gera a funcão: Y = 7 * X + 15 synt_y = generate_f(synt_x, error_std_dev=STD_DEV) . WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) . plt.plot(synt_x, synt_y, &#39;ro&#39;, alpha=0.4) plot_line(synt_x, synt_x * 7 + 15) plt.show() . Implementando Rede Neural . def define_params(sizes=[1, 1]): &#39;&#39;&#39;He-et-all initialization&#39;&#39;&#39; weights = [] for i, (in_dim, out_dim) in enumerate(zip(sizes[:-1], sizes[1:])): weights.append({&quot;w&quot;: np.random.randn(in_dim, out_dim) * np.sqrt(2/in_dim), &quot;b&quot;: np.random.randn(out_dim) * np.sqrt(2/in_dim)}) return weights def apply_fn(weights, batch_x, activations): output = batch_x for layer, act_fn in zip(weights, activations): output = jnp.dot(output, layer[&quot;w&quot;]) + layer[&quot;b&quot;] output = act_fn(output) return output def l2_loss(weights, batch_x, real_y, activations): pred_y = apply_fn(weights, batch_x, activations) return 0.5 * np.mean((pred_y - real_y)**2) def cross_entropy(weights, batch_x, real_y, activations): pred_y = apply_fn(weights, batch_x, activations) real_y = jnp.asarray(real_y) return -jnp.mean(jnp.sum(pred_y * real_y, axis=1)) . Implementando SGD . def train_step(weights, batch_x, batch_y, activations, loss_fn=l2_loss, lr=0.1): loss, grads = jax.value_and_grad(loss_fn)(weights, batch_x, batch_y, activations) weights = jax.tree_util.tree_multimap(lambda v, g: v - lr*g, weights, grads) return weights, loss def evaluate(weights, activations, batch_x, batch_y, metrics=[], loss_fn=l2_loss): # run feed forward network pred_y = apply_fn(weights, batch_x, activations) # loss loss = loss_fn(weights, batch_x, batch_y, activations) # metrics res_metrics = [] for m in metrics: res_metrics.append(m(pred_y, batch_y)) return loss, res_metrics def plot_losses(train_losses, eval_losses, step): if len(eval_losses) &gt; 0: plt.title(&#39;Train Loss: %.4f | Test Loss: %.4f for step %d&#39; % (train_losses[-1], eval_losses[-1], step)) plt.plot([i for i in range(0, step, 10)], eval_losses) else: plt.title(&#39;Train Loss: %.4f for step %d&#39; % (train_losses[-1], step)) plt.plot([i for i in range(step)], train_losses) . Gradients . L2 loss with 1 layer, no activation . Loss . $$L = 1/2 * 1/n * sum{(y_i - ŷ_i)^{2}}$$ $$L = 1/2 * 1/n * sum{(y_i - w_i * x_i + b_i)^{2}}$$ . Gradients . $$ frac{ partial L}{ partial w_i} = 1/2 * 1/n * 2 * sum{(y_i - ŷ_i)} * frac{ partial {ŷ_i}}{ partial w_i} $$ $$ frac{ partial L}{ partial w_i} = 1/n * sum{(y_i - ŷ_i)} * x_i$$ . . $$ frac{ partial L}{ partial b_i} = 1/2 * 1/n * 2 * sum{(y_i - ŷ_i)} * frac{ partial {ŷ_i}}{ partial b_i} $$ $$ frac{ partial L}{ partial b_i} = 1/n * sum{(y_i - ŷ_i)} * 1$$ . L2 loss with 2 layers, relu activation in the hidden layer . Loss . $$L = 1/2 * 1/n * sum{(y_i - ŷ_i)^{2}}$$ $$L = 1/2 * 1/n * sum{(y_i - (w_j * x_j + b_j))^{2}}$$ $$x_j = relu(w_i * x_i + b_i)$$ . Gradients . $$ frac{ partial L}{ partial w_i} = 1/n * sum{(y_i - ŷ_i)} * x_j $$ $$ frac{ partial L}{ partial b_i} = 1/n * sum{(y_i - ŷ_i)} * 1$$ . $$ frac{ partial L}{ partial w_j} = 1/n * sum{(y_i - ŷ_i)} * x_j * x_i, se relu() &gt; 0$$ $$ frac{ partial L}{ partial b_j} = 1/n * sum{(y_i - ŷ_i)} * x_j, se relu() &gt; 0$$ . Treinando . neural_net = define_params() train_losses = [] eval_losses = [] for i in range(1000): neural_net, loss = train_step(neural_net, synt_x, synt_y, [identity]) train_losses.append(loss) # if i % 10 == 0: # loss, metrics = evaluate(neural_net, [identity], synt_x, synt_y, metrics=[_accuracy]) # eval_losses.append(loss) plot_losses(train_losses, eval_losses, 1000) . Comparando com a realidade . print(&#39;Parâmetros aprendidos:&#39;) print(&#39;pesos:&#39;, neural_net[0][&quot;w&quot;]) print(&#39;bias:&#39;, neural_net[0][&quot;b&quot;]) print(&#39;Função que modela os dados: 7 * X + 15&#39;) plot_line(synt_x, apply_fn(neural_net, synt_x, [identity]), &#39;--r&#39;) plot_line(synt_x, synt_y) plt.show() . Parâmetros aprendidos: pesos: [[6.692199]] bias: [15.075487] Função que modela os dados: 7 * X + 15 . Uma fun&#231;&#227;o um pouco mais complicada . $Y = 7 * log(x) + 1$ . def get_random_error(size, mu=0, std_dev=0.8): return np.random.normal(mu, std_dev, size) synt_x = np.random.rand(SYNT_TRAIN_SIZE) synt_y = jnp.reshape(7 * np.log(synt_x) + 1 + get_random_error(SYNT_TRAIN_SIZE), (SYNT_TRAIN_SIZE, 1)) synt_x = jnp.reshape(synt_x, (SYNT_TRAIN_SIZE, 1)) . plt.plot(synt_x, synt_y, &#39;ro&#39;, alpha=0.5) . [&lt;matplotlib.lines.Line2D at 0x7f1b2066d990&gt;] . nn = define_params(sizes=[1, 10, 1]) activations=[jax.nn.sigmoid, identity] train_losses = [] eval_losses = [] . for i in range(1000): nn, loss = train_step(nn, synt_x, synt_y, activations) train_losses.append(loss) # if i % 10 == 0: # loss, metrics = evaluate(neural_net, [identity], synt_x, synt_y, metrics=[_accuracy]) # eval_losses.append(loss) plot_losses(train_losses, eval_losses, 1000) . print(&#39;Parâmetros aprendidos:&#39;) print(&#39;pesos:&#39;, [weight[&quot;w&quot;] for weight in nn]) print(&#39;bias:&#39;, [weight[&quot;b&quot;] for weight in nn]) print(&#39;Função que modela os dados: 7 * X + 15&#39;) plt.plot(synt_x, apply_fn(nn, synt_x, activations), &#39;or&#39;, alpha=0.3) plt.plot(synt_x, synt_y, &#39;og&#39;, alpha=0.3) plt.show() . Parâmetros aprendidos: pesos: [DeviceArray([[-4.405657 , 0.4860704, -7.596714 , -2.9677794, 10.011526 , -2.6017227, -1.8996462, -8.952797 , 1.0131673, -8.821257 ]], dtype=float32), DeviceArray([[-5.419962 ], [-0.21313462], [-7.451459 ], [-5.0258594 ], [ 8.097725 ], [-4.566792 ], [-2.3503356 ], [-8.587139 ], [-1.6483995 ], [-8.4986925 ]], dtype=float32)] bias: [DeviceArray([ 0.02200077, -0.5802973 , -0.1432708 , 1.2722787 , -0.08803245, 1.3667308 , 0.03255983, -0.02162656, 2.0635371 , -0.03442049], dtype=float32), DeviceArray([-3.8411722], dtype=float32)] Função que modela os dados: 7 * X + 15 . E se os dados forem n&#227;o lineares? . xor_x = jnp.array([[0, 0], [0, 1], [1, 0], [1, 1]]) xor_y = jnp.array([[0], [1], [1], [0]]) . activations=[jax.nn.relu, identity] nn = define_params(sizes=[2, 10, 2]) for i in range(1000): nn, loss = train_step(nn, xor_x, xor_y, activations) # plot_losses() . plt.plot(xor_x, apply_fn(nn, xor_x, activations), &#39;bo&#39;, xor_x, xor_y, &#39;ro&#39;, alpha=0.3) . [&lt;matplotlib.lines.Line2D at 0x7f1b0072c590&gt;, &lt;matplotlib.lines.Line2D at 0x7f1b0071f290&gt;, &lt;matplotlib.lines.Line2D at 0x7f1b00738410&gt;, &lt;matplotlib.lines.Line2D at 0x7f1b00738850&gt;] . activations=[identity, identity] nn = define_params(sizes=[2, 10, 2]) for i in range(1000): nn, loss = train_step(nn, xor_x, xor_y, activations) # plot_losses() . plt.plot(xor_x, apply_fn(nn, xor_x, activations), &#39;bo&#39;, xor_x, xor_y, &#39;ro&#39;, alpha=0.3) . [&lt;matplotlib.lines.Line2D at 0x7f1b204f0ed0&gt;, &lt;matplotlib.lines.Line2D at 0x7f1b00694fd0&gt;, &lt;matplotlib.lines.Line2D at 0x7f1b006af3d0&gt;, &lt;matplotlib.lines.Line2D at 0x7f1b006af490&gt;] . Exemplo: base dados Iris . Digamos que para um exemplo da base de dados queremos determinar qual a espécie dessa planta. . Entradas . A base de dados iris tem 4 atributos de uma planta que iremos usar como entrada. . Saídas . Neste caso a saída que nos interessa é a espécie da planta. Então digamos que a saída é um número que indica qual a espécie: . 0 = Iris Setosa , 1 = Iris Versicolour, 2 = Iris Virginica . Obtendo os dados . iris = load_iris() # np.c_ concatena as features e targets do dataset iris_data = pd.DataFrame(data=np.c_[iris[&#39;data&#39;], iris[&#39;target&#39;]], columns=[&#39;x0&#39;, &#39;x1&#39;, &#39;x2&#39;, &#39;x3&#39;, &#39;target&#39;]) . iris_data.head() . x0 x1 x2 x3 target . 0 5.1 | 3.5 | 1.4 | 0.2 | 0.0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 0.0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 0.0 | . 3 4.6 | 3.1 | 1.5 | 0.2 | 0.0 | . 4 5.0 | 3.6 | 1.4 | 0.2 | 0.0 | . iris_data.describe() . x0 x1 x2 x3 target . count 150.000000 | 150.000000 | 150.000000 | 150.000000 | 150.000000 | . mean 5.843333 | 3.057333 | 3.758000 | 1.199333 | 1.000000 | . std 0.828066 | 0.435866 | 1.765298 | 0.762238 | 0.819232 | . min 4.300000 | 2.000000 | 1.000000 | 0.100000 | 0.000000 | . 25% 5.100000 | 2.800000 | 1.600000 | 0.300000 | 0.000000 | . 50% 5.800000 | 3.000000 | 4.350000 | 1.300000 | 1.000000 | . 75% 6.400000 | 3.300000 | 5.100000 | 1.800000 | 2.000000 | . max 7.900000 | 4.400000 | 6.900000 | 2.500000 | 2.000000 | . iris_data.drop([&#39;target&#39;], axis=1).diff().hist(color=&#39;k&#39;, alpha=0.5, bins=10, figsize=(4, 5)) plt.show() . def _one_hot(x, k, dtype=np.float32): &quot;&quot;&quot;Create a one-hot encoding of x of size k.&quot;&quot;&quot; return np.array(x[:, None] == np.arange(k), dtype) . x = iris.data y = iris.target y = _one_hot(y, 3) x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42) . def batches(x, y, batch_size=16): idx = np.random.permutation(len(x)) x = x[idx] y = y[idx] for i in range(0, len(x)-batch_size-1, batch_size): batch_x = x[i:i+batch_size] batch_y = y[i:i+batch_size] yield batch_x, batch_y . activations=[jax.nn.relu, jax.nn.log_softmax] nn = define_params(sizes=[4, 10, 3]) for i in range(100): for batch_x, batch_y in batches(x_train, y_train): nn, loss = train_step(nn, batch_x, batch_y, activations, loss_fn=cross_entropy, lr=0.01) if i % 10 == 0: loss, metrics = evaluate(nn, activations, x_test, y_test, metrics=[_accuracy], loss_fn=cross_entropy) print(&#39;Test loss = %.5f, accuracy %.5f&#39; % (loss, metrics[0])) plot_losses(train_losses, eval_losses, 1000) . Test loss = 0.96247, accuracy 0.63158 Test loss = 0.76534, accuracy 0.71053 Test loss = 0.66534, accuracy 0.71053 Test loss = 0.62524, accuracy 0.71053 Test loss = 0.59800, accuracy 0.71053 Test loss = 0.57885, accuracy 0.71053 Test loss = 0.56278, accuracy 0.71053 Test loss = 0.54652, accuracy 0.71053 Test loss = 0.53486, accuracy 0.76316 Test loss = 0.52478, accuracy 0.76316 . MNIST . mnist = fetch_openml(&#39;mnist_784&#39;, version=1, return_X_y=True, as_frame=False) . x = mnist[0] / np.max(mnist[0]) y = np.array([int(label) for label in mnist[1]]) y = _one_hot(y, 10) x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42) . activations=[jax.nn.relu, jax.nn.relu, jax.nn.log_softmax] nn = define_params(sizes=[784, 512, 256, 10]) for i in range(20): for batch_x, batch_y in batches(x_train, y_train, 64): nn, loss = train_step(nn, batch_x, batch_y, activations, loss_fn=cross_entropy, lr=0.001) loss, metrics = evaluate(nn, activations, x_test, y_test, metrics=[_accuracy], loss_fn=cross_entropy) print(&#39;Test loss = %.5f, accuracy %.5f&#39; % (loss, metrics[0])) # t.plot_losses() . Test loss = 1.34204, accuracy 0.73646 Test loss = 0.88529, accuracy 0.81646 Test loss = 0.67878, accuracy 0.84709 Test loss = 0.57118, accuracy 0.86326 Test loss = 0.50617, accuracy 0.87320 Test loss = 0.46268, accuracy 0.88023 Test loss = 0.43193, accuracy 0.88600 Test loss = 0.40797, accuracy 0.89120 Test loss = 0.38929, accuracy 0.89606 Test loss = 0.37444, accuracy 0.89977 Test loss = 0.36173, accuracy 0.90217 Test loss = 0.35139, accuracy 0.90497 Test loss = 0.34184, accuracy 0.90789 Test loss = 0.33357, accuracy 0.90869 Test loss = 0.32648, accuracy 0.91091 Test loss = 0.31970, accuracy 0.91257 Test loss = 0.31361, accuracy 0.91423 Test loss = 0.30800, accuracy 0.91611 Test loss = 0.30299, accuracy 0.91669 Test loss = 0.29812, accuracy 0.91777 .",
            "url": "https://hereismari.github.io/codando-deep-learning-jax/2021/02/10/1.8.minha_primeira_rede_neural_jax.html",
            "relUrl": "/2021/02/10/1.8.minha_primeira_rede_neural_jax.html",
            "date": " • Feb 10, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Introdução ao Numpy",
            "content": "De acordo com as próprias palavras da numpy.org (tradução livre): . Numpy é um pacote fundamental pra computação científica com Python. . E de fato, numpy é muito show! É bem fácil de usar e se usado corretamente super eficiente:10/10. Entre outras coisas, o numpy possui: . um podereso ferramental para manipulação de arrays multi-dimensionais | funções de broadcasting sofisticadas (veremos isso já já) | ferramentas para integração de código Fortran e C/C++ (por baixo dos panos pra gente não ter que se preocupar) | utilidades para álgebra linear, tranformação de Fourier e números aleatórios | . Para usar o numpy, tudo que precisamos é importá-lo: . # pq numpy é muito grande pra ficar digitando toda hora import numpy as np . Ta... o que nós podemos fazer com numpy? . Bem eu disse que numpy possui &quot;um podereso ferramental para manipulação de arrais multi-dimensionais&quot;. . o que danado isso significa? . MÁGICA! . Vamos começar criando um array numpy. Um array numpy parece igualzinho a uma lista em python, mas não se engane, não é! . list_py = [1, 2, 3] # Array numpy: cool e descolado vector_np = np.array([1, 2, 3]) print(&#39;list python:&#39;, type(list_py), list_py) print(&#39;vetor numpy&#39;, type(vector_np), vector_np) . list python: &lt;class &#39;list&#39;&gt; [1, 2, 3] vetor numpy &lt;class &#39;numpy.ndarray&#39;&gt; [1 2 3] . Humm... okay, vamos tentar dar um append nas listas... . vector_py.append(1) # Erro! vector_np.append(1) . NameError Traceback (most recent call last) &lt;ipython-input-3-46dcf3fb9fae&gt; in &lt;module&gt; 1 # Ok! -&gt; 2 vector_py.append(1) 3 4 # Erro! 5 vector_np.append(1) NameError: name &#39;vector_py&#39; is not defined . Não existe append diretamente em um vetor numpy... Mas você pode fazer append via numpy... . vector_np = np.append(vector_np, 1) . Mas fica aquele velho ditado: não é porque você pode que você deve! . Normalmente não executamos append em vetores numpy, eles já nascem prontinhos e realizamos operações entre vetores (adição, multiplicação, transposição, ...). Não adicionamos ou removemos valores de vetores numpy. . Por que? Em geral isso é ineficiente, olha o exemplo abaixo. . Obs:Para mais detalhes da uma olhada nesses links 1, 2. . import time N = 100000 # Usando uma lista python e depois convertendo para numpy def time_to_create_vector_py(): start = time.time() l = [] for i in range(N): l.append(i) l = np.array(l) return time.time() - start # Usando um vetor numpy desde o inicio pq sou teimoso def time_to_create_vector_np(): start = time.time() l = np.array([]) for i in range(N): l = np.append(l, i) l = np.array(l, dtype=int) return time.time() - start print(&#39;Tempo utilizando lista: %.3f (s)&#39; % time_to_create_vector_py()) print(&#39;Tempo utilizando numpy: %.3f (s)&#39; % time_to_create_vector_np()) . Tempo utilizando lista: 0.012 (s) Tempo utilizando numpy: 2.194 (s) . Ta, então uma vez que a gente cria um array numpy, normalmente não alteramos ele via operações convencionais de modificação de lista. . Se você para pra pensar isso faz todo sentido: já que um array numpy não é uma lista python, mas sim um vetor. . Já que temos vetores, temos propriedades de vetores associadas aos arrays numpy: . Vamos chamar o número de dimensões de um vetor de ndim; | O shape é uma tupla de inteiros do tamanho do ndim que fornece número de elementos ao longo de cada dimensão. | . vector_np . array([1, 2, 3, 1]) . Quanto é o ndim desse vetor? . vector_np.ndim . 1 . E o formato? . vector_np.shape . (4,) . E se em vez de um array tivermos um número? . scalar_np = np.array(3) # 0 pq não temos nenhuma dimensão print(scalar_np.ndim) print(scalar_np.shape) . 0 () . E matrizes? . matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] matrix_np = np.array(matrix) print(matrix_np.ndim) # (3, 3) pq é uma matrix 3x3 print(matrix_np.shape) . 2 (3, 3) . O céu é o limite! . a = np.array([1, 2, 3]) # cria uma matriz de 1 dimensão (vetor) b = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.int64) # cria uma matriz de 2 dimensões (matriz) print(&#39;Vetor a:&#39;, a, &#39; nFormato de a:&#39;, a.shape) print(&#39;-&#39;) print(&#39;Vetor b: n&#39;, b, &#39; nFormato de b:&#39;, b.shape) . Vetor a: [1 2 3] Formato de a: (3,) - Vetor b: [[1 2 3] [4 5 6]] Formato de b: (2, 3) . tensor_np = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]]) tensor_np.shape . (3, 2, 2) . M&#225;gica com Numpy . Broadcast . Esse código python quebra por que não da pra somar inteiro com lista. . 1 + matrix . TypeError Traceback (most recent call last) &lt;ipython-input-13-b2a55592596e&gt; in &lt;module&gt; -&gt; 1 1 + matrix TypeError: unsupported operand type(s) for +: &#39;int&#39; and &#39;list&#39; . Mas pensa um pouco... o código faz &quot;sentido&quot;, parece que 1 deveria ser somado a cada elemento da lista. . Bem que dava pra python ser mais esperto e entender que o que a gente quer é na verdade propagar o 1 por toda a matriz somando cada elemento a 1. . Essa propagação é justamente o que chamamos de broadcasting. Que é a ideia de que vetores de tamanhos e formatos diferentes são compatíveis para certas operações em alguns casos! . matrix_np . array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) . # Somar um valor a todos os elementos de uma matriz nunca foi tao fácil! 1 + matrix_np . array([[ 2, 3, 4], [ 5, 6, 7], [ 8, 9, 10]]) . Por que? . Porque numpy consegue entender a partir dos formatos dos vetores o que deve ser feito. O que acontece se tentarmos somar um vetor de tamanho 3 a matriz? . [1, 2, 3] + matrix_np . array([[ 2, 4, 6], [ 5, 7, 9], [ 8, 10, 12]]) . Sem broadcasting, como faríamos a operação acima? . v = np.array([1, 2, 3]) v . array([1, 2, 3]) . vv = np.tile(v, (3, 1)) # Cria 4 cópias de v e empilha vv . array([[1, 2, 3], [1, 2, 3], [1, 2, 3]]) . matrix_np + vv . array([[ 2, 4, 6], [ 5, 7, 9], [ 8, 10, 12]]) . O broadcasting nos permite obter o mesmo resultado sem precisar criar cópias do vetor. Sendo mais eficiente tanto em tempo quanto em memória. Além de simplificar bastante a nossa vida. . np.array([[1], [2], [3]]) + matrix_np . array([[ 2, 3, 4], [ 6, 7, 8], [10, 11, 12]]) . Vai ter caso que vai dar errado? . [1, 2] + matrix_np . ValueError Traceback (most recent call last) &lt;ipython-input-21-fe98043e1a08&gt; in &lt;module&gt; 1 # Somar uma lista a todas as linhas de uma matriz nunca foi tao... eita perainda quebrou aqui -&gt; 2 [1, 2] + matrix_np ValueError: operands could not be broadcast together with shapes (2,) (3,3) . O que aconteceu foi que os formatos dos vetores não são compatíveis, então numpy não conseguiu realizar broadcast corretamente para realizar a operação. . O que é até intuitivo, o que danado a gente estava esperando somando um vetor de tamanho 2 com uma matriz 3x3? . Existe um algoritmo que podemos utilizar pra saber se o broadcast vai dar certo: . Recebemos a e b | Percorremos os formatos de a e b de trás pra frente | Para cada uma das dimensões dim_a e dim_b deve ser verdade que: dim_a == dim_b ou dim_a == 1 ou dim_b == 1 | . | . Pense um pouco a respeito... e tende adivinhar os formatos de cada vetor e se os pares são passíveis de broadcast: . Caso 1: [1, 2] e [[1, 2, 3], [4, 5, 6], [7, 8, 9]] | Caso 2: [1] e [[1, 2, 3], [4, 5, 6], [7, 8, 9]] | Caso 3: [[[[[1]]]]] e [[1, 2, 3], [4, 5, 6], [7, 8, 9]] | Caso 4: [1, 2] e [[1, 2], [4, 5], [7, 8]] | . O código da checagem e soluções seguem logo abaixo. . def is_broadcast_possible(a, b): # Oferecimento: https://stackoverflow.com/questions/47243451/checking-if-two-arrays-are-broadcastable-in-python a, b = np.array(a), np.array(b) print(&#39;Formato de a:&#39;, a.shape) print(&#39;Formato de b:&#39;, b.shape) return all((m == n) or (m == 1) or (n == 1) for m, n in zip(a.shape[::-1], b.shape[::-1])) . is_broadcast_possible([1, 2], [[1, 2, 3], [4, 5, 6], [7, 8, 9]]) . Formato de a: (2,) Formato de b: (3, 3) . False . is_broadcast_possible([1], [[1, 2, 3], [4, 5, 6], [7, 8, 9]]) . Formato de a: (1,) Formato de b: (3, 3) . True . is_broadcast_possible([[[[[1]]]]], [[1, 2, 3], [4, 5, 6], [7, 8, 9]]) . Formato de a: (1, 1, 1, 1, 1) Formato de b: (3, 3) . True . is_broadcast_possible([1, 2], [[1, 2], [4, 5], [7, 8]]) . Formato de a: (2,) Formato de b: (3, 2) . True . Manipula&#231;&#227;o de matrizes . matrix_np.transpose() . array([[1, 4, 7], [2, 5, 8], [3, 6, 9]]) . matrix_np.dot(matrix_np) . array([[ 30, 36, 42], [ 66, 81, 96], [102, 126, 150]]) . Criando tipos espec&#237;ficos de arrays . np.zeros((3, 3)) . array([[ 0., 0., 0.], [ 0., 0., 0.], [ 0., 0., 0.]]) . np.ones((3, 3)) . array([[ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.]]) . np.full((3, 3), 7) . array([[7, 7, 7], [7, 7, 7], [7, 7, 7]]) . np.eye(3) . array([[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]) . np.diag([1, 2, 3]) . array([[1, 0, 0], [0, 2, 0], [0, 0, 3]]) . np.diag([1, 2, 3], k=1) # k = offset da diagonal . array([[0, 1, 0, 0], [0, 0, 2, 0], [0, 0, 0, 3], [0, 0, 0, 0]]) . np.mgrid[1:4, 1:4] # similar ao meshgrid no Matlab . array([[[1, 1, 1], [2, 2, 2], [3, 3, 3]], [[1, 2, 3], [1, 2, 3], [1, 2, 3]]]) . np.random.rand(3, 3) # distribuição aleatória . array([[ 0.91364028, 0.4408339 , 0.65251051], [ 0.89478035, 0.49410589, 0.68962237], [ 0.11590382, 0.32048899, 0.75107557]]) . np.random.randn(3, 3) # distribuição normal (gaussiana) . array([[ 0.05290034, -0.87768182, -0.94384387], [-0.63596293, -0.00936166, 0.25018653], [ 0.99313152, -1.10631277, 1.02928441]]) . np.random.randint(1, 10, (3, 3)) # número aleatórios inteiros de 1 a 10 . array([[7, 7, 9], [9, 4, 2], [1, 5, 7]]) . np.arange([start,] stop[,step,], dtype=None) . np.arange(10) . array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . np.arange(1,10) . array([1, 2, 3, 4, 5, 6, 7, 8, 9]) . np.arange(1, 10, 0.5) . array([ 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5, 6. , 6.5, 7. , 7.5, 8. , 8.5, 9. , 9.5]) . np.arange(1, 10, 3) . array([1, 4, 7]) . np.arange(1, 10, 2, dtype=np.float64) . array([ 1., 3., 5., 7., 9.]) . np.linspace(start, stop, num=50, endpoint=True, retstep=False) . np.linspace(1, 5, num=10) . array([ 1. , 1.44444444, 1.88888889, 2.33333333, 2.77777778, 3.22222222, 3.66666667, 4.11111111, 4.55555556, 5. ]) . np.linspace(0, 2, num=4) . array([ 0. , 0.66666667, 1.33333333, 2. ]) . np.linspace(0, 2, num=4, endpoint=False) . array([ 0. , 0.5, 1. , 1.5]) . Examinando um array n-dimensional . ds = np.array([[1,2,3],[4,5,6],[7,8,9]]) ds.ndim . 2 . ds.shape . (3, 3) . ds.size . 9 . ds.dtype # tipo dos elementos guardados . dtype(&#39;int64&#39;) . ds.itemsize # qtde de bytes por valor . 8 . ds.size * ds.itemsize # espaço total ocupado em memória (em bytes) . 72 . An&#225;lise Estat&#237;stica . data_set = np.random.random((2, 3)) data_set . array([[ 0.97054673, 0.35857832, 0.20979014], [ 0.6939001 , 0.87199282, 0.7442244 ]]) . np.max(a, axis=None, out=None, keepdims=False) . np.max(data_set) . 0.97054672767941563 . np.max(data_set, axis=0) . array([ 0.97054673, 0.87199282, 0.7442244 ]) . np.max(data_set, axis=1) . array([ 0.97054673, 0.87199282]) . np.min(a, axis=None, out=None, keepDims=False) . np.min(data_set) . 0.20979014003667951 . np.mean(a, axis=None, dtype=None, out=None, keepdims=False) . np.mean(data_set) . 0.64150541875340539 . np.median(a, axis=None, out=None, overwrite_input=False) . np.median(data_set) . 0.71906224976763788 . np.std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False) . np.std(data_set) . 0.27114413265171439 . np.sum(a, axis=None, dtype=None, out=None, keepdims=False) . np.sum(data_set) . 3.8490325125204325 . np.prod(a, axis=None, dtype=None, out=None, keepdims=False) . np.prod(data_set) . 0.032877540156687972 . np.cumsum(a, axis=None, dtype=None, out=None) . np.cumsum(data_set) . array([ 0.97054673, 1.32912505, 1.53891519, 2.23281529, 3.10480811, 3.84903251]) . np.cumprod(a, axis=None, dtype=None, out=None) . np.cumprod(data_set) . array([ 0.97054673, 0.34801702, 0.07301054, 0.05066202, 0.04417692, 0.03287754]) . Redimensionando Arrays . np.reshape(a, newshape, order=&#39;C&#39;) . np.reshape(data_set, (3, 2)) . array([[ 0.97054673, 0.35857832], [ 0.20979014, 0.6939001 ], [ 0.87199282, 0.7442244 ]]) . np.reshape(data_set, (6, 1)) . array([[ 0.97054673], [ 0.35857832], [ 0.20979014], [ 0.6939001 ], [ 0.87199282], [ 0.7442244 ]]) . np.reshape(data_set, 6) . array([ 0.97054673, 0.35857832, 0.20979014, 0.6939001 , 0.87199282, 0.7442244 ]) . np.ravel(a, order=&#39;C&#39;) . np.ravel(data_set) . array([ 0.97054673, 0.35857832, 0.20979014, 0.6939001 , 0.87199282, 0.7442244 ]) . data_set.flatten() # igual ao ravel . array([ 0.97054673, 0.35857832, 0.20979014, 0.6939001 , 0.87199282, 0.7442244 ]) . Acessando elementos . Indexa&#231;&#227;o . data_set = np.random.randint(1, 10, (5, 5)) data_set . array([[1, 2, 7, 5, 5], [9, 9, 7, 1, 2], [9, 4, 4, 5, 3], [9, 6, 6, 5, 3], [7, 9, 8, 5, 1]]) . data_set[1] # segunda linha . array([9, 9, 7, 1, 2]) . data_set[1][0] # segunda linha, primeira coluna . 9 . data_set[1, 0] # equivalente a de cima . 9 . Indexa&#231;&#227;o por inteiros . Quando você indexa matrizes numpy usando slicing, a matriz resultante sempre será um subarray da matriz original. Por outro lado, a indexação de matrizes por inteiros permite que você construa matrizes arbitrárias usando os dados de outra matriz. Aqui está um exemplo: . a = np.array([[1,2], [3,4], [5,6]]) a . array([[1, 2], [3, 4], [5, 6]]) . a[[0, 1, 2], [0, 1, 0]] . array([1, 4, 5]) . np.array([a[0, 0], a[1, 1], a[2, 0]]) # equivalente ao de cima . array([1, 4, 5]) . Indexa&#231;&#227;o booleana . Boolean array indexing lets you pick out arbitrary elements of an array. Frequently this type of indexing is used to select the elements of an array that satisfy some condition. Here is an example: . a = np.array([[1,2], [3,4], [5,6]]) a . array([[1, 2], [3, 4], [5, 6]]) . bool_idx = (a &gt; 2) print(bool_idx) . [[False False] [ True True] [ True True]] . print(a[bool_idx]) . [3 4 5 6] . print(a[a &gt; 2]) . [3 4 5 6] . Slicing . data_set[2:4] # terceira e quarta linhas . array([[9, 4, 4, 5, 3], [9, 6, 6, 5, 3]]) . data_set[2:4, 0] # terceira e quarta linhas, primeira coluna . array([9, 9]) . data_set[2:4, 0:2] # terceira e quarta linhas, primeira e segunda coluna . array([[9, 4], [9, 6]]) . data_set[:, 0] # todas as linhas, primeira coluna . array([1, 9, 9, 9, 7]) . Steping . data_set[:, 0:10:2] # 1ª, 3ª, 5ª, 7ª e 9ª colunas para todas as linhas . array([[1, 7, 5], [9, 7, 2], [9, 4, 3], [9, 6, 3], [7, 8, 1]]) . data_set[::] . array([[1, 2, 7, 5, 5], [9, 9, 7, 1, 2], [9, 4, 4, 5, 3], [9, 6, 6, 5, 3], [7, 9, 8, 5, 1]]) . data_set[::2] # 1ª, 3ª e 5ª linha, todas as colunas . array([[1, 2, 7, 5, 5], [9, 4, 4, 5, 3], [7, 9, 8, 5, 1]]) . Opera&#231;&#245;es com matrizes . x = np.array([[1,2], [3,4]]) y = np.array([[5,6], [7,8]]) print(x+y) print(np.add(x,y)) . [[ 6 8] [10 12]] [[ 6 8] [10 12]] . print(x - y) print(np.subtract(x, y)) . [[-4 -4] [-4 -4]] [[-4 -4] [-4 -4]] . print(x * y) print(np.multiply(x, y)) . [[ 5 12] [21 32]] [[ 5 12] [21 32]] . print(x / y) print(np.divide(x, y)) . [[ 0.2 0.33333333] [ 0.42857143 0.5 ]] [[ 0.2 0.33333333] [ 0.42857143 0.5 ]] . print(np.sqrt(x)) . [[ 1. 1.41421356] [ 1.73205081 2. ]] . Observe que, diferentemente do MATLAB, $*$ é a multiplicação elementar, não a multiplicação de matrizes. Em vez disso, usamos a função dot para calcular produtos internos de vetores, multiplicar um vetor por uma matriz e multiplicar matrizes. . v = np.array([9, 10]) w = np.array([11, 12]) # produto interno print(v.dot(w)) print(np.dot(v, w)) . 219 219 . print(x.dot(v)) print(np.dot(x, v)) . [29 67] [29 67] . print(x.dot(y)) print(np.dot(x, y)) . [[19 22] [43 50]] [[19 22] [43 50]] . print(x.T) print(&#39;-&#39;) # ou print(np.transpose(x)) . [[1 3] [2 4]] - [[1 3] [2 4]] . Lista de todas as operações: documentation. .",
            "url": "https://hereismari.github.io/codando-deep-learning-jax/2021/01/21/intro_a_numpy.html",
            "relUrl": "/2021/01/21/intro_a_numpy.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Introdução ao JAX",
            "content": "JAX é uma nova biblioteca de Python da Google com foco em pesquisa de alta performance em Aprendizado de Máquina e seguindo o paradigma de programação funcional. . Mais especificamente JAX nos dá acesso a: e , as principais sendo grad, jit, vmap e pmap(que vai ter seu próprio post no futuro). . uma API compatível com numpy e scipy | uma própria API de números aleatório manejados manualmente | transformações de função composicionais (você pode aplicar elas em conjunto sem muito problema) | transformações para trivialmente computar derivadas de funções | transformações para usar aceleradores (CPU, GPU e TPU) | transformações que permitem paralelizar seu código facilmente | . Para usar o JAX é recomendado checar as instruções de instalação, e a seguir importá-lo: . # jax.numpy toda hora, então fazemos um alias dele para jnp import jax import jax.numpy as jnp . Agora vamos ver cada uma dessas features da biblioteca . O Wrapper de Numpy: jax.numpy: . import numpy as np a = np.array([1., 2., 3.]) b = np.array([1., 1., -1.]) print(np.dot(a, b), jnp.dot(a, b)) . 0.0 0.0 . /home/joaogui/miniconda3/envs/jax/lib/python3.7/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn(&#39;No GPU/TPU found, falling back to CPU.&#39;) . Esse warning que aparece é ele dizendo que sou pobre e não tenho nem GPU nem TPU :( . (np.square(a), jnp.square(a)) . (array([1., 4., 9.]), DeviceArray([1., 4., 9.], dtype=float32)) . Note que JAX tem seu próprio tipo de array, o DeviceArray, em geral as funções vão transformar arrays de numpy em DeviceArrays, então se você quiser boa performance é melhor fazer essa transformação manualmente antes de passar os dados para várias funções. . N&#250;meros aleat&#243;rios jax.random . Uma das partes mais peculiares de JAX, para faciliar implementações usando paralelismo não existe uma semente global para geradores de números aleatórios, em vez disso em JAX você passa explicitamente a seed para cada função que envolve aleatoriedade, e cabe a você atualizá-la . key = jax.random.PRNGKey(42) #cria um semente aleatória a = jax.random.normal(key, ()) b = jax.random.normal(key, ()) print(a, b) print(a == b) #como usamos a mesma semente para a mesma função temos valores iguais k1, k2 = jax.random.split(key, 2) #vamos criar duas novas seeds a partir da primeira a = jax.random.normal(k1, ()) b = jax.random.normal(k2, ()) print(a, b) #Agora são diferentes . -0.18471184 -0.18471184 True 0.13790314 1.3694694 . Transforma&#231;&#245;es . O principal diferencial de JAX são suas tranformações de funções, que nos permitem modificar facilmente funções definidas a partir de outras funções do JAX e algumas primitivas de Python. . Algo muito útil e legal delas é que podem ser utilizadas em conjunto (são &quot;composicionais&quot;), nos permitindo por exemplo compilar a derivada de uma função vetorizada apenas aplicando 3 transformações uma seguida da outra a função original. . Porém existem alguns cuidados que dever ser tomados ao se usar esses transformações, para entender esse cuidados melhor cheque esse link e abra o notebook . Diferencia&#231;&#227;o Autom&#225;tica: jax.grad . Em aprendizado de máquina, principalmente quando estamos tratando de redes neurais, lidamos com muitas derivadas, gradientes e afins: Para treinar uma regressão linear ou logística, precisamos computar um hessiano, para treinar uma rede neural usamos descida de gradiente, que requer o cálculo de um gradiente, dentre outros exemplos. . Computar essas derivadas na mão é muitas vezes extremamente trabalhoso, ou até mesmo impossível dado o tempo disponível, assim temos algoritmos como o backpropagation para redes neurais, porém se sempre tivessemos que implementar nós mesmos esse algoritmo, e implementar a derivada de cada uma das funções que vamos usar, terminaríamos com uma quatidade imensa de código duplicado, além de uma imensa chance de errarmos algo na implementação e terminarmos sem conseguir bons resultados ou com resultados que não correspodem a realidade. . Para lidar com isso temos diferenciação automática, transformações que recebem uma função e retornam algum tipo de derivada dela. Simplesmente ter diferenciação automática para as funções de Numpy já é o bastante para uma biblioteca mostrar seu valor, tanto que existe uma biblioteca que é exatamente isso, chamada de Autograd, em muitos sentidos JAX é um sucessor dessa biblioteca. . from jax import grad from math import pi, sqrt dup = grad(jnp.square) print(dup(3.0)) #A derivada de x² é 2x print(grad(dup)(3.0)) #Podemos aplicar várias vezes a grad @grad #Podemos usar as transformações como decoradores def composite_func(x): y = x**2 return jnp.cos(y) # Pela regra da cadeia, dcos(x²)/dx = -2xsen(x²) print(composite_func(jnp.sqrt(pi/2)), -2*sqrt(pi/2)) . 6.0 2.0 -2.5066283 -2.5066282746310002 . Para funções com várias variáveis de entrada a grad por padrão nos dá a derivada em função do primeiro parâmetro, mas podemos mudar isso com o argumento argnums. Também vale ressaltar que os argumentos não precisam ser apenas números e podem ser vetores . def f(x, y): return x*(y**2) dfdy = grad(f, argnums=(1)) print(dfdy(3.0, 4.0)) gradient = grad(f, argnums=(0, 1)) print(gradient(3.0, 4.0)) def g(v): return jnp.linalg.norm(v) print(grad(g)(a)) . 24.0 (DeviceArray(16., dtype=float32), DeviceArray(24., dtype=float32)) 1.0 . Compila&#231;&#227;o com XLA: jit . Mas as vantagens de jax não param em diferenciação automática, se não seria apenas um clone do autograd, jax também tem a habilidade de compilar funções usando o XLA (accelerated linear algebra) da Google, tornando-as bem mais rápidas, além de permitir o uso de aceleradores como GPUs e TPUs. . from jax import jit a = 1 + jax.random.normal(k1, (2024, 2024)) b = 1 + jax.random.normal(k2, (2024, 2024)) . A vantagem se torna maior (&gt; 20x mais rápido) quando usamos aceleradores. . @jit #equivalente a definir jcos e escrever jcos = jit(jcos) def jcos(a, b): return jnp.dot(a, b)/jnp.sqrt(jnp.dot(a, a)*jnp.dot(b, b)) def npcos(a, b): return np.dot(a, b)/np.sqrt(np.dot(a, a)*np.dot(b, b)) . %%timeit npcos(a, b) . 241 ms ± 60.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . jcos(a, b) #Rodamos uma vez fora para compilar a função . DeviceArray([[1.0336248 , 1.028297 , 1.0310295 , ..., 0.96507126, 1.0339284 , 1.0405782 ], [0.94755113, 1.0175968 , 0.9993737 , ..., 1.0073111 , 1.0127649 , 1.0145004 ], [0.99184996, 1.0617024 , 1.0004048 , ..., 1.0069526 , 1.0510893 , 1.0679886 ], ..., [0.965744 , 1.0246744 , 1.0708025 , ..., 1.0135127 , 1.0477784 , 0.98690724], [0.9184314 , 0.99969995, 0.9819234 , ..., 0.9972953 , 0.9442775 , 0.9897808 ], [0.99618256, 1.0751995 , 1.0236498 , ..., 1.0285234 , 1.0351353 , 1.0573723 ]], dtype=float32) . %%timeit jcos(a, b) . 220 ms ± 4.32 ms per loop (mean ± std. dev. of 7 runs, 100 loops each) . Vetoriza&#231;&#227;o Autom&#225;tica: vmap . Vmap é uma transformação muito interessante, usando ela é possível vetorizar automaticamente nossas funções, ou seja, em vez de ter que fazer uma função que lida com um batch de dados, podemos fazer uma função que recebe um único dado e depois usar a trasformação para ganhar a versão que lida com o batch. . a = np.array([1., 2., 3.]) b = np.array([1., 1., -1.]) c = np.array([[1., 2., 3.], [4., 5., 6.]]) @jax.vmap #Podemos usar as transformações como decoradores def f(x, y): return x/y + 1. print(f(a, b)) def prod(x, y): return x@y print(prod(a, b)) . [ 2. 3. -2.] 0.0 . try: prod(a, c) #a e c não têm dimensões compatíveis except Exception as e: print(e) . matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 3) . batch_prod = jax.vmap(prod, in_axes=(None, 0)) #vamos multiplica a por cada linha de c batch_prod(a, c) . DeviceArray([14., 32.], dtype=float32) . Nessa primeira parte vimos qual o propósito da biblioteca e suas principais funções, nos próximos posts vamos explorar como criar redes neurais com jax, suas bibliotecas experimentais, o ecossistema de bibliotecas escritas usando jax. .",
            "url": "https://hereismari.github.io/codando-deep-learning-jax/2021/01/21/intro_a_jax.html",
            "relUrl": "/2021/01/21/intro_a_jax.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Introdução a Deep Learning",
            "content": "Deep Learning têm recebido bastante atenção nos últimos anos, tanto no campo da computação como na mídia em geral. . Técnicas de Deep Learning (DL) se tornaram o estado da arte em várias tarefas de Inteligência Artificial e mudado vários grandes campos da área, na visão computacional (classificação de imagens, segmentação de imagens), NLP (tradução, classificação de textos), reinforcement learning (agentes capazes de jogar jogos complexos como Go, Atari e DOTA). . Esse impacto não fica só na teoria, DL está mudando o campo da Medicina, Ciência, Matemática, Física. Artigos de Deep Learning são rotinamente publicados em grandes revistas científicas como: Nature, Science e JAMA. . Nesta seção iremos discutir: . O que é Deep Learning? | Eu realmente deveria me importar com DL ou é só hype? | Por que eu como programador(a) deveria me importar? | . DL = Muitos Dados + Redes Neurais + Otimiza&#231;&#227;o . Intelig&#234;ncia Artificial $ supseteq$ Machine Learning $ supseteq$ Deep Learning . . IA (Inteligência Artificial), ML (Machine Learning) e DL (Deep Learning) são utilizados na mídia quase como sinônimos. . Inteligência Artificial se refere ao campo da ciência da computação que se preocupa com o estudo de máquinas para realização de atividades ditas inteligentes. Esse é um grande campo com diferentes &quot;escolas&quot;/&quot;ramificações&quot;. Inteligência é um conceito muito amplo e bastante subjetivo. . Uma grande pergunta é como construir máquinas inteligentes? . Machine Learning se refere ao subcampo da IA que busca obter máquinas inteligentes através da extração de estrutura e inteligência (padrões) de dados (experiência). . &quot;Um computador aprende a partir da experiência E com respeito a alguma tarefa T e alguma medida de performance P, se sua performance em T melhora com sua experiência E&quot; - Tom Mitchell (1998) . A ideia de ML é que, para realizar uma tarefa, nós (programadoras(es)) não implementemos as regras que definem as saídas para uma dada entrada. Imagine que queremos traduzir qualquer frase do inglês para português, quais regras iremos utilizar? Listar todas as traduções e regras possíveis é não trivial. . “A complexidade de programas de computadores convencionais está no código (programas que as pessoas escrevem). Em Machine Learning, algoritmos são em princípio simples e a complexidade (estrutura) está nos dados. Existe uma maneira de aprender essa estrutura automaticamente? Esse é o princípio fundamental de Machine Learning.”- Andrew Ng Ao invés de listar as regras e funcionamento do sistema podemos utilizar um modelo de Machine Learning, e mostrar para o modelo vários exemplos de frases em inglês traduzidas para português. Padrões são extraídos dos dados e utilizados para que o modelo defina como tomar suas próprias decisões de modo a otimizar a tradução. . Modelos de Machine Learning extraem padrões dos dados, e claro são extremamente dependentes da qualidade dos dados para sucesso em suas tarefas, mas também depende de como os dados são representados. Por exemplo, como um modelo de ML deve representar uma frase? . . Seria muito bom não ter que se preocupar com a representação dos dados... Tipo, dar para o modelo uma representação bastante simplificada e ele que se preocupe em encontrar algum sentindo nesse dado bruto... . Seria e é possível!! Essa abordagem é conhecida como Representation Learning, o modelo deve ser capaz não só de resolver a tarefa (exemplo: tradução), mas também de encontrar representações úteis para os dados de modo a resolver essa tarefa. Afinal, a gente não sabe explicar como resolve o problema, provavelmente também não sabemos qual a melhor representação para tal. . Deep Learning é um subcampo de ML que utiliza representation learning definindo representações mais complexas a partir de outras representações mais simples. . . O nome Deep vem das múltiplas representações (camadas) que utilizamos para construir os modelos. Então, de maneira geral, quando pensamos em DL pensamos em um modelo com váááárias camadas, e também precisamos de muuuuitos dados. Métodos de DL tem fome de dados. Quer boas representações? Me dê vários exemplos de dados; Quer que eu saiba traduzir bem? Me dê vários exemplos de traduções já feitas que eu consigo extrair os padrões e aprender umas representações bem bacanas e ser um excelente tradutor! . Eu realmente deveria me importar com DL ou &#233; s&#243; hype? . Populariza&#231;&#227;o de DL . O termo Deep Learning começou a se popularizar em 2010~2012 quando técnicas de DL foram aplicadas com sucesso para a tarefa de classificação de imagens num desafio chamado &quot;Imaginet Challenge&quot; que consiste em classificar corretamente imagens em 1000 diferentes categorias. . . Desde então modelos de DL são a melhor coisa para essa tarefa e só ficam melhor. Na imagem acima é mostrado o erro do vencedor do desafio em cada ano. Sendo as barras azuis modelos de DL. . Mas... redes neurais (cora&#231;&#227;o do DL) existem desde 1930, por que s&#243; agora? . Dois grandes fatores contribuiram para o sucesso de métodos de DL e sua popularização: . Muitos dados disponívels | Hardware disponível | Com esses fatores foi possível explorar redes neurais com muito mais camadas do que antes e assim DL se tornou imparável e se firmou como estado na arte não só na tarefa de classificação de imagens mas numa esmagadora gama de tarefas e áreas. . Hoje em dia todo mundo que é legal ta usando Deep Learning. . Ta... mas &#233; hype ou n&#227;o &#233;? . O hype é real, mas o avanço também! . Ainda existem vários problemas em abertos, porém a área de DL está em constante evolução! Cada dia que passa novas técnicas surgem e mais resultados surpreendentes são alcançados. . Aprender sobre DL e seus conceitos básicos é fundamental para acompanhar, entender e contribuir para esse avanço! . Por que eu deveria me importar? . ML e DL estão mudando como se constrói e se pensa sobre software! | DL é muito legal, se você entender de DL você se torna mais legal e interessante por tabela :D | Mas sério, a razão 1. é realmente muito relevante, leia esse blog) e entenda como DL está mudando como construimos e construiremos software. . Refer&#234;ncias . Este conteúdo é baseado nos seguintes materiais: . Machine Learning in Formal Verification, FMCAD 2016 tutorial - Manish Pandey | Deep Learning Book, capítulo 1 - GoodFellow et Al. | Deep Learning 101 - Part 1: History and Background - Andrew L. Beam | .",
            "url": "https://hereismari.github.io/codando-deep-learning-jax/2021/01/21/intro_a_deep_learning.html",
            "relUrl": "/2021/01/21/intro_a_deep_learning.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "História das Redes Neurais",
            "content": "O come&#231;o de tudo . A história das redes neurais é curiosa, engraçada e triste ao mesmo tempo. Então, senta que lá vem história... . Certo dia, em 1958, um certo cientista nova-yorkino, chamado Frank Rosenblatt, inspirado pelo trabalho de seus colegas que estudavam sobre o cérebro humano, terminou de projetar um algoritmo que conseguia aprender sozinho a resolver problemas simples. Nascia ali, o Perceptron (que veremos mais adiante no curso). . A ideia inicial do Perceptron, na verdade, era ser uma máquina ao invés de um programa. E foi o que aconteceu. Utilizando o poderosíssimo IBM 704 que fazia 12 mil adições por segundo (só para você ter uma noção, um iPhone 7 faz 300 bilhões) os cientistas construíram uma máquina denominada Mark I Perceptron. . . A ideia do Mark I Perceptron era que, para uma dada imagem de entrada, a máquina deveria aprender sozinha a classificar a saída em 0 ou 1. . Mas... 1958? Nem imagens existiam naquela época! Então, como projetaram isso? . Os cientistas na época conectaram 400 foto-células a entrada do Mark I para simular uma imagem 20x20 (um pouco maior talvez do que as letras desse texto). E para permitir a calibração da máquina, cada entrada dessa era conectada a um potenciômetro que eram ajustados automaticamente por motores elétricos para mapear as entradas na saída final 0 ou 1. . The winter is coming... &#128532; . O Perceptron, na época, foi um sucesso! Afinal de contas, ele foi um dos primeiros algoritmos capazes de aprender sozinho. Além disso, já foi provado que o Perceptron tem garantia de sucesso quando as duas classes são linearmente separáveis. Porém, é aí que está o problema: o Perceptron nada mais é que um classificador binário linear. Ou seja, ele só funciona quando o seu problema é binário (2 classes) e seus dados podem ser separados por uma simples reta. Essas condições, no mundo real, são muito difíceis de acontecer. Só para exemplificar, como você separia os dados abaixo com apenas uma reta? . . Fonte da imagem . Pois é. O Perceptron também não consegue resolver esse simples problema (para quem não reparou, o gráfico acima representa a porta XOR). Na época, isso desanimou tanto os estudiosos da área de Inteligência Artificial, que pesquisas nessa área só foram retomadas de fato na década de 80. Tal período ficou conhecido como o inverno da Inteligência Artificial (AI Winter). . O salvador da p&#225;tria: Geoffrey Hinton . O maior problema do Perceptron era que ele era apenas um só. Intuitivamente, é fácil perceber que um neurônio só não faz uma rede neural (assim como uma andorinha só não faz verão). Pensando assim, muita gente tentou colocar um monte de Perceptrons conectados entre si para tentar resolver um problema. Porém, muitos do que fizeram isso se depararam com um problema: como propagar o erro da saída para as entradas? Em outras palavras, como fazer esse monte de Perceptrons aprenderem ao mesmo tempo sem que um atrapalhe o que o outro aprendeu?&quot;. . Pensando nisso, o famoso Geoffrey Hinton desenvolveu o algoritmo backpropagation na década de 80. Utilizando o conceito de gradientes e regra da cadeia, tal algoritmo pega o erro de uma rede neural e propaga-o até as entradas, fazendo leves ajustes nos parâmetros (pesos) da rede. Com isso, Redes Neurais com mais de um neurônio e mais de uma camada poderiam começar a ser desenvolvidas e treinadas em problemas mais complexos. Como eu disse, poderiam... . O problema era que na década de 80, e mesmo na década de 90, o treinamento de tais redes e aplicação do backpropagation era muito pesado ainda. Mesmo supercomputadores se matavam para treinar e executar tais redes ainda. Então, o que fazer? . Obrigado, gamers! . Mais uma vez o mundo foi salvo graças aos gamers. Isso mesmo. Essa obsessão dos gamers em sempre querer computadores mais potentes e jogos com gráficos cada vez melhores, fez com que a indústria dos computadores, especialmente a das GPUs se desenvolvessem num ritmo assustador - regido pela Lei de Murphy. Mas, o que os jogos têm a ver com o desenvolvimento das Redes Neurais? . A resposta é: matrizes! Como vamos ver num dos próximos assuntos, Redes Neurais tem tudo a ver com matrizes. Basicamente, Redes Neurais fazem um monte de cálculo sobre matrizes, como: soma, multiplicão, operaçõe ponto-a-ponto, etc... E, como GPUs são computadores especializados em cálculos sobre matrizes, o campo das Redes Neurais pode se desenvolver como nunca. Cada vez mais, redes mais complexas e pesadas puderam ser desenvolvidas e treinadas. . . Agora que já tivemos uma introdução sobre a história das Redes Neurais, chegou a hora de aprendermos mais sobre elas. Então, prepara um café que chegou a hora de começar os estudos... . Refer&#234;ncias . Este conteúdo é baseado nos seguintes materiais: . Capítulo 3 de Grokking Deep Learning. | Perceptron da Wikipedia | Frank Rosenblatt da Wikipedia | Geoffrey Hinton da Wikipedia | Backpropagation da Wikipedia | .",
            "url": "https://hereismari.github.io/codando-deep-learning-jax/2021/01/21/historia_das_redes_neurais.html",
            "relUrl": "/2021/01/21/historia_das_redes_neurais.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Glossário",
            "content": ". Acur&#225;cia . Quanto porcento da base de dados avaliamos corretamente. . Exemplo: . Digamos que temos 3 exemplos na nossa base de dados, onde esperamos os seguintes valores de saída para cada exemplo [0, 1, 1]. . Temos: . acurácia = 1, apenas se acertamos tudo: [0, 1, 1]. acurácia = 0.66, se comparamos esses valores com [1, 1, 1] (acertamos as duas últimas saídas). acurácia = 0.33, se comparamos esses valores com [1, 0, 1] (acertamos apenas a última saída). acurácia = 0, se errarmos todas as saídas: [1, 0, 0]. . &lt;a id=&#39;atributo&#39;&gt;&lt;/a&gt; . File &#34;&lt;ipython-input-1-f8315d3082fd&gt;&#34;, line 1 &lt;a id=&#39;atributo&#39;&gt;&lt;/a&gt; ^ SyntaxError: invalid syntax . Atributo . Um aspecto que distíngue um elemento específico de outros elementos em um dado contexto. . Exemplo 1: . Digamos que você quer comprar um carro, neste contexto alguns dos atributos do carro seriam: cor, tamanho, preço, número de portas, se tem ou não ar-condicionado, etc. . Exemplo 2: . Digamos que você perdeu seu carro em um estacionamento, neste contexto alguns dos atributos relevantes do carro seriam: cor, tamanho, se tem ou não um adesivo escrito &quot;MENGÃO&quot;, placa, etc. . . Base de dados Iris . A base de dados Iris foi publicada originalmente no UCI Machine Learning Repository e é bastante utilizada em tutoriais por sua simplicidade. . É uma pequena base contendo 150 exemplos de plantas de 3 diferentes espécies (setosa, versicolour e virginica). É bastante utilizada para a tarefa de classificação onde nos baseando nos atributos tentamos classificar um exemplo como uma das espécies: setosa, versicolour ou virginica. . Atributos: comprimento da sépala (centímetros) | largura da sépala (centímetros) | comprimento da petala (centímetros) | largura da pétala (centímetros) | | . Classes: Iris Setosa | Iris Versicolour | Iris Virginica | | Imagem de: https://www.datacamp.com/community/tutorials/machine-learning-in-r .",
            "url": "https://hereismari.github.io/codando-deep-learning-jax/2021/01/21/glossario.html",
            "relUrl": "/2021/01/21/glossario.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "1.7 Inicialização dos Pesos",
            "content": "Bem vind@s ao mundo estoc&#225;stico! . Vimos que redes neurais são um formadas por um conjunto de parâmetros chamados de pesos e bias. . Não confunda parâmetros com hiperparâmetros. Parâmetros são os valores que sua rede aprende (pesos e bias). Hiperparâmetros são os valores relacionados ao treinamento da sua rede:learning rate, número de camadas/neurônios, função de ativação de cada camada, etc... Nós também vimos que os pesos e bias são, em geral, inicializados com valores aleatórios. Por conta disso, é muito díficil que duas redes distintas chegem no mesmo resultado. Além disso, nós nunca teremos certeza que tal resultado é o melhor possível (mínimo global) e, dependendo dos valores iniciais, uma rede pode nunca convergir. . Claro que, para evitar alguns desses problemas, nós podemos mexer nos hiperparâmetros como learning rate, o otimizador, adicionar momentum, entre outros. Se você não conhece sobre eles ainda, não se preocupe! Nós vamos falar sobre eles mais a frente nesse curso. Outra alternativa é inicializar os pesos de diferentes maneiras. E aí, vamos aprender sobre elas? . Inicializa&#231;&#227;o com Zeros . &quot;Eu prefiro lutar em 0 graus, pois não é nem quente, nem frio!&quot; Maguila (ex-lutador de boxe) . Baseado nesse pensamento, muitas pessoas já pensaram em inicializar os pesos com zero. Dessa forma, todos os pesos começarão com valores iguais, nenhum sendo mais importante que o outro, nem dando mais importância para um determinado atributo dos seus dados. Além disso, é muito simples fazer isso com numpy: . import numpy as np pesos = np.zeros(shape=(3, 3)) pesos . array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]) . Parece uma boa ideia, né? **Errado!** Essa é pior coisa que se pode fazer ao inicializar os pesos de uma rede neural! Parando para pensar um pouco melhor, fazendo isso nós vamos zerar todos os neurônios bem como os gradientes de aprendizagem. Se os gradientes são zeros, como a rede vai aprender alguma coisa, então? Vale salientar que seus gradientes podem tender a zero por outras razões (que nós veremos depois). Mas, quando isso acontece, chamamos esse fenômeno de desaparecimento dos gradientes (gradient vanishing). . Sem contar que você será mal visto pela sua família e amigos se inicializar seus pesos com zeros. Em outras palavras, não pense como o Maguila e não faça isso nunca! . Por outro lado é uma inicialização bem comum para os biases . Inicializa&#231;&#227;o com 1s . Nesse momento, você deve estar pensando: &quot;Tá bom, tá bom. Eu já entendi que inicializar todos os pesos com zero é ruim. Mas, e se inicializamos todos os pesos com 1s, então?&quot; . A resposta é: é tão ruim quanto! Pensando um pouquinho, lembra que na primeira camada todos os neurônios recebem as mesmas entradas? Então, se todos os neurônios receberam as mesmas entradas e todos eles tem o mesmo valor de peso (=1), o que que vai acontecer? Isso mesmo: todos os neurônios vão dar o mesmo resultado! . Ainda nessa linha de pensamento, como todos os neurônios vão da próxima camada vão receber o resultado de cada neurônio dessa camada (que são todos iguais), a mesma coisa vai se repetir. No fim das contas, você pode até ter um pouquinho de gradiente, mas os gradientes para os neurônios de uma mesma camada vão ser iguais. Assim, todos os seus pesos se manterão iguais para sempre! Nada adianta ter uma camada com milhões de pesos se todos eles são idênticos. . Resumindo até aqui:não inicialize os seus neurônios com o mesmo valor, pior ainda se esse valor for igual a zero! (Novamente vale ressaltar que inicializar os viéses dessa maneira não é um problema) . Se você entendeu isso, deve ter percebido que a melhor maneira de inicializar pesos é dando valores aleatórios para cada um deles. A pergunta agora é: será que é melhor uma variância baixa, alta? Será que a média deve ser zero? É melhor uma distribuição normal ou uniforme? . É isso que vamos ver nas próximas seções. . Inicializa&#231;&#227;o aleat&#243;ria uniforme . Inicializar os pesos aleatórios de forma uniforme (mesma probabilidade para todo mundo) é fácil com numpy: . import matplotlib.pyplot as plt %matplotlib inline pesos_uniforme = 10*np.random.rand(1000) - 5 # pesos entre -5 e 5 plt.hist(pesos_uniforme) plt.show() . Repare que como os valores dos pesos têm praticamente uma mesma probabilidade de ocorrência (distribuição uniforme). Pesos como essa distribuição são bons para quebrar a simetria da rede e vão fazer sua rede aprender alguma coisa. . Inicializa&#231;&#227;o aleat&#243;ria normal . pesos_normal = 5*np.random.randn(1000) + 3 # média = 3 e desvio-padrão = 5 plt.hist(pesos_normal, bins=40) plt.show() . Diferente da distribuição uniforme, os pesos agora são inicializados por uma distribuição normal, isto é, com certa média e desvio padrão. Ou seja, pesos mais próximos da média são mais comuns e quanto mais &quot;desvios-padrões&quot; além da média, menor a probabilidade. Da mesma forma que a distribuição uniforme, pesos com essa distribuição também são bons para quebrar a simetria da rede. . Inicializa&#231;&#227;o Glorot Uniforme . A gente viu que inicilizar os pesos com uma distribuição normal ou uniforme é uma boa ideia. Entretanto, elas apresentam um defeito: como definir a faixa de valores que meus pesos vão assumir? Em outras palavras, qual deve ser a variância dos meus pesos? Quanto menor sua variância, maiores são as chances de acontecer o vanishing dos gradientes - e a gente já viu que isso não é bom. Por outro lado, quando a variância é muito grande, o contrário acontece. Isto é, seus gradientes vão tender ao infinito. Nesse caso, damos o nome de explosão dos gradientes (exploding gradients). Portanto, tenha em mente o seguinte: . O que buscamos na inicialização de pesos são valores pequenos com variância ok (pequena, mas nem tanto) . Porém, não existe um número mágico para variância dos pesos. Ela vai depender principalmente da quantidade de pesos que a sua rede tem, ou seja, depende da arquitetura da sua rede. O ideal seria que a variância dos pesos fosse definida automaticamente. Ahhh, ia ser tão bom se existisse um método que fizesse isso para gente, né? E existe!!! . A Glorot Uniforme, também conhecida como Xavier Uniforme, é uma distribuição uniforme que leva em consideração a quantidade de neurônios da camada atual e anterior da sua rede. Na prática, os pesos são inicializados baseados na seguinte fórmula:$$w = 2* sigma*rand() - sigma$$ onde, . $$ sigma = sqrt{ frac{6}{in + out}}$$ . Supondo que a camada atual tem 5 neurônios e a camada anterior tinha 3, a implementação em Python ficaria dessa forma: . np.random.seed(42) # para que o seu resultado seja igual ao meu :) sigma = np.sqrt(6 / (5 + 3)) pesos = 2 * sigma * np.random.rand(5, 3) - sigma print(pesos) . [[-0.21730289 0.78066008 0.40182529] [ 0.17088151 -0.59579319 -0.59583497] [-0.76542164 0.63423569 0.17513634] [ 0.36039228 -0.83037201 0.81390774] [ 0.57580754 -0.49824328 -0.55109532]] . Inicializa&#231;&#227;o Glorot Normal . A Glorot Normal, também conhecida como Xavier Normal, é bem parecida com a sua irmã uniforme. A diferença, na verdade, é que agora vamos fazer a amostragem dos pesos baseado numa distribuição normal (não diga!), também levando em consideração a quantidade de neurônios da camada atual e anterior: . $$w = sigma * randN()$$ . também teremos uma pequena diferençazinha no $ sigma$: . $$ sigma = sqrt{ frac{2}{in + out}}$$ . Em Python, considerando a mesma rede com 5 e 3 neurônios: . np.random.seed(42) # mesma semente do exemplo anterior para gente comparar os pesos sigma = np.sqrt(2 / (5 + 3)) pesos = sigma * np.random.randn(5, 3) print(pesos) . [[ 0.24835708 -0.06913215 0.32384427] [ 0.76151493 -0.11707669 -0.11706848] [ 0.78960641 0.38371736 -0.23473719] [ 0.27128002 -0.23170885 -0.23286488] [ 0.12098114 -0.95664012 -0.86245892]] . Pronto! Agora você sabe tudo sobre inicialização de pesos em redes neurais. Provavelmente, a sua única dúvida na cabeça agora é: da onde vem o $2$ e $6$ nas fórmulas do $ sigma$ das distribuições Glorot? No artigo original, os autores assumem uma função de ativação simétrica (na época era comum usar sigmóides e tangentes hiperbólicas) e concluem que a variância ótima para a geração de saídas é $1/in$, enquanto a ideal para a backpropagation é $1/out$, como um compromisso eles tomam a variância como sendo o inverso da média $2/(in + out)$. O 6 basicamente vem do fato que a variância de uma distribuição uniforme é $(max - min)^2/12$, como a média é 0, min = -max, $ frac{4max^2}{12} = 2/(in + out) implies max = sqrt( frac{6}{in + out})$ .",
            "url": "https://hereismari.github.io/codando-deep-learning-jax/2021/01/21/1.7.inicializacao_dos_pesos.html",
            "relUrl": "/2021/01/21/1.7.inicializacao_dos_pesos.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "1.5 Introdução a Redes Neurais: minha primeira Rede Neural <3",
            "content": "Depend&#234;ncias . Gerando dados sint&#233;ticos . $Y = 7 * X + 15$ . SYNT_TRAIN_SIZE = 200 # controla o quão espalhados são os dados STD_DEV = 0.7 def random_error(size, mu=0, std_dev=0.5): return np.random.normal(mu, std_dev, size) def add_batch_dim(tensor): if len(tensor.shape) == 1: return np.expand_dims(tensor, axis=1) else: return tensor def remove_batch_dim(tensor): return np.squeeze(tensor, axis=1) def generate_x(size, use_batch_dim=True): x = np.random.rand(size) if use_batch_dim: x = add_batch_dim(x) return x def plot_line(x, y, style=&#39;-b&#39;): x, y = remove_batch_dim(x), remove_batch_dim(y) return plt.plot([min(x), max(x)], [min(y), max(y)], style) def generate_f(x, a=7, b=15, error_std_dev=0.5, use_batch_dim=True): y = a * x + b + random_error(x.shape, std_dev=error_std_dev) if use_batch_dim: y = add_batch_dim(y) return y # gera valores aleatórios para x synt_x = generate_x(SYNT_TRAIN_SIZE) # gera a funcão: Y = 7 * X + 15 synt_y = generate_f(synt_x, error_std_dev=STD_DEV) . plt.plot(synt_x, synt_y, &#39;ro&#39;, alpha=0.4) plot_line(synt_x, synt_x * 7 + 15) plt.show() . Implementando Rede Neural . class NeuralNetwork(object): def __init__(self, layers=[1], input_size=1, activations=[None]): assert len(layers) == len(activations) self.input_size = input_size self.layers = layers self.activations, self._act_devs = self.get_act(activations) self.weights, self.biases = self.define_params() self._current_batch = [] def get_act(self, act_names): def _no_act(x): return x def _dev_no_act(x): return np.ones(x.shape) def _sigmoid(x): return 1 / (1 + np.exp(-x)) def _dev_sigmoid(x): return x * (1 - x) def _relu(x): return np.maximum(1e-15, x) def _dev_relu(x): return (x &gt; 0) * 1.0 activations = [] act_devs = [] for act_name in act_names: if act_name is None: act, dev_act = _no_act, _dev_no_act elif act_name == &#39;sigmoid&#39;: act, dev_act = _sigmoid, _dev_sigmoid elif act_name == &#39;relu&#39;: act, dev_act = _relu, _dev_relu else: raise ValueError(&#39;Activation function is not valid: %s&#39; % act_name) activations.append(act) act_devs.append(dev_act) return activations, act_devs def define_params(self): &#39;&#39;&#39;He-et-all initialization&#39;&#39;&#39; weights = [] biases = [] for i, (in_dim, out_dim) in enumerate(zip([self.input_size] + self.layers, self.layers)): weights.append(np.random.randn(in_dim, out_dim) * np.sqrt(2/in_dim)) biases.append(np.random.randn(out_dim) * np.sqrt(2/in_dim)) return weights, biases def update_params(self, gradients, learning_rate=0.1): assert len(gradients) == len(self.weights), (len(gradients), len(self.weights)) assert len(gradients) == len(self.biases), (len(gradients), len(self.biases)) for i, grad in enumerate(gradients[::-1]): assert grad[&#39;weights&#39;].shape == self.weights[i].shape self.weights[i] -= learning_rate * grad[&#39;weights&#39;] self.biases[i] -= learning_rate * grad[&#39;biases&#39;] def run_batch(self, batch): self._current_batch = [batch] for i, (w, b) in enumerate(zip(self.weights, self.biases)): output = np.dot(self._current_batch[-1], w) + b output = self.activations[i](output) self._current_batch.append(output) self._current_batch = self._current_batch[::-1] return output . Implementando SGD . class Trainer(object): def __init__(self, model, learning_rate = 0.01, loss_name=&#39;l2&#39;, print_mod=1000, verbose=True): def _accuracy(pred_y, real_y): p = np.argmax(self.softmax(pred_y), axis=1) return np.sum(p == real_y) / len(pred_y) self.model = model self.loss_name = loss_name self.learning_rate = learning_rate self.loss, self.loss_dev = self._define_loss() self.train_step = 0 self.eval_steps = [] self.verbose = verbose self.print_mod = print_mod self.train_losses = [] self.eval_losses = [] self._metrics = { &#39;accuracy&#39;: _accuracy } def softmax(self, x): exps = np.exp(x) return (exps / np.sum(exps, axis=1, keepdims=True)) def _define_loss(self): def _l2(pred_y, real_y): n = len(pred_y) return (1.0/2) * (1.0/n) * np.sum(np.power(pred_y - real_y, 2)) def _l2_dev(pred_y, real_y): n = len(pred_y) return (pred_y - real_y) * (1.0/n) def _cross_entropy(pred_y, real_y): m = real_y.shape[0] p = self.softmax(pred_y) # We use multidimensional array indexing to extract # softmax probability of the correct label for each sample. # Refer to https://docs.scipy.org/doc/numpy/user/basics.indexing.html#indexing-multi-dimensional-arrays for understanding multidimensional array indexing. log_likelihood = -np.log(p[range(m), real_y.astype(int)]) loss = np.sum(log_likelihood) / m return loss def _cross_entropy_dev(pred_y, real_y): m = real_y.shape[0] grad = self.softmax(pred_y) grad[range(m), real_y.astype(int)] -= 1 grad = grad / m return grad if self.loss_name == &#39;l2&#39;: return _l2, _l2_dev elif self.loss_name == &#39;cross-entropy&#39;: return _cross_entropy, _cross_entropy_dev else: raise ValueError(&#39;Invalid loss name: %s&#39; % self.loss_name) def train(self, batch_x, batch_y): self.train_step += 1 # run feed forward network pred_y = self.model.run_batch(batch_x) # save loss self.train_losses.append(self.loss(pred_y, batch_y)) # get gradients grads = self.generate_gradients(pred_y, batch_y, batch_x) # update parameters self.model.update_params(grads, self.learning_rate) if self.verbose and (self.train_step - 1) % self.print_mod == 0: print(&#39;Loss: %.4f for step %d&#39; % (self.train_losses[-1], self.train_step)) def eval(self, batch_x, batch_y, metrics=[]): # run feed forward network pred_y = self.model.run_batch(batch_x) # loss loss = self.loss(pred_y, batch_y) self.eval_losses.append(loss) # metrics res_metrics = [] for m in metrics: if m in self._metrics: res_metrics.append(self._metrics[m](pred_y, batch_y)) else: raise ValueError(&#39;Invalid metric: %s&#39; % m) self.eval_steps.append(self.train_step) return loss, res_metrics def plot_losses(self): if len(self.eval_losses) &gt; 0: plt.title(&#39;Train Loss: %.4f | Test Loss: %.4f for step %d&#39; % (self.train_losses[-1], self.eval_losses[-1], self.train_step)) else: plt.title(&#39;Train Loss: %.4f for step %d&#39; % (self.train_losses[-1], self.train_step)) plt.plot([i for i in range(self.train_step)], self.train_losses) plt.plot([i for i in self.eval_steps], self.eval_losses) def generate_gradients(self, pred_y, real_y, data_x): grad = [] input_size = pred_y.shape[0] j = len(self.model.activations) - 1 k = len(self.model.weights) - 1 dly = self.loss_dev(pred_y, real_y) * self.model._act_devs[j](self.model._current_batch[0]) dlx = np.dot(dly, self.model.weights[k].T) for i, (w, b) in enumerate(zip(self.model.weights[::-1], self.model.biases[::-1])): dlw = np.dot(self.model._current_batch[i+1].T, dly) dlb = np.sum(dly) # print(&#39;weight:&#39;, w.shape, &#39;bias:&#39;, b.shape) # print(&#39;dlw:&#39;, dlw.shape, &#39;dlb:&#39;, dlb.shape) # print(&#39;dly:&#39;, dly.shape, &#39;dlx:&#39;, dlx.shape) grad.append({ &#39;weights&#39;: dlw, &#39;biases&#39;: dlb }) j -= 1 k -= 1 if i != len(self.model.weights)-1: dly = dlx * self.model._act_devs[j](self.model._current_batch[i+1]) dlx = np.dot(dly, self.model.weights[k].T) return grad . Gradients . L2 loss with 1 layer, no activation . Loss . $$L = 1/2 * 1/n * sum{(y_i - ŷ_i)^{2}}$$ $$L = 1/2 * 1/n * sum{(y_i - w_i * x_i + b_i)^{2}}$$ . Gradients . $$ frac{ partial L}{ partial w_i} = 1/2 * 1/n * 2 * sum{(y_i - ŷ_i)} * frac{ partial {ŷ_i}}{ partial w_i} $$ $$ frac{ partial L}{ partial w_i} = 1/n * sum{(y_i - ŷ_i)} * x_i$$ . . $$ frac{ partial L}{ partial b_i} = 1/2 * 1/n * 2 * sum{(y_i - ŷ_i)} * frac{ partial {ŷ_i}}{ partial b_i} $$ $$ frac{ partial L}{ partial b_i} = 1/n * sum{(y_i - ŷ_i)} * 1$$ . L2 loss with 2 layers, relu activation in the hidden layer . Loss . $$L = 1/2 * 1/n * sum{(y_i - ŷ_i)^{2}}$$ $$L = 1/2 * 1/n * sum{(y_i - (w_j * x_j + b_j))^{2}}$$ $$x_j = relu(w_i * x_i + b_i)$$ . Gradients . $$ frac{ partial L}{ partial w_i} = 1/n * sum{(y_i - ŷ_i)} * x_j $$ $$ frac{ partial L}{ partial b_i} = 1/n * sum{(y_i - ŷ_i)} * 1$$ . $$ frac{ partial L}{ partial w_j} = 1/n * sum{(y_i - ŷ_i)} * x_j * x_i, se relu() &gt; 0$$ $$ frac{ partial L}{ partial b_j} = 1/n * sum{(y_i - ŷ_i)} * x_j, se relu() &gt; 0$$ . Treinando . nn = NeuralNetwork() t = Trainer(nn, verbose=False) for i in range(100000): t.train(synt_x, synt_y) t.plot_losses() . Comparando com a realidade . print(&#39;Parâmetros aprendidos:&#39;) print(&#39;pesos:&#39;, nn.weights) print(&#39;bias:&#39;, nn.biases) print(&#39;Função que modela os dados: 7 * X + 15&#39;) plot_line(synt_x, nn.run_batch(synt_x), &#39;--r&#39;) plot_line(synt_x, synt_y) plt.show() . Parâmetros aprendidos: pesos: [array([[6.69336918]])] bias: [array([15.07485933])] Função que modela os dados: 7 * X + 15 . Uma fun&#231;&#227;o um pouco mais complicada . $Y = 7 * log(x) + 1$ . def get_random_error(size, mu=0, std_dev=0.8): return np.random.normal(mu, std_dev, size) synt_x = np.random.rand(SYNT_TRAIN_SIZE) synt_y = np.reshape(7 * np.log(synt_x) + 1 + get_random_error(SYNT_TRAIN_SIZE), (SYNT_TRAIN_SIZE, 1)) synt_x = np.reshape(synt_x, (SYNT_TRAIN_SIZE, 1)) . plt.plot(synt_x, synt_y, &#39;ro&#39;, alpha=0.5) . [&lt;matplotlib.lines.Line2D at 0x7fedbb344e80&gt;] . nn = NeuralNetwork(layers=[10, 1], activations=[&#39;sigmoid&#39;, None]) t = Trainer(nn) . for i in range(10000): t.train(synt_x, synt_y) t.plot_losses() . Loss: 36.5701 for step 1 Loss: 2.5772 for step 1001 Loss: 1.6035 for step 2001 Loss: 1.3071 for step 3001 Loss: 1.1386 for step 4001 Loss: 1.0233 for step 5001 Loss: 0.9374 for step 6001 Loss: 0.8700 for step 7001 Loss: 0.8157 for step 8001 Loss: 0.7707 for step 9001 . print(&#39;Parâmetros aprendidos:&#39;) print(&#39;pesos:&#39;, nn.weights) print(&#39;bias:&#39;, nn.biases) print(&#39;Função que modela os dados: 7 * X + 15&#39;) plt.plot(synt_x, nn.run_batch(synt_x), &#39;or&#39;, alpha=0.3) plt.plot(synt_x, synt_y, &#39;og&#39;, alpha=0.3) plt.show() . Parâmetros aprendidos: pesos: [array([[ -5.36855323, -3.03145738, -5.21423527, -14.09143894, -0.83027733, -2.08127383, -1.48456986, 2.56702234, -3.32490432, 4.39467954]]), array([[ -6.11528773], [ -3.83950468], [ -6.11003906], [-14.98979137], [ -2.35375611], [ -2.63361214], [ -2.25129261], [ 1.55817272], [ -1.84865704], [ 4.95015308]])] bias: [array([ 0.47915267, -0.06928394, -0.20321051, 0.77060138, 2.57464764, -1.61306825, -0.24671431, 0.04661517, -2.91871913, -2.46777321]), array([-2.53449749])] Função que modela os dados: 7 * X + 15 . E se os dados forem n&#227;o lineares? . xor_x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) xor_y = np.array([[0], [1], [1], [0]]) . nn = NeuralNetwork(layers=[10, 2], input_size=2, activations=[&#39;relu&#39;, None]) t = Trainer(nn, verbose=False) for i in range(100000): t.train(xor_x, xor_y) t.plot_losses() . plt.plot(xor_x, nn.run_batch(xor_x), &#39;bo&#39;, xor_x, xor_y, &#39;ro&#39;, alpha=0.3) . [&lt;matplotlib.lines.Line2D at 0x7fedbab47400&gt;, &lt;matplotlib.lines.Line2D at 0x7fedbab47550&gt;, &lt;matplotlib.lines.Line2D at 0x7fedbab476a0&gt;, &lt;matplotlib.lines.Line2D at 0x7fedbab47c18&gt;] . nn = NeuralNetwork(layers=[10, 1], input_size=2, activations=[None, None]) t = Trainer(nn, verbose=False) for i in range(100000): t.train(xor_x, xor_y) t.plot_losses() . plt.plot(xor_x, nn.run_batch(xor_x), &#39;bo&#39;, xor_x, xor_y, &#39;ro&#39;, alpha=0.3) . [&lt;matplotlib.lines.Line2D at 0x7fedba861198&gt;, &lt;matplotlib.lines.Line2D at 0x7fedba8612e8&gt;, &lt;matplotlib.lines.Line2D at 0x7fedba861438&gt;, &lt;matplotlib.lines.Line2D at 0x7fedba8619b0&gt;] . Exemplo: base dados Iris . Digamos que para um exemplo da base de dados queremos determinar qual a espécie dessa planta. . Entradas . A base de dados iris tem 4 atributos de uma planta que iremos usar como entrada. . Saídas . Neste caso a saída que nos interessa é a espécie da planta. Então digamos que a saída é um número que indica qual a espécie: . 0 = Iris Setosa , 1 = Iris Versicolour, 2 = Iris Virginica . Obtendo os dados . iris = fetch_mldata(&#39;iris&#39;) # np.c_ concatena as features e targets do dataset iris_data = pd.DataFrame(data=np.c_[iris[&#39;data&#39;], iris[&#39;target&#39;]], columns=[&#39;x0&#39;, &#39;x1&#39;, &#39;x2&#39;, &#39;x3&#39;, &#39;target&#39;]) . /home/marianne/anaconda3/envs/syft/lib/python3.6/site-packages/sklearn/utils/deprecation.py:85: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22. Please use fetch_openml. warnings.warn(msg, category=DeprecationWarning) /home/marianne/anaconda3/envs/syft/lib/python3.6/site-packages/sklearn/utils/deprecation.py:85: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22. Please use fetch_openml. warnings.warn(msg, category=DeprecationWarning) . iris_data.head() . x0 x1 x2 x3 target . 0 -0.555556 | 0.250000 | -0.864407 | -0.916667 | 1.0 | . 1 -0.666667 | -0.166667 | -0.864407 | -0.916667 | 1.0 | . 2 -0.777778 | 0.000000 | -0.898305 | -0.916667 | 1.0 | . 3 -0.833333 | -0.083333 | -0.830508 | -0.916667 | 1.0 | . 4 -0.611111 | 0.333333 | -0.864407 | -0.916667 | 1.0 | . iris_data.describe() . x0 x1 x2 x3 target . count 150.000000 | 150.000000 | 150.000000 | 1.500000e+02 | 150.000000 | . mean -0.142593 | -0.121667 | -0.064859 | -8.444446e-02 | 2.000000 | . std 0.460037 | 0.361329 | 0.598109 | 6.359674e-01 | 0.819232 | . min -1.000000 | -1.000000 | -1.000000 | -1.000000e+00 | 1.000000 | . 25% -0.555556 | -0.333333 | -0.796610 | -8.333330e-01 | 1.000000 | . 50% -0.166667 | -0.166667 | 0.135593 | -4.035730e-08 | 2.000000 | . 75% 0.166667 | 0.083333 | 0.389830 | 4.166670e-01 | 3.000000 | . max 1.000000 | 1.000000 | 1.000000 | 1.000000e+00 | 3.000000 | . iris_data.drop([&#39;target&#39;], axis=1).diff().hist(color=&#39;k&#39;, alpha=0.5, bins=10, figsize=(4, 5)) plt.show() . x = iris.data y = iris.target x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42) . def batches(x, y, batch_size=True): idx = np.random.permutation(len(x)) x = x[idx] y = y[idx] for i in range(0, len(x)-batch_size-1, batch_size): batch_x = x[i:i+batch_size] batch_y = y[i:i+batch_size] yield batch_x, batch_y . nn = NeuralNetwork(layers=[10, 4], input_size=4, activations=[&#39;relu&#39;, None]) t = Trainer(nn, verbose=False, loss_name=&#39;cross-entropy&#39;) for i in range(1000): for batch_x, batch_y in batches(x, y, 16): t.train(batch_x, batch_y) if i % 10 == 0: loss, metrics = t.eval(x_test, y_test, metrics=[&#39;accuracy&#39;]) print(&#39;Test loss = %.5f, accuracy %.5f&#39; % (loss, metrics[0])) t.plot_losses() . Test loss = 1.21398, accuracy 0.44737 Test loss = 1.19670, accuracy 0.52632 Test loss = 1.20644, accuracy 0.55263 Test loss = 1.20227, accuracy 0.55263 Test loss = 1.17978, accuracy 0.55263 Test loss = 1.12980, accuracy 0.55263 Test loss = 1.04272, accuracy 0.52632 Test loss = 0.90289, accuracy 0.63158 Test loss = 0.72344, accuracy 0.94737 Test loss = 0.57425, accuracy 0.94737 Test loss = 0.48888, accuracy 0.94737 Test loss = 0.44399, accuracy 0.94737 Test loss = 0.42637, accuracy 0.94737 Test loss = 0.41667, accuracy 0.94737 Test loss = 0.40954, accuracy 0.94737 Test loss = 0.40582, accuracy 0.94737 Test loss = 0.40292, accuracy 0.94737 Test loss = 0.39989, accuracy 0.94737 Test loss = 0.40108, accuracy 0.94737 Test loss = 0.40485, accuracy 0.92105 Test loss = 0.40582, accuracy 0.92105 Test loss = 0.40479, accuracy 0.92105 Test loss = 0.40645, accuracy 0.92105 Test loss = 0.40586, accuracy 0.92105 Test loss = 0.40694, accuracy 0.92105 Test loss = 0.40653, accuracy 0.92105 Test loss = 0.40742, accuracy 0.92105 Test loss = 0.40905, accuracy 0.92105 Test loss = 0.41011, accuracy 0.89474 Test loss = 0.41421, accuracy 0.89474 Test loss = 0.41587, accuracy 0.89474 Test loss = 0.41675, accuracy 0.89474 Test loss = 0.42038, accuracy 0.89474 Test loss = 0.42003, accuracy 0.89474 Test loss = 0.41858, accuracy 0.89474 Test loss = 0.42077, accuracy 0.89474 Test loss = 0.42187, accuracy 0.89474 Test loss = 0.42020, accuracy 0.89474 Test loss = 0.42129, accuracy 0.89474 Test loss = 0.42043, accuracy 0.89474 Test loss = 0.41884, accuracy 0.89474 Test loss = 0.41788, accuracy 0.89474 Test loss = 0.41868, accuracy 0.89474 Test loss = 0.41957, accuracy 0.89474 Test loss = 0.41866, accuracy 0.89474 Test loss = 0.41977, accuracy 0.89474 Test loss = 0.41721, accuracy 0.89474 Test loss = 0.41759, accuracy 0.89474 Test loss = 0.41893, accuracy 0.89474 Test loss = 0.42085, accuracy 0.89474 Test loss = 0.41991, accuracy 0.89474 Test loss = 0.42016, accuracy 0.89474 Test loss = 0.41809, accuracy 0.89474 Test loss = 0.41918, accuracy 0.89474 Test loss = 0.42003, accuracy 0.89474 Test loss = 0.41890, accuracy 0.89474 Test loss = 0.41868, accuracy 0.89474 Test loss = 0.41820, accuracy 0.89474 Test loss = 0.42099, accuracy 0.89474 Test loss = 0.41767, accuracy 0.89474 Test loss = 0.42132, accuracy 0.89474 Test loss = 0.42207, accuracy 0.89474 Test loss = 0.42142, accuracy 0.89474 Test loss = 0.42151, accuracy 0.89474 Test loss = 0.42057, accuracy 0.89474 Test loss = 0.42098, accuracy 0.89474 Test loss = 0.41887, accuracy 0.89474 Test loss = 0.42033, accuracy 0.89474 Test loss = 0.42009, accuracy 0.89474 Test loss = 0.42172, accuracy 0.86842 Test loss = 0.42314, accuracy 0.86842 Test loss = 0.42426, accuracy 0.86842 Test loss = 0.42466, accuracy 0.86842 Test loss = 0.42567, accuracy 0.86842 Test loss = 0.42706, accuracy 0.86842 Test loss = 0.42778, accuracy 0.86842 Test loss = 0.42908, accuracy 0.86842 Test loss = 0.42779, accuracy 0.86842 Test loss = 0.43049, accuracy 0.86842 Test loss = 0.42742, accuracy 0.86842 Test loss = 0.42618, accuracy 0.86842 Test loss = 0.42585, accuracy 0.86842 Test loss = 0.42660, accuracy 0.86842 Test loss = 0.42408, accuracy 0.86842 Test loss = 0.42237, accuracy 0.86842 Test loss = 0.42289, accuracy 0.86842 Test loss = 0.42527, accuracy 0.86842 Test loss = 0.42560, accuracy 0.86842 Test loss = 0.42581, accuracy 0.86842 Test loss = 0.42484, accuracy 0.86842 Test loss = 0.42468, accuracy 0.86842 Test loss = 0.42536, accuracy 0.86842 Test loss = 0.42508, accuracy 0.86842 Test loss = 0.42454, accuracy 0.86842 Test loss = 0.42427, accuracy 0.86842 Test loss = 0.42377, accuracy 0.86842 Test loss = 0.42520, accuracy 0.86842 Test loss = 0.42558, accuracy 0.86842 Test loss = 0.42519, accuracy 0.86842 Test loss = 0.42507, accuracy 0.86842 . MNIIST . mnist = fetch_mldata(&#39;MNIST original&#39;) . x = mnist.data / np.max(mnist.data) y = mnist.target x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42) . nn = NeuralNetwork(layers=[512, 256, 10], input_size=784, activations=[&#39;relu&#39;, &#39;relu&#39;, None]) t = Trainer(nn, verbose=False, loss_name=&#39;cross-entropy&#39;, learning_rate=0.001) for i in range(20): for batch_x, batch_y in batches(x, y, 64): t.train(batch_x, batch_y) if i % 1 == 0: loss, metrics = t.eval(x_test, y_test, metrics=[&#39;accuracy&#39;]) print(&#39;Test loss = %.5f, accuracy %.5f&#39; % (loss, metrics[0])) t.plot_losses() . Em busca da fun&#231;&#227;o perfeita . Exemplo: base dados Iris . Digamos que para um exemplo da base de dados queremos determinar qual a espécie dessa planta. . Entradas . A base de dados iris tem 4 atributos de uma planta que iremos usar como entrada. . Saídas . Neste caso a saída que nos interessa é a espécie da planta. Então digamos que a saída é um número que indica qual a espécie: . 0 = Iris Setosa , 1 = Iris Versicolour, 2 = Iris Virginica . Obtendo a base de dados . import warnings warnings.filterwarnings(&#39;ignore&#39;) # Trabalhar com os dados from sklearn.datasets import fetch_mldata # Atributos: iris_dataset.data # Espécie: iris_dataset.target iris_dataset = fetch_mldata(&#39;iris&#39;) print(&#39;Numero de exemplos na base:&#39;, len(iris_dataset.data)) print(&#39;Atributos da primeira planta:&#39;, iris_dataset.data[0]) print(&#39;Especie da primeira planta:&#39;, iris_dataset.target[0]) . Vamos codar uma fun&#231;&#227;o que resolve esse problema! . Uma função para resolver esse problema precisa receber 4 parâmetros (cada um dos atributos da planta) e produzir uma saída (espécie da planta). . def f(x1, x2, x3, x4): # Não importa os atributos pra mim a resposta é sempre: Setosa!!!! return 0 print(f(*iris_dataset.data[0]), iris_dataset.target[0]) print(f(*iris_dataset.data[1]), iris_dataset.target[1]) print(f(*iris_dataset.data[-1]), iris_dataset.target[-1]) . A função acima é válida para este problema (4 entradas, 1 saída), o problema dela é que... Ela não ta ajudando a gente no nosso problema em nada! Ela simplesmente ignora os atributos e nos diz que qualquer exemplo é da espécie Setosa. . Como podemos avaliar o quão boa é essa função? Uma métrica possível é acurácia . Por exemplo: dado um banco com a altura de determinadas pessoas (entrada), queremos estimar o &quot;peso&quot; dessas pessoas. Nesse caso, o &quot;peso&quot; das pessoas é a variável que queremos estimar. Portanto, o &quot;peso&quot; nesse caso representaria a nossa saída. Sempre que a nossa saída é conhecida, nós dizemos que esse tipo de problema é um problema de Aprendizagem Supervisionada.Há casos em que não necessariamente o nosso problema tem uma saída explícita. Nesse caso, teremos uma Aprendizagem Não-Supervisionada. . Além disso, quando a saída assume qualquer valor real (0, 1.2, 3.14, -26, +34, ...), nós dizemos que temos um Problema de Regressão. Por outro lado, quando a saída é discreta (0/1, saudável/doente, cachorro/gato/passarinho), nós temos Problemas de Classificação. . A grande sacada é o que acontece dentro de f! A ideia é que não sabemos qual o melhor f possível, e poderíamos tentar várias funções para se ajustar aos dados. . Em geral, elas são matrizes $NxD$, onde $N$ (#linhas) representa o número de amostras que seu banco de dados tem e $D$ (#colunas) representa a quantidade de atributos de cada amostra, também conhecida por dimensionalidade. Como exemplo, imagine que tenhamos um banco de dados com 1.000 amostras e cada amostra tem 5 atributos. Logo, nossas entradas seriam representadas por uma matriz $1000x5$, sacou? . As entradas são representadas pelas amostras dos seus dados. Em geral, elas são matrizes $NxD$, onde $N$ (#linhas) representa o número de amostras que seu banco de dados tem e $D$ (#colunas) representa a quantidade de atributos de cada amostra, também conhecida por dimensionalidade. Como exemplo, imagine que tenhamos um banco de dados com 1.000 amostras e cada amostra tem 5 atributos. Logo, nossas entradas seriam representadas por uma matriz $1000x5$, sacou? . As saídas, por sua vez, representam o que você quer que a sua rede aprenda. Por exemplo: dado um banco com a altura de determinadas pessoas (entrada), queremos estimar o &quot;peso&quot; dessas pessoas. Nesse caso, o &quot;peso&quot; das pessoas é a variável que queremos estimar. Portanto, o &quot;peso&quot; nesse caso representaria a nossa saída. Sempre que a nossa saída é conhecida, nós dizemos que esse tipo de problema é um problema de Aprendizagem Supervisionada. Há casos em que não necessariamente o nosso problema tem uma saída explícita. Nesse caso, teremos uma Aprendizagem Não-Supervisionada. Além disso, quando a saída assume qualquer valor real (0, 1.2, 3.14, -26, +34, ...), nós dizemos que temos um Problema de Regressão. Por outro lado, quando a saída é discreta (0/1, homem/mulher, cachorro/gato/passarinho), nós temos Problemas de Classificação. .",
            "url": "https://hereismari.github.io/codando-deep-learning-jax/2021/01/21/1.5.minha_primeira_rede_neural.html",
            "relUrl": "/2021/01/21/1.5.minha_primeira_rede_neural.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "1.4 Funções de ativação",
            "content": "Depend&#234;ncias . import matplotlib.pyplot as plt # Matemática + manipulação de vetores import math import numpy as np # &quot;Fixar&quot; números aleatórios gerados np.random.seed(0) # Utilidades import utils # Recarregar automaticamente dependências caso elas mudem %load_ext autoreload %autoreload 2 # Ignorar warnings import warnings warnings.filterwarnings(&#39;ignore&#39;) np.seterr(all=&#39;raise&#39;) . {&#39;divide&#39;: &#39;warn&#39;, &#39;over&#39;: &#39;warn&#39;, &#39;under&#39;: &#39;ignore&#39;, &#39;invalid&#39;: &#39;warn&#39;} . x = np.linspace(-10, 10) . def plot_line(ax, x, y, *args, **kwargs): ax.plot(x, y, *args, **kwargs) def plot_activation_vis(f, x, name): fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4.5), tight_layout=True) # função de activação y = f(x) ax1.set_title(&#39;Funcao de ativacao: &#39; + name) plot_line(ax1, x, y) # derivada dy = f(x, derivative=True) ax2.set_title(&#39;Derivada da Funcao de ativacao: &#39; + name) plot_line(ax2, x, dy) plt.show() . Sem ativa&#231;&#227;o (Linear) . ... e sem comentários . def linear(x, derivative=False): if derivative: return np.ones(len(x)) return x plot_activation_vis(linear, x, &#39;linear&#39;) . ReLU . A ReLU é atualmente a função de ativação mais comumente usada para as camadas internas de redes neurais. Ela tem uma série de propriedades úteis, como ser barata de computar e ter um gradiente constante para valores positivos. . def relu(x, derivative=False): if derivative: return np.where(relu(x) &gt; 0, 1, 0) return np.where(x &gt; 0, x, 0) plot_activation_vis(relu, x, &#39;relu&#39;) . SELU . SELU é Scaled Exponential Linear Function, é uma função que aproxima a ReLU, porém ela não tem um gradiente nulo nos números negativos, evitando o problema da &quot;ReLU morta&quot;. . def selu(x, alpha=3, derivative=False): if derivative: return np.where(selu(x) &gt; 0, 1, alpha * np.exp(x)) return np.where(x &gt; 0, x, alpha * np.exp(x) - alpha) plot_activation_vis(selu, x, &#39;SeLU&#39;) . Sigmoid . A função sigmoíde costumava ser a mais usada para as camadas internas antigamente, em especial por ter interpretações matemáticas e biológicas. Hoje em dia ainda é usada na camada final para classificação binária . Porém a sigmóide tem um sério problema com seus gradientes, eles são extremamente pequenos fora do intervalo [-2, 2], e mesmo nesse intervalos não são muito altos, assim o treinamento de redes neurais profundas se torna muito lento ou até mesmo impossível, o &quot;vanishing gradient problem&quot;. . def sigmoid(x, derivative=False): if derivative: return sigmoid(x) * (1 - sigmoid(x)) return 1 / (1 + np.e ** (-x)) plot_activation_vis(sigmoid, x, &#39;Sigmoid&#39;) . Softmax . Softmax é uma função multivariada, ou seja ela recebe vários argumentos de entrada. O propósito dela é gerar uma distribuição de probabilidade a partir de um conjunto de valores. Desse modo ela é usada na camada final de uma rede neural quando queremos fazer classificação multiclasse. . Para transformarmos os valores em probabilidades fazemos os seguintes passos: . Exponenciamos os valores, garantindo que todos se tornem positivos (e que se $x_i &gt; x_j$, $ exp(x_i) &gt; exp(x_j)$ ). | Depois dividimos cada exponencial pela soma das exponenciais, garantindo que a soma dos termos finais seja 1. | Às vezes usam um parâmetro chamado temperatura, que suaviza a diferença entre as probabilidades. . def softmax(x, temperature=1): exps = np.exp(x/temperature) return exps/np.sum(exps) x = np.arange(5) - 2 print(x) fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4.5), tight_layout=True) y = softmax(x) ax1.set_title(&#39;Temperatura igual a 1&#39;) ax1.bar(x, y) y = softmax(x, 2) ax2.set_title(&#39;Temperatura igual a 2&#39;) ax2.bar(x, y) plt.show() . [-2 -1 0 1 2] . Tanh . Tem um formato e propriedades bastante semelhantes à sigmóide, porém seus valores variam entre -1 e 1, e seu gradiente é significativamente maior . def tanh(x, derivative=False): #ela já está implementada no numpy if derivative: return 1 - tanh(x)**2 return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x)) plot_activation_vis(tanh, x, &#39;Hyperbolic Tangent&#39;) . Imagem de: https://medium.com/machine-learning-world/how-to-debug-neural-networks-manual-dc2a200f10f2 .",
            "url": "https://hereismari.github.io/codando-deep-learning-jax/2021/01/21/1.4.funcoes_de_ativacao.html",
            "relUrl": "/2021/01/21/1.4.funcoes_de_ativacao.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "1.3 Introdução a Redes Neurais: como aprendem?",
            "content": "Depend&#234;ncias . Problema 1: regress&#227;o linear simples . Problema: Suponha que há n pontos de dados {xi, yi}, em que i = [1, 2, …, n]. O objetivo é encontrar a equação da reta $Y = X * a + b$ que proporcionaria o &quot;melhor&quot; ajuste para os dados. . Exemplo: imaginemos que xi é a temperatura em um certo dia e yi são quantos picolés são vendidos naquele dia... queremos saber como a mudança de temperatura influencia a venda de sorvetes. Vamos imaginar que esses valores se relacionam de maneira linear, isto é, existe uma equação que responde nossa pergunta, que é do tipo: $numSorvetesVendidosNoDia = temperaturaNoDia * a + b$. Queremos encontrar a e b. . A pergunta é: como encontrar a e b? . Vamos tentar algumas ideias! . Gerando dados sint&#233;ticos . Pegando o exemplo, do picolé... Espera-se que quanto mais alta a temperatura mais sorvetes sejam vendidos, isto é, esperamos que quando X cresça Y também cresça, digamos que a equação é a seguinte: . $Y = 70 * X + 15 + erro$ . Perceba que adicionaremos também um erro a equação, por que? No mundo real dificilmente X e Y irão estar relacionados de maneira perfeitamente linear, já que podem existir erros de medição ou outros motivos podem interferir no valor de Y. Por exemplo, pode ser que esteja muito quente porém aconteça um problema de distribuição de sorvetes ou esteja muito frio mas esteja acontecendo um festival de sorvetes na cidade... Enfim, o mundo é complicado! . PS:poderíamos ter escolhido qualquer valor para a e b. Escolhemos 70 e 15 de forma arbitrária. . . N = 200 # controla o erro STD_DEV = 170 def random_error(size, mu=0, std_dev=0.5): return np.random.normal(mu, std_dev, size) def add_batch_dim(tensor): if len(tensor.shape) == 1: return np.expand_dims(tensor, axis=1) else: return tensor def remove_batch_dim(tensor): return np.squeeze(tensor, axis=1) def generate_x(size, use_batch_dim=True, scale=40): x = np.random.rand(size) * scale if use_batch_dim: x = add_batch_dim(x) return x def plot_line(x, y, style=&#39;-b&#39;): x, y = remove_batch_dim(x), remove_batch_dim(y) return plt.plot([min(x), max(x)], [min(y), max(y)], style) def generate_f(x, a=70, b=15, error_std_dev=0.5, use_batch_dim=True): y = a * x + b + random_error(x.shape, std_dev=error_std_dev) if use_batch_dim: y = add_batch_dim(y) return y # gera valores aleatórios para x synt_x = generate_x(N) # gera a funcão: Y = 70 * X + 15 + erro synt_y = generate_f(synt_x, a=70, b=15, error_std_dev=STD_DEV) . plt.plot(synt_x, synt_y, &#39;ro&#39;, alpha=0.4) plt.xlabel(&quot;Temperatura&quot;) plt.ylabel(&quot;Número de sorvetes vendidos&quot;) plot_line(synt_x, synt_x * 70 + 15) plt.show() . No gráfico podemos ver que: quanto mais quente -&gt; mais sorventes são vedidos. . Implementando Rede Neural . class NeuralNetwork(object): def __init__(self, layers=[1], input_size=1, activations=[None]): assert len(layers) == len(activations) self.input_size = input_size self.layers = layers self.activations, self._act_devs = self.get_act(activations) self.weights, self.biases = self.define_params() self._current_batch = [] def get_act(self, act_names): def _no_act(x): return x def _dev_no_act(x): return np.ones(x.shape) def _sigmoid(x): return 1 / (1 + np.exp(-x)) def _dev_sigmoid(x): return x * (1 - x) def _relu(x): return np.maximum(1e-15, x) def _dev_relu(x): return (x &gt; 0) * 1.0 activations = [] act_devs = [] for act_name in act_names: if act_name is None: act, dev_act = _no_act, _dev_no_act elif act_name == &#39;sigmoid&#39;: act, dev_act = _sigmoid, _dev_sigmoid elif act_name == &#39;relu&#39;: act, dev_act = _relu, _dev_relu else: raise ValueError(&#39;Activation function is not valid: %s&#39; % act_name) activations.append(act) act_devs.append(dev_act) return activations, act_devs def define_params(self): &#39;&#39;&#39;He-et-all initialization&#39;&#39;&#39; weights = [] biases = [] for i, (in_dim, out_dim) in enumerate(zip([self.input_size] + self.layers, self.layers)): weights.append(np.random.randn(in_dim, out_dim) * np.sqrt(2/in_dim)) biases.append(np.random.randn(out_dim) * np.sqrt(2/in_dim)) print(&#39;Weight %d shape =&#39; % i, weights[i].shape) print(&#39;Bias %d shape =&#39; % i, biases[i].shape) return weights, biases def update_params(self, gradients, learning_rate=0.1): assert len(gradients) == len(self.weights), (len(gradients), len(self.weights)) assert len(gradients) == len(self.biases), (len(gradients), len(self.biases)) for i, grad in enumerate(gradients[::-1]): assert grad[&#39;weights&#39;].shape == self.weights[i].shape self.weights[i] -= learning_rate * grad[&#39;weights&#39;] self.biases[i] -= learning_rate * grad[&#39;biases&#39;] def run_batch(self, batch): self._current_batch = [batch] for i, (w, b) in enumerate(zip(self.weights, self.biases)): output = np.dot(self._current_batch[-1], w) + b output = self.activations[i](output) self._current_batch.append(output) self._current_batch = self._current_batch[::-1] return output . Implementando SGD . class Trainer(object): def __init__(self, model, learning_rate = 0.01, loss_name=&#39;l2&#39;, print_mod=1000, verbose=True): def _accuracy(pred_y, real_y): print(pred_y, real_y) p = np.argmax(self.softmax(pred_y), axis=1) return np.sum(p == real_y) / len(pred_y) self.model = model self.loss_name = loss_name self.learning_rate = learning_rate self.loss, self.loss_dev = self._define_loss() self.train_step = 0 self.eval_steps = [] self.verbose = verbose self.print_mod = print_mod self.train_losses = [] self.eval_losses = [] self._metrics = { &#39;accuracy&#39;: _accuracy } def softmax(self, x): exps = np.exp(x) return (exps / np.sum(exps, axis=1, keepdims=True)) def _define_loss(self): def _l2(pred_y, real_y): return (1.0/2) * np.mean(np.power(pred_y - real_y, 2)) def _l2_dev(pred_y, real_y): n = len(pred_y) return (pred_y - real_y) * (1.0/n) def _cross_entropy(pred_y, real_y): m = real_y.shape[0] p = self.softmax(pred_y) # We use multidimensional array indexing to extract # softmax probability of the correct label for each sample. # Refer to https://docs.scipy.org/doc/numpy/user/basics.indexing.html#indexing-multi-dimensional-arrays for understanding multidimensional array indexing. log_likelihood = -np.log(p[range(m), real_y.astype(int)]) loss = np.sum(log_likelihood) / m return loss def _cross_entropy_dev(pred_y, real_y): m = real_y.shape[0] grad = self.softmax(pred_y) grad[range(m), real_y.astype(int)] -= 1 grad = grad / m return grad if self.loss_name == &#39;l2&#39;: return _l2, _l2_dev elif self.loss_name == &#39;cross-entropy&#39;: return _cross_entropy, _cross_entropy_dev else: raise ValueError(&#39;Invalid loss name: %s&#39; % self.loss_name) def train(self, batch_x, batch_y): self.train_step += 1 # run feed forward network pred_y = self.model.run_batch(batch_x) # save loss self.train_losses.append(self.loss(pred_y, batch_y)) # get gradients grads = self.generate_gradients(pred_y, batch_y, batch_x) # update parameters self.model.update_params(grads, self.learning_rate) if self.verbose and (self.train_step - 1) % self.print_mod == 0: print(&#39;Loss: %.4f for step %d&#39; % (self.train_losses[-1], self.train_step)) def eval(self, batch_x, batch_y, metrics=[]): # run feed forward network pred_y = self.model.run_batch(batch_x) # loss loss = self.loss(pred_y, batch_y) self.eval_losses.append(loss) # metrics res_metrics = [] for m in metrics: if m in self._metrics: res_metrics.append(self._metrics[m](pred_y, batch_y)) else: raise ValueError(&#39;Invalid metric: %s&#39; % m) self.eval_steps.append(self.train_step) return loss, res_metrics def plot_losses(self): if len(self.eval_losses) &gt; 0: plt.title(&#39;Train Loss: %.4f | Test Loss: %.4f for step %d&#39; % (self.train_losses[-1], self.eval_losses[-1], self.train_step)) else: plt.title(&#39;Train Loss: %.4f for step %d&#39; % (self.train_losses[-1], self.train_step)) plt.plot([i for i in range(self.train_step)], self.train_losses) plt.plot([i for i in self.eval_steps], self.eval_losses) def generate_gradients(self, pred_y, real_y, data_x): grad = [] input_size = pred_y.shape[0] j = len(self.model.activations) - 1 k = len(self.model.weights) - 1 dly = self.loss_dev(pred_y, real_y) * self.model._act_devs[j](self.model._current_batch[0]) dlx = np.dot(dly, self.model.weights[k].T) for i, (w, b) in enumerate(zip(self.model.weights[::-1], self.model.biases[::-1])): dlw = np.dot(self.model._current_batch[i+1].T, dly) dlb = np.sum(dly) # print(&#39;weight:&#39;, w.shape, &#39;bias:&#39;, b.shape) # print(&#39;dlw:&#39;, dlw.shape, &#39;dlb:&#39;, dlb.shape) # print(&#39;dly:&#39;, dly.shape, &#39;dlx:&#39;, dlx.shape) grad.append({ &#39;weights&#39;: dlw, &#39;biases&#39;: dlb }) j -= 1 k -= 1 if i != len(self.model.weights)-1: dly = dlx * self.model._act_devs[j](self.model._current_batch[i+1]) dlx = np.dot(dly, self.model.weights[k].T) return grad . Gradients . L2 loss with 1 layer, no activation . Loss . $$L = 1/2 * 1/n * sum{(y_i - ŷ_i)^{2}}$$ $$L = 1/2 * 1/n * sum{(y_i - w_i * x_i + b_i)^{2}}$$ . Gradients . $$ frac{ partial L}{ partial w_i} = 1/2 * 1/n * 2 * sum{(y_i - ŷ_i)} * frac{ partial {ŷ_i}}{ partial w_i} $$ $$ frac{ partial L}{ partial w_i} = 1/n * sum{(y_i - ŷ_i)} * x_i$$ . . $$ frac{ partial L}{ partial b_i} = 1/2 * 1/n * 2 * sum{(y_i - ŷ_i)} * frac{ partial {ŷ_i}}{ partial b_i} $$ $$ frac{ partial L}{ partial b_i} = 1/n * sum{(y_i - ŷ_i)} * 1$$ . L2 loss with 2 layers, relu activation in the hidden layer . Loss . $$L = 1/2 * 1/n * sum{(y_i - ŷ_i)^{2}}$$ $$L = 1/2 * 1/n * sum{(y_i - (w_j * x_j + b_j))^{2}}$$ $$x_j = relu(w_i * x_i + b_i)$$ . Gradients . $$ frac{ partial L}{ partial w_i} = 1/n * sum{(y_i - ŷ_i)} * x_j $$ $$ frac{ partial L}{ partial b_i} = 1/n * sum{(y_i - ŷ_i)} * 1$$ . $$ frac{ partial L}{ partial w_j} = 1/n * sum{(y_i - ŷ_i)} * x_j * x_i, se relu() &gt; 0$$ $$ frac{ partial L}{ partial b_j} = 1/n * sum{(y_i - ŷ_i)} * x_j, se relu() &gt; 0$$ . Treinando . nn = NeuralNetwork() t = Trainer(nn, verbose=False) for i in range(4): t.train(synt_x, synt_y) t.plot_losses() . Comparando com a realidade . print(&#39;Parâmetros aprendidos:&#39;) print(&#39;pesos:&#39;, nn.weights) print(&#39;bias:&#39;, nn.biases) print(&#39;Função que modela os dados: 7 * X + 15&#39;) plot_line(synt_x, nn.run_batch(synt_x), &#39;--r&#39;) plot_line(synt_x, synt_y) plt.show() . ⚠️ Cuidado:não confunda parâmetro com hiperparâmetros! Parâmetros são o que a sua rede usa para aprender (pesos e bias), enquanto hiperparâmetros são o que você define acerca da sua rede (número de camadas, qtde. de neurônios por camada, função de ativação de cada camada, etc...) . Uma fun&#231;&#227;o um pouco mais complicada . $Y = 7 * log(x) + 1$ . SYNT_TRAIN_SIZE = 10 def get_random_error(size, mu=0, std_dev=0.8): return np.random.normal(mu, std_dev, size) synt_x = np.random.rand(SYNT_TRAIN_SIZE) synt_y = np.reshape(7 * np.log(synt_x) + 1 + get_random_error(SYNT_TRAIN_SIZE), (SYNT_TRAIN_SIZE, 1)) synt_x = np.reshape(synt_x, (SYNT_TRAIN_SIZE, 1)) . plt.plot(synt_x, synt_y, &#39;ro&#39;, alpha=0.5) . nn = NeuralNetwork(layers=[10, 1], activations=[&#39;sigmoid&#39;, None]) t = Trainer(nn) . for i in range(10000): t.train(synt_x, synt_y) t.plot_losses() . print(&#39;Parâmetros aprendidos:&#39;) print(&#39;pesos:&#39;, nn.weights) print(&#39;bias:&#39;, nn.biases) print(&#39;Função que modela os dados: 7 * X + 15&#39;) plt.plot(synt_x, nn.run_batch(synt_x), &#39;or&#39;, alpha=0.3) plt.plot(synt_x, synt_y, &#39;og&#39;, alpha=0.3) plt.show() . E se os dados forem n&#227;o lineares? . xor_x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) xor_y = np.array([[0], [1], [1], [0]]) . nn = NeuralNetwork(layers=[10, 2], input_size=2, activations=[&#39;relu&#39;, None]) t = Trainer(nn, verbose=False) for i in range(100000): t.train(xor_x, xor_y) t.plot_losses() . plt.plot(xor_x, nn.run_batch(xor_x), &#39;bo&#39;, xor_x, xor_y, &#39;ro&#39;, alpha=0.3) . nn = NeuralNetwork(layers=[10, 1], input_size=2, activations=[None, None]) t = Trainer(nn, verbose=False) for i in range(100000): t.train(xor_x, xor_y) t.plot_losses() . plt.plot(xor_x, nn.run_batch(xor_x), &#39;bo&#39;, xor_x, xor_y, &#39;ro&#39;, alpha=0.3) . Base de dados Iris . A base de dados Iris foi publicada originalmente no UCI Machine Learning Repository. . Uma das bases de dados mais conhecidas. É uma pequena base contendo informações sobre plantas de 3 diferentes espécies (setosa, versicolour e virginica). É bastante utilizada para classificação das espécies . Atributos: sepal length in cm | sepal width in cm | petal length in cm | petal width in cm | | Classes: Iris Setosa | Iris Versicolour | Iris Virginica | | . Obtendo os dados . iris = fetch_mldata(&#39;iris&#39;) # np.c_ concatena as features e targets do dataset iris_data = pd.DataFrame(data=np.c_[iris[&#39;data&#39;], iris[&#39;target&#39;]], columns=[&#39;x0&#39;, &#39;x1&#39;, &#39;x2&#39;, &#39;x3&#39;, &#39;target&#39;]) . iris_data.head() . iris_data.describe() . iris_data.drop([&#39;target&#39;], axis=1).diff().hist(color=&#39;k&#39;, alpha=0.5, bins=10, figsize=(4, 5)) plt.show() . x = iris.data y = iris.target x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42) . def batches(x, y, batch_size=True): idx = np.random.permutation(len(x)) x = x[idx] y = y[idx] for i in range(0, len(x)-batch_size-1, batch_size): batch_x = x[i:i+batch_size] batch_y = y[i:i+batch_size] yield batch_x, batch_y . nn = NeuralNetwork(layers=[10, 4], input_size=4, activations=[&#39;relu&#39;, None]) t = Trainer(nn, verbose=False, loss_name=&#39;cross-entropy&#39;) for i in range(1000): for batch_x, batch_y in batches(x, y, 16): t.train(batch_x, batch_y) if i % 10 == 0: loss, metrics = t.eval(x_test, y_test, metrics=[&#39;accuracy&#39;]) print(&#39;Test loss = %.5f, accuracy %.5f&#39; % (loss, metrics[0])) t.plot_losses() . MNIST . mnist = fetch_mldata(&#39;MNIST original&#39;) . x = mnist.data / np.max(mnist.data) y = mnist.target x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42) . nn = NeuralNetwork(layers=[512, 256, 10], input_size=784, activations=[&#39;relu&#39;, &#39;relu&#39;, None]) t = Trainer(nn, verbose=False, loss_name=&#39;cross-entropy&#39;, learning_rate=0.001) for i in range(20): for batch_x, batch_y in batches(x, y, 64): t.train(batch_x, batch_y) if i % 1 == 0: loss, metrics = t.eval(x_test, y_test, metrics=[&#39;accuracy&#39;]) print(&#39;Test loss = %.5f, accuracy %.5f&#39; % (loss, metrics[0])) t.plot_losses() . Refer&#234;ncias . Blog do Matheus Facure sobre Gradiente Descendente (10/10) | .",
            "url": "https://hereismari.github.io/codando-deep-learning-jax/2021/01/21/1.3.como_aprendem.html",
            "relUrl": "/2021/01/21/1.3.como_aprendem.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "1.2 Introdução a Redes Neurais: do que se alimentam?",
            "content": "Falando em Deep Learning e redes neurais, as entradas são denominadas tensores. Nesta seção iremos discutir o que são tensores e como manipulá-los com Numpy. . Caso não tenha familiaridade com Numpy recomendamos a seção Introdução a Numpy. . O que s&#227;o tensores? . Tensores são vetores de diferentes dimensões. Você já conhece tensores, os mais comuns tem nomes próprios: . Um tensor 0-dimensional é denominado escalar; um tensor uni-dimensional é um vetor (array); um tensor bi-dimensional é uma matriz (array de arrays); Um tensor N-dimensional é um... Tensor! . Um tensor é qualquer vetor com N-dimensões (por exemplo, um cubo é um tensor de 3 dimensões). Vale salientar que, tecnicamente, o conceito de tensor engloba todos eles. Veja esse vídeo para mais detalhes. . Ainda pensando dessa forma, poderíamos imaginar um tensor 4-dimensional como um array de cubos; um tensor 5-dimensional seria uma matriz de cubos; um tensor 6-dimensional seria um cubo de cubos, e assim por diante... . . Visualiza&#231;&#227;o matem&#225;tica . De maneira matemática, você pode pensar em tensores da seguinte forma: . Visualiza&#231;&#227;o canina . Já os matemáticos que gostam de cachorros, preferem pensar em tensores da seguinte forma: . . Vis&#227;o Pythonica: Numpy! . Numpy é uma biblioteca em Python para manipulação eficiente de tensores. . import numpy as np # todo mundo que é legal chama numpy de np ;) . scalar_py = 3 scalar_np = np.array(3) print(scalar_py, scalar_np) . 3 3 . vector_py = [1, 2, 3] vector_np = np.array([1, 2, 3]) print(vector_py, vector_np) . [1, 2, 3] [1 2 3] . matrix_py = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] matrix_np = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) print(matrix_py) print(matrix_np) . [[1, 2, 3], [4, 5, 6], [7, 8, 9]] [[1 2 3] [4 5 6] [7 8 9]] . tensor_py = [[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]] tensor_np = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]]) print(tensor_py) print(tensor_np) . [[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]] [[[ 1 2] [ 3 4]] [[ 5 6] [ 7 8]] [[ 9 10] [11 12]]] . Manipulando tensores . matrix_np.T . array([[1, 4, 7], [2, 5, 8], [3, 6, 9]]) . matrix_np.dot(matrix_np.T) . array([[ 14, 32, 50], [ 32, 77, 122], [ 50, 122, 194]]) . [1, 2, 3] + matrix_np . array([[ 2, 4, 6], [ 5, 7, 9], [ 8, 10, 12]]) . Depois de ler isso tudo sobre tensores, você deve estar pensando: &quot;Beleza, mas... para que redes neurais usam esses tais tensores?&quot; Redes neurais usam tensores não apenas pra representar as entradas, mas basicamente para manipular toda a computação do início ao fim, desde a entrada até a saída tudo são tensores. . Por exemplo: as entradas podem ser representadas por tensores bi-dimensionais (matrizes). Em geral, cada linha dessa matriz vai representar uma amostra do seu banco, enquanto cada coluna representa um atributo (também chamada de feature). Por exemplo, no seguinte banco de dados: | . . nós temos 5 amostras (5 linhas) e 4 atributos (sepal length, sepal width, petal length e petal width) - a coluna target nesse banco representa um outro atributo que estamos interessados em identificar para cada amostra. . Já considerando imagens as entradas vão ser agora representadas por tensores 4-dimensionais. Em geral, a maioria dos frameworks assumem que esses tensores estão no formato NxHxWxC, onde: N: representa a quantidade de imagens no seu banco | H: a altura de cada imagem | W: a largura de cada imagem | C: a quantidade de canais de cada imagem. Imagens em níveis de cinza têm apenas 1 canal, enquanto imagens coloridas possuem 3 canais - vermelho (R), verde (G) e azul (B). | . | . Alguns frameworks também aceitam tensores no formato NxCxHxW, ou seja, os canais da imagem vêm logo após a quantidade de imagens. .",
            "url": "https://hereismari.github.io/codando-deep-learning-jax/2021/01/21/1.2.do_que_se_alimentam.html",
            "relUrl": "/2021/01/21/1.2.do_que_se_alimentam.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "1.1 Introdução a Redes Neurais: o que são?",
            "content": "Nesta seção iremos discutir o que são redes neurais e como implementá-las de forma eficiente utilizando matrizes. . Redes Neurais s&#227;o aproximadores de fun&#231;&#245;es . Redes neurais podem ser vistas de maneira bastante simplificada como funções matemáticas. . Ideia abstrata . Entradas: x -&gt; Computação: f &gt; Saídas: f(x) . Matemática . $f(x) = x times 2 + 3$ . Python . def f(x): return x * 2 + 3 . Similar à funções, as redes neurais possuem entradas e saídas. As entradas são representadas por amostras dos seus dados. As saídas, por sua vez, dependem de que tarefa estamos desempenhando. . Por exemplo, se estamos treinando uma rede neural para classificar imagens que contém cachorros ou gatos, as entradas serão imagens (matrizes de píxels) e as saídas podem ser simplismente um valor binário (1 se tiver um gato na imagem e 0 se tiver um cachorro). A questão é: não sabemos a melhor função f para mapear as entradas para as saídas. Podemos treinar uma rede neural para achar essa função. . Assim, uma simples definição de redes neurais é: . &quot;Redes Neurais são aproximadores de funções&quot; . A diferenção é que não sabemos qual a função ótima para representar uma entrada:pythondef f(x): return x * ? + ?? . Não sabemos qual o melhor f possível, mais especificamente, quais valores deveriam ser os valores de ? e ??? . Poderíamos tentar várias funções para se ajustar aos dados, quando treinamos rede neurais estamos na verdade buscando aprender quais são os melhores parâmetros (aqui representados por ? e ??) para a uma certa função função. . Redes Neurais s&#227;o constru&#237;das a partir de camadas . Redes Neurais são definidas em termos de camadas. A primeira camada representa as entradas da rede, enquanto a última camada representa as saídas. . Todas as camadas que estão entre as camadas de entrada e saída são chamadas de camadas escondidas (ou hidden layers). Um exemplo de uma Rede Neural com 2 camadas escondidas pode ser vista na figura abaixo: . . Sobre as camadas... . Com exceção da camada de entrada, toda camada de uma rede é composta pela seguintes propriedades: . número de neurônios: na figura acima, a primeira camada escondida tem 4 neurônios, já a segunda camada escondida tem 3 neurônios, enquanto a camada de saída tem apenas 1. Cada neurônio representa um valor. . | parâmetros: cada neurônio recebe como entrada todos as saídas dos neurônios das camadas anteriores. Cada entrada dessa é multiplicada por um peso correspondente. Tais pesos representam o que a Rede Neural pode ajustar para encontrar a solução do problema e são conhecidos como parâmetros. . | . A imagem acima mostra apenas uma arquitetura possível para uma rede neural. Existem vários outros tipos de arquiteturas e outros tipos de camadas que podemos usar para construir uma rede neural. Nessa parte do curso iremos focar nessa arquitetura que se chama MultiLayerPerceptron (MLP). . Todas essas camadas são chamadas camadas densas. A arquitetura MLP é caracterizada por várias camadas densas uma após a outra (várias camadas intermediárias). Apesar de na imagem acima termos escolhido mostrar apenas 2 camadas intermediárias poderíamos ter quantas quiséssemos, com quantos nós quiséssemos! . O número de nós de entrada e de saída são fixos dependendo do problema que estamos tentando resolver. Por exemplo, no problema da classificação de imagens de cachorros e gatos, o número de neurônios de entrada seria o número de píxeis da imagem e o número de neurônios na camada de saída um único neurônio (0 ou 1). . Entendendo a matem&#225;tica . Vamos focar apenas na computação que envolve o segundo nó da segunda camada intermediária: . . Cada peso (linha conectando os neurônios da camada anterior a esse neurônio) é um número, e cada nó também representa um número. . . Interpretamos esse número como a saída do neurônio, se a saída é um valor alto (&gt; 0) dizemos que o neurônio foi ativado. Como a imagem sugere, utilizamos a saída dos neurônios anteriores para calcular a saída do próximo neurônio. Mas como? . $$SN_j = f( sum{(SN_i cdot pij)} + b_j)$$ . $SN_j$ = Saída do Neurônio j $P_ij$ = Peso que liga o neurônio i ao neurônio j $b_j$ = Bias de do neurônio j . . def funcao_de_ativacao(x): if x &gt; 0: return x else: return 0 def liga_neuronios(saida_neuronios_da_camada_anterior, pesos_neuronio, peso_bias, f): saida_neuronio = 0 # Soma ponderada dos pesos com a saída for saida_neuronio_anterior, peso in zip(saida_neuronios_da_camada_anterior, pesos_neuronio): saida_neuronio += saida_neuronio_anterior * peso # Adiciona bias saida_neuronio += peso_bias # Aplica função de ativação saida_neuronio = f(saida_neuronio) return saida_neuronio # Exemplo da imagem liga_neuronios([4.1, 2.9, 0.7, -0.3], [0.1, 3.7, -1.3, 4.1], 1, funcao_de_ativacao) . 10.0 . Ok... então para calcular a saída de um neurônio, eu faço a soma ponderada entre os pesos e todas as saídas dos neurônios da camada anterior. Ate aí tudo bem... mas aí eu somo a isso uma coisa chamada bias e depois aplico sobre esse valor uma função de ativação? . Isso! . Imagem de: https://github.com/arnaldog12/Manual-Pratico-Deep-Learning/blob/master/Perceptron.ipynb . OBS:Se isolarmos um nó da rede neural temos o que é chamado perceptron. . Fun&#231;&#227;o de Ativa&#231;&#227;o . Como o próprio nome indica é uma função matemática. Por enquanto não vamos nos preocupar muito com ela, iremos ver mais sobre funções de ativação na seção 1.5. . Por enquanto só vou dizer 2 coisas: . A função de ativação que utilizamos no código anterior se chama ReLU, é uma das mais utilizadas e é comumente utilizada entre as camadas intermediárias. Na camada final geralmente não utilizamos função de ativação ou utilizamos alguma outra função (não relu). | . . Como experado de uma função, uma função de ativação transforma uma entrada em alguma outra coisa que obtemos como saída. No caso das funções de ativação é bastante interessante que ela não seja uma reta, por exemplo na imagem acima temos com a ReLU se comporta, ta vendo que ela é tipo um cotovelo dobrado no zero? Isso faz com que a função não seja uma linha reta, e é isso que queremos na função de ativação, não precisa ser um cotovelo, poderia ser uma barraguinha, mas precisamos de curvas! | . . Bias . O bias é um termo independente da entrada que existe para nos dar mobilidade! . Por exemplo, considere a seguinte função: . y = 2 * x . Essa função tem uma mobilidade limitada, ela passa pela origem (se x = 0, y = 0) e não podemos mudar isso :(. Porém se essa função fosse: . y = 2 * x + bias . Ao modificar o bias, podemos fazer com que a função vá mais para esquerda ou direita, não mais sendo forçada a passar pela origem. . Na nossa rede o bias faz um papel similar! Nos dando liberdade para mover a função que estamos aprendendo, lembra que redes neurais são aproximadores de funções? . Mais informações sobre bias (em inglês) nesse link. . Importante . Nos diagramas que mostramos, o bias não era mostrado, por que? Geralmente pra deixar a visualização mais bonitinha não mostramos o bias, abaixo segue nossa rede reural com o bias no diagrama: . . O bias é um nó na rede como qualquer outro, porém ele não se conecta aos nós da camada anterior, podemos pensar que seu valor é fixo e igual a 1. . “Se hoje é o Dia da rede neural, ontem eu disse que rede neural... o dia da rede neural é dia da camada, do peso e das funções de ativação, mas também é o dia dos bias. Sempre que você olha uma camada densa, há sempre uma figura oculta, que é um bias atrás, o que é algo muito importante.” - Marianne Rousseff. . Jutando os peda&#231;os e codando uma Rede Neural . Vamos codar a seguinte rede neural: . . Nomeamos os neurônios pra ficar mais simples de identificá-los no código. . ni_cj = i-ésimo neuronio da j-ésima camada | . Os valores escolhidos são arbitrários e a saída não significa nada. . # Definimos esse valor! Na prática irá vir de uma base de dados. entrada = -3 # - Primeira camada n1_c1 = liga_neuronios([entrada], [-4.3], 1, funcao_de_ativacao) n2_c1 = liga_neuronios([entrada], [-1.3], 2.2, funcao_de_ativacao) # - Segunda camada n1_c2 = liga_neuronios([n1_c1, n2_c1], [0.2, 3.3], 2.7, funcao_de_ativacao) # - Camada de saída -- # Na última camada não usamos ReLU, lembra? Podemos simplesmente não usar # função de ativação, para tal é só usar uma função que não faz nada (função identidade) # essa função é também denominada como linear nos frameworks de Deep Learning. def faz_nada(x): return x saida = liga_neuronios([n1_c2], [4.1], -10, faz_nada) print(saida) . 95.00099999999999 . A grande sacada das redes neurais é a definição de camadas e como elas se comunicam. Uma rede neural consiste basicamente da mesma computação baseada em camadas onde a entrada de uma camada é a camada imediamente anterior a ela. . Ou seja: uma camada aprende a partir das camadas anteriores! . Isso permite que a rede neural busque modificar os pesos de uma camada de maneira a gerar representações melhores para que a próxima camada tenha acesso a informações mais relevantes pra resolver o problema! . Deixando o c&#243;digo eficiente, bonito e cheiroso . Como a(o) boa(bom) programadora(programador) que você é, deve ter notado que o código anterior é bastante ineficiente!!! . PERGUNTA: mais espeficamente para cada nó em uma rede neural quantas operações são necessárias para calcular sua saída? . Pense a respeito, a resposta segue abaixo... . Para calcular a saída desse nó precisamos cacular a soma ponderada de todos os nós da camada anterior, digamos que a camada anterior tem N nós. . Então precisaríamos de O(N) operações para calcular a saída de um único nó nessa camada. . Então, por exemplo, se tivermos 1 camada de entrada com 100 nós e 2 camadas intermediárias com 200 nós cada, precisaríamos realizar o seguinte número de operações em cada camada: . camada intermediária 1: 100 * 200 (100 entradas para cada nó, 200 nós) = 20000 | camada intermediária 2: 200 * 200 (200 entradas para cada nó, 200 nós) = 40000 | . Totalizando: 60000 operações para calcular todas as saídas. . Isso é péssimo :((! Queremos várias camadas na nossa rede, não podemos ser tão lentos :(( . Felizmente temos uma solução para deixar esse cálculo mais eficiente!!! . . Perceba que: . Cada nó em uma camada é independente dos demais! Então podemos &quot;paralelizar&quot; essa computação! | Uma camada L precisa de todos os nós da camada L-1 computados. | . Agora vem a mágica: 3. Podemos computar todos os valores de uma camada através de multiplicação de matrizes . . . Por que utilizar multiplica&#231;&#227;o de matrizes &#233; mais eficiente? . GPUs manipulam matrizes de forma extremamente eficiente! | Existem otimizações para lidar com esse tipo de operação de maneira eficiente. | Multiplicação de matrizes é altamente paralelizável. | Codando uma rede neural com multiplica&#231;&#227;o de matrizes . Vamos modificar o código que fizemos anteriormente para utilizar multiplicação de matrizes. Para isso iremos utilizar a biblioteca Numpy, caso não tenha familiaridade recomendamos a seção de introdução a Numpy . . import numpy as np class DenseLayer(object): def __init__(self, activation_function): self._weights = [] self.activation_function = activation_function def set_weights(self, weights): self._weights = weights def _add_bias(self, _input): # Adiciona 1&#39;s a entrada return np.column_stack((np.ones(len(_input)), _input)) def __call__(self, _input): # adiciona bias a entrada, lembra? input_with_bias = self._add_bias(_input) # f_act(entrada * transposta(pesos)) return self.activation_function(np.matmul(input_with_bias, self._weights.transpose())) . # def funcao_de_ativacao(x): # if x &gt; 0: # return x # else: # return 0 # Agora def funcao_de_ativacao(x): return np.where(x &gt; 0, x, 0) . # Definimos esse valor! Na prática irá vir de uma base de dados. entrada = -3 # - Primeira camada # Antes # n1_c1 = liga_neuronios([entrada], [-4.3], 1, funcao_de_ativacao) # n2_c1 = liga_neuronios([entrada], [-1.3], 2.2, funcao_de_ativacao) # Agora camada1 = DenseLayer(funcao_de_ativacao) camada1.set_weights(np.array([[1, -4.3], [2.2, -1.3]])) # - Segunda camada # Antes # n1_c2 = liga_neuronios([n1_c1, n2_c1], [0.2, 3.3], 2.7, funcao_de_ativacao) # Agora camada2 = DenseLayer(funcao_de_ativacao) camada2.set_weights(np.array([[2.7, 0.2, 3.3]])) # - Camada de saída -- # Na última camada não usamos ReLU, lembra? Podemos simplesmente não usar # função de ativação, para tal é só usar uma função que não faz nada (função identidade) # essa função é também denominada como linear nos frameworks de Deep Learning. def faz_nada(x): return x # Antes # saida = liga_neuronios([n1_c2], [4.1], -10, faz_nada) # Agora ultima_camada = DenseLayer(faz_nada) ultima_camada.set_weights(np.array([[-10, 4.1]])) saida = ultima_camada(camada2(camada1(np.array([entrada])))) print(saida) . [[95.001]] . Refer&#234;ncias . Capítulo 3 - Grokking Deep Learning | .",
            "url": "https://hereismari.github.io/codando-deep-learning-jax/2021/01/21/1.1.o_que_sao.html",
            "relUrl": "/2021/01/21/1.1.o_que_sao.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://hereismari.github.io/codando-deep-learning-jax/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://hereismari.github.io/codando-deep-learning-jax/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://hereismari.github.io/codando-deep-learning-jax/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}