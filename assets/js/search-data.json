{
  
    
        "post0": {
            "title": "Introdução ao Numpy",
            "content": "De acordo com as próprias palavras da numpy.org (tradução livre): . Numpy é um pacote fundamental pra computação científica com Python. . E de fato, numpy é muito show! É bem fácil de usar e se usado corretamente super eficiente:10/10. Entre outras coisas, o numpy possui: . um podereso ferramental para manipulação de arrays multi-dimensionais | funções de broadcasting sofisticadas (veremos isso já já) | ferramentas para integração de código Fortran e C/C++ (por baixo dos panos pra gente não ter que se preocupar) | utilidades para álgebra linear, tranformação de Fourier e números aleatórios | . Para usar o numpy, tudo que precisamos é importá-lo: . # pq numpy é muito grande pra ficar digitando toda hora import numpy as np . Ta... o que nós podemos fazer com numpy? . Bem eu disse que numpy possui &quot;um podereso ferramental para manipulação de arrais multi-dimensionais&quot;. . o que danado isso significa? . MÁGICA! . Vamos começar criando um array numpy. Um array numpy parece igualzinho a uma lista em python, mas não se engane, não é! . list_py = [1, 2, 3] # Array numpy: cool e descolado vector_np = np.array([1, 2, 3]) print(&#39;list python:&#39;, type(list_py), list_py) print(&#39;vetor numpy&#39;, type(vector_np), vector_np) . list python: &lt;class &#39;list&#39;&gt; [1, 2, 3] vetor numpy &lt;class &#39;numpy.ndarray&#39;&gt; [1 2 3] . Humm... okay, vamos tentar dar um append nas listas... . vector_py.append(1) # Erro! vector_np.append(1) . NameError Traceback (most recent call last) &lt;ipython-input-3-46dcf3fb9fae&gt; in &lt;module&gt; 1 # Ok! -&gt; 2 vector_py.append(1) 3 4 # Erro! 5 vector_np.append(1) NameError: name &#39;vector_py&#39; is not defined . Não existe append diretamente em um vetor numpy... Mas você pode fazer append via numpy... . vector_np = np.append(vector_np, 1) . Mas fica aquele velho ditado: não é porque você pode que você deve! . Normalmente não executamos append em vetores numpy, eles já nascem prontinhos e realizamos operações entre vetores (adição, multiplicação, transposição, ...). Não adicionamos ou removemos valores de vetores numpy. . Por que? Em geral isso é ineficiente, olha o exemplo abaixo. . Obs:Para mais detalhes da uma olhada nesses links 1, 2. . import time N = 100000 # Usando uma lista python e depois convertendo para numpy def time_to_create_vector_py(): start = time.time() l = [] for i in range(N): l.append(i) l = np.array(l) return time.time() - start # Usando um vetor numpy desde o inicio pq sou teimoso def time_to_create_vector_np(): start = time.time() l = np.array([]) for i in range(N): l = np.append(l, i) l = np.array(l, dtype=int) return time.time() - start print(&#39;Tempo utilizando lista: %.3f (s)&#39; % time_to_create_vector_py()) print(&#39;Tempo utilizando numpy: %.3f (s)&#39; % time_to_create_vector_np()) . Tempo utilizando lista: 0.012 (s) Tempo utilizando numpy: 2.194 (s) . Ta, então uma vez que a gente cria um array numpy, normalmente não alteramos ele via operações convencionais de modificação de lista. . Se você para pra pensar isso faz todo sentido: já que um array numpy não é uma lista python, mas sim um vetor. . Já que temos vetores, temos propriedades de vetores associadas aos arrays numpy: . Vamos chamar o número de dimensões de um vetor de ndim; | O shape é uma tupla de inteiros do tamanho do ndim que fornece número de elementos ao longo de cada dimensão. | . vector_np . array([1, 2, 3, 1]) . Quanto é o ndim desse vetor? . vector_np.ndim . 1 . E o formato? . vector_np.shape . (4,) . E se em vez de um array tivermos um número? . scalar_np = np.array(3) # 0 pq não temos nenhuma dimensão print(scalar_np.ndim) print(scalar_np.shape) . 0 () . E matrizes? . matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] matrix_np = np.array(matrix) print(matrix_np.ndim) # (3, 3) pq é uma matrix 3x3 print(matrix_np.shape) . 2 (3, 3) . O céu é o limite! . a = np.array([1, 2, 3]) # cria uma matriz de 1 dimensão (vetor) b = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.int64) # cria uma matriz de 2 dimensões (matriz) print(&#39;Vetor a:&#39;, a, &#39; nFormato de a:&#39;, a.shape) print(&#39;-&#39;) print(&#39;Vetor b: n&#39;, b, &#39; nFormato de b:&#39;, b.shape) . Vetor a: [1 2 3] Formato de a: (3,) - Vetor b: [[1 2 3] [4 5 6]] Formato de b: (2, 3) . tensor_np = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]]) tensor_np.shape . (3, 2, 2) . M&#225;gica com Numpy . Broadcast . Esse código python quebra por que não da pra somar inteiro com lista. . 1 + matrix . TypeError Traceback (most recent call last) &lt;ipython-input-13-b2a55592596e&gt; in &lt;module&gt; -&gt; 1 1 + matrix TypeError: unsupported operand type(s) for +: &#39;int&#39; and &#39;list&#39; . Mas pensa um pouco... o código faz &quot;sentido&quot;, parece que 1 deveria ser somado a cada elemento da lista. . Bem que dava pra python ser mais esperto e entender que o que a gente quer é na verdade propagar o 1 por toda a matriz somando cada elemento a 1. . Essa propagação é justamente o que chamamos de broadcasting. Que é a ideia de que vetores de tamanhos e formatos diferentes são compatíveis para certas operações em alguns casos! . matrix_np . array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) . # Somar um valor a todos os elementos de uma matriz nunca foi tao fácil! 1 + matrix_np . array([[ 2, 3, 4], [ 5, 6, 7], [ 8, 9, 10]]) . Por que? . Porque numpy consegue entender a partir dos formatos dos vetores o que deve ser feito. O que acontece se tentarmos somar um vetor de tamanho 3 a matriz? . [1, 2, 3] + matrix_np . array([[ 2, 4, 6], [ 5, 7, 9], [ 8, 10, 12]]) . Sem broadcasting, como faríamos a operação acima? . v = np.array([1, 2, 3]) v . array([1, 2, 3]) . vv = np.tile(v, (3, 1)) # Cria 4 cópias de v e empilha vv . array([[1, 2, 3], [1, 2, 3], [1, 2, 3]]) . matrix_np + vv . array([[ 2, 4, 6], [ 5, 7, 9], [ 8, 10, 12]]) . O broadcasting nos permite obter o mesmo resultado sem precisar criar cópias do vetor. Sendo mais eficiente tanto em tempo quanto em memória. Além de simplificar bastante a nossa vida. . np.array([[1], [2], [3]]) + matrix_np . array([[ 2, 3, 4], [ 6, 7, 8], [10, 11, 12]]) . Vai ter caso que vai dar errado? . [1, 2] + matrix_np . ValueError Traceback (most recent call last) &lt;ipython-input-21-fe98043e1a08&gt; in &lt;module&gt; 1 # Somar uma lista a todas as linhas de uma matriz nunca foi tao... eita perainda quebrou aqui -&gt; 2 [1, 2] + matrix_np ValueError: operands could not be broadcast together with shapes (2,) (3,3) . O que aconteceu foi que os formatos dos vetores não são compatíveis, então numpy não conseguiu realizar broadcast corretamente para realizar a operação. . O que é até intuitivo, o que danado a gente estava esperando somando um vetor de tamanho 2 com uma matriz 3x3? . Existe um algoritmo que podemos utilizar pra saber se o broadcast vai dar certo: . Recebemos a e b | Percorremos os formatos de a e b de trás pra frente | Para cada uma das dimensões dim_a e dim_b deve ser verdade que: dim_a == dim_b ou dim_a == 1 ou dim_b == 1 | . | . Pense um pouco a respeito... e tende adivinhar os formatos de cada vetor e se os pares são passíveis de broadcast: . Caso 1: [1, 2] e [[1, 2, 3], [4, 5, 6], [7, 8, 9]] | Caso 2: [1] e [[1, 2, 3], [4, 5, 6], [7, 8, 9]] | Caso 3: [[[[[1]]]]] e [[1, 2, 3], [4, 5, 6], [7, 8, 9]] | Caso 4: [1, 2] e [[1, 2], [4, 5], [7, 8]] | . O código da checagem e soluções seguem logo abaixo. . def is_broadcast_possible(a, b): # Oferecimento: https://stackoverflow.com/questions/47243451/checking-if-two-arrays-are-broadcastable-in-python a, b = np.array(a), np.array(b) print(&#39;Formato de a:&#39;, a.shape) print(&#39;Formato de b:&#39;, b.shape) return all((m == n) or (m == 1) or (n == 1) for m, n in zip(a.shape[::-1], b.shape[::-1])) . is_broadcast_possible([1, 2], [[1, 2, 3], [4, 5, 6], [7, 8, 9]]) . Formato de a: (2,) Formato de b: (3, 3) . False . is_broadcast_possible([1], [[1, 2, 3], [4, 5, 6], [7, 8, 9]]) . Formato de a: (1,) Formato de b: (3, 3) . True . is_broadcast_possible([[[[[1]]]]], [[1, 2, 3], [4, 5, 6], [7, 8, 9]]) . Formato de a: (1, 1, 1, 1, 1) Formato de b: (3, 3) . True . is_broadcast_possible([1, 2], [[1, 2], [4, 5], [7, 8]]) . Formato de a: (2,) Formato de b: (3, 2) . True . Manipula&#231;&#227;o de matrizes . matrix_np.transpose() . array([[1, 4, 7], [2, 5, 8], [3, 6, 9]]) . matrix_np.dot(matrix_np) . array([[ 30, 36, 42], [ 66, 81, 96], [102, 126, 150]]) . Criando tipos espec&#237;ficos de arrays . np.zeros((3, 3)) . array([[ 0., 0., 0.], [ 0., 0., 0.], [ 0., 0., 0.]]) . np.ones((3, 3)) . array([[ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.]]) . np.full((3, 3), 7) . array([[7, 7, 7], [7, 7, 7], [7, 7, 7]]) . np.eye(3) . array([[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]) . np.diag([1, 2, 3]) . array([[1, 0, 0], [0, 2, 0], [0, 0, 3]]) . np.diag([1, 2, 3], k=1) # k = offset da diagonal . array([[0, 1, 0, 0], [0, 0, 2, 0], [0, 0, 0, 3], [0, 0, 0, 0]]) . np.mgrid[1:4, 1:4] # similar ao meshgrid no Matlab . array([[[1, 1, 1], [2, 2, 2], [3, 3, 3]], [[1, 2, 3], [1, 2, 3], [1, 2, 3]]]) . np.random.rand(3, 3) # distribuição aleatória . array([[ 0.91364028, 0.4408339 , 0.65251051], [ 0.89478035, 0.49410589, 0.68962237], [ 0.11590382, 0.32048899, 0.75107557]]) . np.random.randn(3, 3) # distribuição normal (gaussiana) . array([[ 0.05290034, -0.87768182, -0.94384387], [-0.63596293, -0.00936166, 0.25018653], [ 0.99313152, -1.10631277, 1.02928441]]) . np.random.randint(1, 10, (3, 3)) # número aleatórios inteiros de 1 a 10 . array([[7, 7, 9], [9, 4, 2], [1, 5, 7]]) . np.arange([start,] stop[,step,], dtype=None) . np.arange(10) . array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . np.arange(1,10) . array([1, 2, 3, 4, 5, 6, 7, 8, 9]) . np.arange(1, 10, 0.5) . array([ 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5, 6. , 6.5, 7. , 7.5, 8. , 8.5, 9. , 9.5]) . np.arange(1, 10, 3) . array([1, 4, 7]) . np.arange(1, 10, 2, dtype=np.float64) . array([ 1., 3., 5., 7., 9.]) . np.linspace(start, stop, num=50, endpoint=True, retstep=False) . np.linspace(1, 5, num=10) . array([ 1. , 1.44444444, 1.88888889, 2.33333333, 2.77777778, 3.22222222, 3.66666667, 4.11111111, 4.55555556, 5. ]) . np.linspace(0, 2, num=4) . array([ 0. , 0.66666667, 1.33333333, 2. ]) . np.linspace(0, 2, num=4, endpoint=False) . array([ 0. , 0.5, 1. , 1.5]) . Examinando um array n-dimensional . ds = np.array([[1,2,3],[4,5,6],[7,8,9]]) ds.ndim . 2 . ds.shape . (3, 3) . ds.size . 9 . ds.dtype # tipo dos elementos guardados . dtype(&#39;int64&#39;) . ds.itemsize # qtde de bytes por valor . 8 . ds.size * ds.itemsize # espaço total ocupado em memória (em bytes) . 72 . An&#225;lise Estat&#237;stica . data_set = np.random.random((2, 3)) data_set . array([[ 0.97054673, 0.35857832, 0.20979014], [ 0.6939001 , 0.87199282, 0.7442244 ]]) . np.max(a, axis=None, out=None, keepdims=False) . np.max(data_set) . 0.97054672767941563 . np.max(data_set, axis=0) . array([ 0.97054673, 0.87199282, 0.7442244 ]) . np.max(data_set, axis=1) . array([ 0.97054673, 0.87199282]) . np.min(a, axis=None, out=None, keepDims=False) . np.min(data_set) . 0.20979014003667951 . np.mean(a, axis=None, dtype=None, out=None, keepdims=False) . np.mean(data_set) . 0.64150541875340539 . np.median(a, axis=None, out=None, overwrite_input=False) . np.median(data_set) . 0.71906224976763788 . np.std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False) . np.std(data_set) . 0.27114413265171439 . np.sum(a, axis=None, dtype=None, out=None, keepdims=False) . np.sum(data_set) . 3.8490325125204325 . np.prod(a, axis=None, dtype=None, out=None, keepdims=False) . np.prod(data_set) . 0.032877540156687972 . np.cumsum(a, axis=None, dtype=None, out=None) . np.cumsum(data_set) . array([ 0.97054673, 1.32912505, 1.53891519, 2.23281529, 3.10480811, 3.84903251]) . np.cumprod(a, axis=None, dtype=None, out=None) . np.cumprod(data_set) . array([ 0.97054673, 0.34801702, 0.07301054, 0.05066202, 0.04417692, 0.03287754]) . Redimensionando Arrays . np.reshape(a, newshape, order=&#39;C&#39;) . np.reshape(data_set, (3, 2)) . array([[ 0.97054673, 0.35857832], [ 0.20979014, 0.6939001 ], [ 0.87199282, 0.7442244 ]]) . np.reshape(data_set, (6, 1)) . array([[ 0.97054673], [ 0.35857832], [ 0.20979014], [ 0.6939001 ], [ 0.87199282], [ 0.7442244 ]]) . np.reshape(data_set, 6) . array([ 0.97054673, 0.35857832, 0.20979014, 0.6939001 , 0.87199282, 0.7442244 ]) . np.ravel(a, order=&#39;C&#39;) . np.ravel(data_set) . array([ 0.97054673, 0.35857832, 0.20979014, 0.6939001 , 0.87199282, 0.7442244 ]) . data_set.flatten() # igual ao ravel . array([ 0.97054673, 0.35857832, 0.20979014, 0.6939001 , 0.87199282, 0.7442244 ]) . Acessando elementos . Indexa&#231;&#227;o . data_set = np.random.randint(1, 10, (5, 5)) data_set . array([[1, 2, 7, 5, 5], [9, 9, 7, 1, 2], [9, 4, 4, 5, 3], [9, 6, 6, 5, 3], [7, 9, 8, 5, 1]]) . data_set[1] # segunda linha . array([9, 9, 7, 1, 2]) . data_set[1][0] # segunda linha, primeira coluna . 9 . data_set[1, 0] # equivalente a de cima . 9 . Indexa&#231;&#227;o por inteiros . Quando você indexa matrizes numpy usando slicing, a matriz resultante sempre será um subarray da matriz original. Por outro lado, a indexação de matrizes por inteiros permite que você construa matrizes arbitrárias usando os dados de outra matriz. Aqui está um exemplo: . a = np.array([[1,2], [3,4], [5,6]]) a . array([[1, 2], [3, 4], [5, 6]]) . a[[0, 1, 2], [0, 1, 0]] . array([1, 4, 5]) . np.array([a[0, 0], a[1, 1], a[2, 0]]) # equivalente ao de cima . array([1, 4, 5]) . Indexa&#231;&#227;o booleana . Boolean array indexing lets you pick out arbitrary elements of an array. Frequently this type of indexing is used to select the elements of an array that satisfy some condition. Here is an example: . a = np.array([[1,2], [3,4], [5,6]]) a . array([[1, 2], [3, 4], [5, 6]]) . bool_idx = (a &gt; 2) print(bool_idx) . [[False False] [ True True] [ True True]] . print(a[bool_idx]) . [3 4 5 6] . print(a[a &gt; 2]) . [3 4 5 6] . Slicing . data_set[2:4] # terceira e quarta linhas . array([[9, 4, 4, 5, 3], [9, 6, 6, 5, 3]]) . data_set[2:4, 0] # terceira e quarta linhas, primeira coluna . array([9, 9]) . data_set[2:4, 0:2] # terceira e quarta linhas, primeira e segunda coluna . array([[9, 4], [9, 6]]) . data_set[:, 0] # todas as linhas, primeira coluna . array([1, 9, 9, 9, 7]) . Steping . data_set[:, 0:10:2] # 1ª, 3ª, 5ª, 7ª e 9ª colunas para todas as linhas . array([[1, 7, 5], [9, 7, 2], [9, 4, 3], [9, 6, 3], [7, 8, 1]]) . data_set[::] . array([[1, 2, 7, 5, 5], [9, 9, 7, 1, 2], [9, 4, 4, 5, 3], [9, 6, 6, 5, 3], [7, 9, 8, 5, 1]]) . data_set[::2] # 1ª, 3ª e 5ª linha, todas as colunas . array([[1, 2, 7, 5, 5], [9, 4, 4, 5, 3], [7, 9, 8, 5, 1]]) . Opera&#231;&#245;es com matrizes . x = np.array([[1,2], [3,4]]) y = np.array([[5,6], [7,8]]) print(x+y) print(np.add(x,y)) . [[ 6 8] [10 12]] [[ 6 8] [10 12]] . print(x - y) print(np.subtract(x, y)) . [[-4 -4] [-4 -4]] [[-4 -4] [-4 -4]] . print(x * y) print(np.multiply(x, y)) . [[ 5 12] [21 32]] [[ 5 12] [21 32]] . print(x / y) print(np.divide(x, y)) . [[ 0.2 0.33333333] [ 0.42857143 0.5 ]] [[ 0.2 0.33333333] [ 0.42857143 0.5 ]] . print(np.sqrt(x)) . [[ 1. 1.41421356] [ 1.73205081 2. ]] . Observe que, diferentemente do MATLAB, $*$ é a multiplicação elementar, não a multiplicação de matrizes. Em vez disso, usamos a função dot para calcular produtos internos de vetores, multiplicar um vetor por uma matriz e multiplicar matrizes. . v = np.array([9, 10]) w = np.array([11, 12]) # produto interno print(v.dot(w)) print(np.dot(v, w)) . 219 219 . print(x.dot(v)) print(np.dot(x, v)) . [29 67] [29 67] . print(x.dot(y)) print(np.dot(x, y)) . [[19 22] [43 50]] [[19 22] [43 50]] . print(x.T) print(&#39;-&#39;) # ou print(np.transpose(x)) . [[1 3] [2 4]] - [[1 3] [2 4]] . Lista de todas as operações: documentation. .",
            "url": "https://mari-linhares.github.io/codando-deep-learning-jax/2021/01/21/intro_a_numpy.html",
            "relUrl": "/2021/01/21/intro_a_numpy.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Introdução ao JAX",
            "content": "JAX é uma nova biblioteca de Python da Google com foco em pesquisa de alta performance em Aprendizado de Máquina e seguindo o paradigma de programação funcional. . Mais especificamente JAX nos dá acesso a: e , as principais sendo grad, jit, vmap e pmap(que vai ter seu próprio post no futuro). . uma API compatível com numpy e scipy | uma própria API de números aleatório manejados manualmente | transformações de função composicionais (você pode aplicar elas em conjunto sem muito problema) | transformações para trivialmente computar derivadas de funções | transformações para usar aceleradores (CPU, GPU e TPU) | transformações que permitem paralelizar seu código facilmente | . Para usar o JAX é recomendado checar as instruções de instalação, e a seguir importá-lo: . # jax.numpy toda hora, então fazemos um alias dele para jnp import jax import jax.numpy as jnp . Agora vamos ver cada uma dessas features da biblioteca . O Wrapper de Numpy: jax.numpy: . import numpy as np a = np.array([1., 2., 3.]) b = np.array([1., 1., -1.]) print(np.dot(a, b), jnp.dot(a, b)) . 0.0 0.0 . /home/joaogui/miniconda3/envs/jax/lib/python3.7/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn(&#39;No GPU/TPU found, falling back to CPU.&#39;) . Esse warning que aparece é ele dizendo que sou pobre e não tenho nem GPU nem TPU :( . (np.square(a), jnp.square(a)) . (array([1., 4., 9.]), DeviceArray([1., 4., 9.], dtype=float32)) . Note que JAX tem seu próprio tipo de array, o DeviceArray, em geral as funções vão transformar arrays de numpy em DeviceArrays, então se você quiser boa performance é melhor fazer essa transformação manualmente antes de passar os dados para várias funções. . N&#250;meros aleat&#243;rios jax.random . Uma das partes mais peculiares de JAX, para faciliar implementações usando paralelismo não existe uma semente global para geradores de números aleatórios, em vez disso em JAX você passa explicitamente a seed para cada função que envolve aleatoriedade, e cabe a você atualizá-la . key = jax.random.PRNGKey(42) #cria um semente aleatória a = jax.random.normal(key, ()) b = jax.random.normal(key, ()) print(a, b) print(a == b) #como usamos a mesma semente para a mesma função temos valores iguais k1, k2 = jax.random.split(key, 2) #vamos criar duas novas seeds a partir da primeira a = jax.random.normal(k1, ()) b = jax.random.normal(k2, ()) print(a, b) #Agora são diferentes . -0.18471184 -0.18471184 True 0.13790314 1.3694694 . Transforma&#231;&#245;es . O principal diferencial de JAX são suas tranformações de funções, que nos permitem modificar facilmente funções definidas a partir de outras funções do JAX e algumas primitivas de Python. . Algo muito útil e legal delas é que podem ser utilizadas em conjunto (são &quot;composicionais&quot;), nos permitindo por exemplo compilar a derivada de uma função vetorizada apenas aplicando 3 transformações uma seguida da outra a função original. . Porém existem alguns cuidados que dever ser tomados ao se usar esses transformações, para entender esse cuidados melhor cheque esse link e abra o notebook . Diferencia&#231;&#227;o Autom&#225;tica: jax.grad . Em aprendizado de máquina, principalmente quando estamos tratando de redes neurais, lidamos com muitas derivadas, gradientes e afins: Para treinar uma regressão linear ou logística, precisamos computar um hessiano, para treinar uma rede neural usamos descida de gradiente, que requer o cálculo de um gradiente, dentre outros exemplos. . Computar essas derivadas na mão é muitas vezes extremamente trabalhoso, ou até mesmo impossível dado o tempo disponível, assim temos algoritmos como o backpropagation para redes neurais, porém se sempre tivessemos que implementar nós mesmos esse algoritmo, e implementar a derivada de cada uma das funções que vamos usar, terminaríamos com uma quatidade imensa de código duplicado, além de uma imensa chance de errarmos algo na implementação e terminarmos sem conseguir bons resultados ou com resultados que não correspodem a realidade. . Para lidar com isso temos diferenciação automática, transformações que recebem uma função e retornam algum tipo de derivada dela. Simplesmente ter diferenciação automática para as funções de Numpy já é o bastante para uma biblioteca mostrar seu valor, tanto que existe uma biblioteca que é exatamente isso, chamada de Autograd, em muitos sentidos JAX é um sucessor dessa biblioteca. . from jax import grad from math import pi, sqrt dup = grad(jnp.square) print(dup(3.0)) #A derivada de x² é 2x print(grad(dup)(3.0)) #Podemos aplicar várias vezes a grad @grad #Podemos usar as transformações como decoradores def composite_func(x): y = x**2 return jnp.cos(y) # Pela regra da cadeia, dcos(x²)/dx = -2xsen(x²) print(composite_func(jnp.sqrt(pi/2)), -2*sqrt(pi/2)) . 6.0 2.0 -2.5066283 -2.5066282746310002 . Para funções com várias variáveis de entrada a grad por padrão nos dá a derivada em função do primeiro parâmetro, mas podemos mudar isso com o argumento argnums. Também vale ressaltar que os argumentos não precisam ser apenas números e podem ser vetores . def f(x, y): return x*(y**2) dfdy = grad(f, argnums=(1)) print(dfdy(3.0, 4.0)) gradient = grad(f, argnums=(0, 1)) print(gradient(3.0, 4.0)) def g(v): return jnp.linalg.norm(v) print(grad(g)(a)) . 24.0 (DeviceArray(16., dtype=float32), DeviceArray(24., dtype=float32)) 1.0 . Compila&#231;&#227;o com XLA: jit . Mas as vantagens de jax não param em diferenciação automática, se não seria apenas um clone do autograd, jax também tem a habilidade de compilar funções usando o XLA (accelerated linear algebra) da Google, tornando-as bem mais rápidas, além de permitir o uso de aceleradores como GPUs e TPUs. . from jax import jit a = 1 + jax.random.normal(k1, (2024, 2024)) b = 1 + jax.random.normal(k2, (2024, 2024)) . A vantagem se torna maior (&gt; 20x mais rápido) quando usamos aceleradores. . @jit #equivalente a definir jcos e escrever jcos = jit(jcos) def jcos(a, b): return jnp.dot(a, b)/jnp.sqrt(jnp.dot(a, a)*jnp.dot(b, b)) def npcos(a, b): return np.dot(a, b)/np.sqrt(np.dot(a, a)*np.dot(b, b)) . %%timeit npcos(a, b) . 241 ms ± 60.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . jcos(a, b) #Rodamos uma vez fora para compilar a função . DeviceArray([[1.0336248 , 1.028297 , 1.0310295 , ..., 0.96507126, 1.0339284 , 1.0405782 ], [0.94755113, 1.0175968 , 0.9993737 , ..., 1.0073111 , 1.0127649 , 1.0145004 ], [0.99184996, 1.0617024 , 1.0004048 , ..., 1.0069526 , 1.0510893 , 1.0679886 ], ..., [0.965744 , 1.0246744 , 1.0708025 , ..., 1.0135127 , 1.0477784 , 0.98690724], [0.9184314 , 0.99969995, 0.9819234 , ..., 0.9972953 , 0.9442775 , 0.9897808 ], [0.99618256, 1.0751995 , 1.0236498 , ..., 1.0285234 , 1.0351353 , 1.0573723 ]], dtype=float32) . %%timeit jcos(a, b) . 220 ms ± 4.32 ms per loop (mean ± std. dev. of 7 runs, 100 loops each) . Vetoriza&#231;&#227;o Autom&#225;tica: vmap . Vmap é uma transformação muito interessante, usando ela é possível vetorizar automaticamente nossas funções, ou seja, em vez de ter que fazer uma função que lida com um batch de dados, podemos fazer uma função que recebe um único dado e depois usar a trasformação para ganhar a versão que lida com o batch. . a = np.array([1., 2., 3.]) b = np.array([1., 1., -1.]) c = np.array([[1., 2., 3.], [4., 5., 6.]]) @jax.vmap #Podemos usar as transformações como decoradores def f(x, y): return x/y + 1. print(f(a, b)) def prod(x, y): return x@y print(prod(a, b)) . [ 2. 3. -2.] 0.0 . try: prod(a, c) #a e c não têm dimensões compatíveis except Exception as e: print(e) . matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 3) . batch_prod = jax.vmap(prod, in_axes=(None, 0)) #vamos multiplica a por cada linha de c batch_prod(a, c) . DeviceArray([14., 32.], dtype=float32) . Nessa primeira parte vimos qual o propósito da biblioteca e suas principais funções, nos próximos posts vamos explorar como criar redes neurais com jax, suas bibliotecas experimentais, o ecossistema de bibliotecas escritas usando jax. .",
            "url": "https://mari-linhares.github.io/codando-deep-learning-jax/2021/01/21/intro_a_jax.html",
            "relUrl": "/2021/01/21/intro_a_jax.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Codando Deep Learning",
            "content": "Motiva&#231;&#227;o . Vários especialistas e pesquisadores experientes na área de Deep Learning apoiam a ideia de que a melhor forma de aprender os conceitos fundamentais de uma determinada área é na prática. Isto é, na medida que vamos sendo expostos a novos conceitos, implementá-los para de fato entendê-los. . De longe a melhor estratégia para aprender muito é reimplementar tudo... Quando eu leio algo me iludo achando que entendi, quando me forço a reimplementar sempre compreendo as coisas melhor. É meu jeito favorito de aprender. - Andrej Kaparthy, Diretor de IA da Tesla&gt; Este material é a lista mais extensiva de implementações de machine learning do zero - isso é uma mina de ouro para quem gosta de aprender como eu gosto (construindo as coisas do zero):https://t.co/LcoPAQBULv - Trask (@iamtrask), Líder da OpenMined, Pesquisador na DeepMind Neste material iremos utilizar a abordagem de ensinar conceitos de Deep Learning para programadoras(es) da maneira que as coisas fiquem mais claras pra você: mostrando como as coisas funcionam, programando do zero! . Sobre o curso . Aprender um conceito de maneira profunda (tun dun tisss) ainda é difícil e é uma tarefa que exige rigor e disciplina. . Os materiais disponíveis nesse site buscam apresentar conceitos básicos e avançados junto a implementações detalhadas e de fácil entendimento. Para tal não utilizaremos nenhum framework de Deep Learning apenas Python. O objetivo do curso não é transformar um programador(a) em um(a) &quot;Machine Learning enginneer&quot; mas sim um excelente material para quem quer entender o funcionamento destas técnicas nos mínimos detalhes. . Junto do código + explicação são apresentados sugestões de exercícios, blogs, artigos, vídeos. . Quer apenas materiais? Quer ir direto ao código? . Estrutura do curso . Atenção: o curso é totalmente open source e colaborativo, tem sugestões? Quer ajudar? Chega mais!! Crie um issue/PR no repositório! . . Extras: . Glossário | . . Pré-requisitos 0.1 Motivação: Introdução a Deep Learning 0.2 História das Redes Neurais 0.3 Introdução a Numpy . | Introdução à Redes Neurais 1.1. O que são ? 1.2. Do que se alimentam ? 1.3. Como aprendem ? 1.4. Funções de ativação 1.5. Minha primeira Rede Neural &lt;3 1.6. Algoritmos de otimização 1.7. Inicialização dos pesos 1.8. Desafio . | Introdução à Redes Neurais Convolucionais 2.1. O que são? 2.2. Convolução 2.3. Pooling 2.4. Batch Norm 2.5. Dropout 2.6. Implementação 2.7. Desafio . | Introdução à Redes Neurais Recorrentes 3.1. O que são? 3.2. RNN Simples 3.3. Vanish gradients 3.4. LSTM 3.5. GRU 3.6. Attention is all you need 3.7. Desafio . | Introdução a Modelos Generativos 4.1. O que são? 4.2. AutoEncoder e Denoising AutoEncoder 4.3. Variational AutoEncoder 4.4. GANs 4.5. Desafio . | Limpeza e tratamento de dados para Deep Learning 5.1. Mamãe, quero ser cientista de dados 5.2. Normalização 5.3. Limpando Imagem 5.4. Limpando Texto 5.5. Data Augmentation . | Introdução a Word2Vec 6.1 skipgram 6.2 nbow 6.3 hierarchical softmax 6.4 negative sampling . | .",
            "url": "https://mari-linhares.github.io/codando-deep-learning-jax/2021/01/21/index.html",
            "relUrl": "/2021/01/21/index.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "História das Redes Neurais",
            "content": "O come&#231;o de tudo . A história das redes neurais é curiosa, engraçada e triste ao mesmo tempo. Então, senta que lá vem história... . Certo dia, em 1958, um certo cientista nova-yorkino, chamado Frank Rosenblatt, inspirado pelo trabalho de seus colegas que estudavam sobre o cérebro humano, terminou de projetar um algoritmo que conseguia aprender sozinho a resolver problemas simples. Nascia ali, o Perceptron (que veremos mais adiante no curso). . A ideia inicial do Perceptron, na verdade, era ser uma máquina ao invés de um programa. E foi o que aconteceu. Utilizando o poderosíssimo IBM 704 que fazia 12 mil adições por segundo (só para você ter uma noção, um iPhone 7 faz 300 bilhões) os cientistas construíram uma máquina denominada Mark I Perceptron. . . A ideia do Mark I Perceptron era que, para uma dada imagem de entrada, a máquina deveria aprender sozinha a classificar a saída em 0 ou 1. . Mas... 1958? Nem imagens existiam naquela época! Então, como projetaram isso? . Os cientistas na época conectaram 400 foto-células a entrada do Mark I para simular uma imagem 20x20 (um pouco maior talvez do que as letras desse texto). E para permitir a calibração da máquina, cada entrada dessa era conectada a um potenciômetro que eram ajustados automaticamente por motores elétricos para mapear as entradas na saída final 0 ou 1. . The winter is coming... &#128532; . O Perceptron, na época, foi um sucesso! Afinal de contas, ele foi um dos primeiros algoritmos capazes de aprender sozinho. Além disso, já foi provado que o Perceptron tem garantia de sucesso quando as duas classes são linearmente separáveis. Porém, é aí que está o problema: o Perceptron nada mais é que um classificador binário linear. Ou seja, ele só funciona quando o seu problema é binário (2 classes) e seus dados podem ser separados por uma simples reta. Essas condições, no mundo real, são muito difíceis de acontecer. Só para exemplificar, como você separia os dados abaixo com apenas uma reta? . . Fonte da imagem . Pois é. O Perceptron também não consegue resolver esse simples problema (para quem não reparou, o gráfico acima representa a porta XOR). Na época, isso desanimou tanto os estudiosos da área de Inteligência Artificial, que pesquisas nessa área só foram retomadas de fato na década de 80. Tal período ficou conhecido como o inverno da Inteligência Artificial (AI Winter). . O salvador da p&#225;tria: Geoffrey Hinton . O maior problema do Perceptron era que ele era apenas um só. Intuitivamente, é fácil perceber que um neurônio só não faz uma rede neural (assim como uma andorinha só não faz verão). Pensando assim, muita gente tentou colocar um monte de Perceptrons conectados entre si para tentar resolver um problema. Porém, muitos do que fizeram isso se depararam com um problema: como propagar o erro da saída para as entradas? Em outras palavras, como fazer esse monte de Perceptrons aprenderem ao mesmo tempo sem que um atrapalhe o que o outro aprendeu?&quot;. . Pensando nisso, o famoso Geoffrey Hinton desenvolveu o algoritmo backpropagation na década de 80. Utilizando o conceito de gradientes e regra da cadeia, tal algoritmo pega o erro de uma rede neural e propaga-o até as entradas, fazendo leves ajustes nos parâmetros (pesos) da rede. Com isso, Redes Neurais com mais de um neurônio e mais de uma camada poderiam começar a ser desenvolvidas e treinadas em problemas mais complexos. Como eu disse, poderiam... . O problema era que na década de 80, e mesmo na década de 90, o treinamento de tais redes e aplicação do backpropagation era muito pesado ainda. Mesmo supercomputadores se matavam para treinar e executar tais redes ainda. Então, o que fazer? . Obrigado, gamers! . Mais uma vez o mundo foi salvo graças aos gamers. Isso mesmo. Essa obsessão dos gamers em sempre querer computadores mais potentes e jogos com gráficos cada vez melhores, fez com que a indústria dos computadores, especialmente a das GPUs se desenvolvessem num ritmo assustador - regido pela Lei de Murphy. Mas, o que os jogos têm a ver com o desenvolvimento das Redes Neurais? . A resposta é: matrizes! Como vamos ver num dos próximos assuntos, Redes Neurais tem tudo a ver com matrizes. Basicamente, Redes Neurais fazem um monte de cálculo sobre matrizes, como: soma, multiplicão, operaçõe ponto-a-ponto, etc... E, como GPUs são computadores especializados em cálculos sobre matrizes, o campo das Redes Neurais pode se desenvolver como nunca. Cada vez mais, redes mais complexas e pesadas puderam ser desenvolvidas e treinadas. . . Agora que já tivemos uma introdução sobre a história das Redes Neurais, chegou a hora de aprendermos mais sobre elas. Então, prepara um café que chegou a hora de começar os estudos... . Refer&#234;ncias . Este conteúdo é baseado nos seguintes materiais: . Capítulo 3 de Grokking Deep Learning. | Perceptron da Wikipedia | Frank Rosenblatt da Wikipedia | Geoffrey Hinton da Wikipedia | Backpropagation da Wikipedia | .",
            "url": "https://mari-linhares.github.io/codando-deep-learning-jax/2021/01/21/historia_das_redes_neurais.html",
            "relUrl": "/2021/01/21/historia_das_redes_neurais.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Glossário",
            "content": ". Acur&#225;cia . Quanto porcento da base de dados avaliamos corretamente. . Exemplo: . Digamos que temos 3 exemplos na nossa base de dados, onde esperamos os seguintes valores de saída para cada exemplo [0, 1, 1]. . Temos: . acurácia = 1, apenas se acertamos tudo: [0, 1, 1]. acurácia = 0.66, se comparamos esses valores com [1, 1, 1] (acertamos as duas últimas saídas). acurácia = 0.33, se comparamos esses valores com [1, 0, 1] (acertamos apenas a última saída). acurácia = 0, se errarmos todas as saídas: [1, 0, 0]. . &lt;a id=&#39;atributo&#39;&gt;&lt;/a&gt; . File &#34;&lt;ipython-input-1-f8315d3082fd&gt;&#34;, line 1 &lt;a id=&#39;atributo&#39;&gt;&lt;/a&gt; ^ SyntaxError: invalid syntax . Atributo . Um aspecto que distíngue um elemento específico de outros elementos em um dado contexto. . Exemplo 1: . Digamos que você quer comprar um carro, neste contexto alguns dos atributos do carro seriam: cor, tamanho, preço, número de portas, se tem ou não ar-condicionado, etc. . Exemplo 2: . Digamos que você perdeu seu carro em um estacionamento, neste contexto alguns dos atributos relevantes do carro seriam: cor, tamanho, se tem ou não um adesivo escrito &quot;MENGÃO&quot;, placa, etc. . . Base de dados Iris . A base de dados Iris foi publicada originalmente no UCI Machine Learning Repository e é bastante utilizada em tutoriais por sua simplicidade. . É uma pequena base contendo 150 exemplos de plantas de 3 diferentes espécies (setosa, versicolour e virginica). É bastante utilizada para a tarefa de classificação onde nos baseando nos atributos tentamos classificar um exemplo como uma das espécies: setosa, versicolour ou virginica. . Atributos: comprimento da sépala (centímetros) | largura da sépala (centímetros) | comprimento da petala (centímetros) | largura da pétala (centímetros) | | . Classes: Iris Setosa | Iris Versicolour | Iris Virginica | | Imagem de: https://www.datacamp.com/community/tutorials/machine-learning-in-r .",
            "url": "https://mari-linhares.github.io/codando-deep-learning-jax/2021/01/21/glossario.html",
            "relUrl": "/2021/01/21/glossario.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "1.7 Inicialização dos Pesos",
            "content": "Bem vind@s ao mundo estoc&#225;stico! . Vimos que redes neurais são um formadas por um conjunto de parâmetros chamados de pesos e bias. . Não confunda parâmetros com hiperparâmetros. Parâmetros são os valores que sua rede aprende (pesos e bias). Hiperparâmetros são os valores relacionados ao treinamento da sua rede:learning rate, número de camadas/neurônios, função de ativação de cada camada, etc... Nós também vimos que os pesos e bias são, em geral, inicializados com valores aleatórios. Por conta disso, é muito díficil que duas redes distintas chegem no mesmo resultado. Além disso, nós nunca teremos certeza que tal resultado é o melhor possível (mínimo global) e, dependendo dos valores iniciais, uma rede pode nunca convergir. . Claro que, para evitar alguns desses problemas, nós podemos mexer nos hiperparâmetros como learning rate, o otimizador, adicionar momentum, entre outros. Se você não conhece sobre eles ainda, não se preocupe! Nós vamos falar sobre eles mais a frente nesse curso. Outra alternativa é inicializar os pesos de diferentes maneiras. E aí, vamos aprender sobre elas? . Inicializa&#231;&#227;o com Zeros . &quot;Eu prefiro lutar em 0 graus, pois não é nem quente, nem frio!&quot; Maguila (ex-lutador de boxe) . Baseado nesse pensamento, muitas pessoas já pensaram em inicializar os pesos com zero. Dessa forma, todos os pesos começarão com valores iguais, nenhum sendo mais importante que o outro, nem dando mais importância para um determinado atributo dos seus dados. Além disso, é muito simples fazer isso com numpy: . import numpy as np pesos = np.zeros(shape=(3, 3)) pesos . array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]) . Parece uma boa ideia, né? **Errado!** Essa é pior coisa que se pode fazer ao inicializar os pesos de uma rede neural! Parando para pensar um pouco melhor, fazendo isso nós vamos zerar todos os neurônios bem como os gradientes de aprendizagem. Se os gradientes são zeros, como a rede vai aprender alguma coisa, então? Vale salientar que seus gradientes podem tender a zero por outras razões (que nós veremos depois). Mas, quando isso acontece, chamamos esse fenômeno de desaparecimento dos gradientes (gradient vanishing). . Sem contar que você será mal visto pela sua família e amigos se inicializar seus pesos com zeros. Em outras palavras, não pense como o Maguila e não faça isso nunca! . Inicializa&#231;&#227;o com 1s . Nesse momento, você deve estar pensando: &quot;Tá bom, tá bom. Eu já entendi que inicializar todos os pesos com zero é ruim. Mas, e se inicializamos todos os pesos com 1s, então?&quot; . A resposta é: é tão ruim quanto! Pensando um pouquinho, lembra que na primeira camada todos os neurônios recebem as mesmas entradas? Então, se todos os neurônios receberam as mesmas entradas e todos eles tem o mesmo valor de peso (=1), o que que vai acontecer? Isso mesmo: todos os neurônios vão dar o mesmo resultado! . Ainda nessa linha de pensamento, como todos os neurônios vão da próxima camada vão receber o resultado de cada neurônio dessa camada (que são todos iguais), a mesma coisa vai se repetir. No fim das contas, você pode até ter um pouquinho de gradiente, mas os gradientes para os neurônios de uma mesma camada vão ser iguais. Assim, seus neurônios vão continuar sem aprender muita coisa. . Resumindo até aqui:não inicialize os seus neurônios com o mesmo valor, pior ainda se esse valor for igual a zero! Se você entendeu isso, deve ter percebido que a melhor maneira de inicializar pesos é dando valores aleatórios para cada um deles. A pergunta agora é: será que é melhor uma variância baixa, alta? Será que a média deve ser zero? É melhor uma distribuição normal ou uniforme? . É isso que vamos ver nas próximas seções. . Inicializa&#231;&#227;o aleat&#243;ria uniforme . Inicializar os pesos aleatórios de forma uniforme (mesma probabilidade para todo mundo) é fácil com numpy: . import matplotlib.pyplot as plt %matplotlib inline pesos_uniforme = 10*np.random.rand(1000) - 5 # pesos entre -5 e 5 plt.hist(pesos_uniforme) plt.show() . Repare que como os valores dos pesos têm praticamente uma mesma probabilidade de ocorrência (distribuição uniforme). Pesos como essa distribuição são bons para quebrar a simetria da rede e vão fazer sua rede aprender alguma coisa. . Inicializa&#231;&#227;o aleat&#243;ria normal . pesos_normal = 5*np.random.randn(1000) + 3 # média = 3 e desvio-padrão = 5 plt.hist(pesos_normal, bins=40) plt.show() . Diferente da distribuição uniforme, os pesos agora são inicializados por uma distribuição normal, isto é, com certa média e desvio padrão. Ou seja, pesos mais próximos da média são mais comuns e quanto mais &quot;desvios-padrões&quot; além da média, menor a probabilidade. Da mesma forma que a distribuição uniforme, pesos com essa distribuição também são bons para quebrar a simetria da rede. . Inicializa&#231;&#227;o Glorot Uniforme . A gente viu que inicilizar os pesos com uma distribuição normal ou uniforme é uma boa ideia. Entretanto, elas apresentam um defeito: como definir a faixa de valores que meus pesos vão assumir? Em outras palavras, qual deve ser a variância dos meus pesos? Quanto menor sua variância, maiores são as chances de acontecer o vanishing dos gradientes - e a gente já viu que isso não é bom. Por outro lado, quando a variância é muito grande, o contrário acontece. Isto é, seus gradientes vão tender ao infinito. Nesse caso, damos o nome de explosão dos gradientes (exploding gradients). Portanto, tenha em mente o seguinte: . O que buscamos na inicialização de pesos são valores pequenos com variância ok (pequena, mas nem tanto) . Porém, não existe um número mágico para variância dos pesos. Ela vai depender principalmente da quantidade de pesos que a sua rede tem, ou seja, depende da arquitetura da sua rede. O ideal seria que a variância dos pesos fosse definida automaticamente. Ahhh, ia ser tão bom se existisse um método que fizesse isso para gente, né? E existe!!! . A Glorot Uniforme, também conhecida como Xavier Uniforme, é uma distribuição uniforme que leva em consideração a quantidade de neurônios da camada atual e anterior da sua rede. Na prática, os pesos são inicializados baseados na seguinte fórmula:$$w = 2* sigma*rand() - sigma$$ onde, . $$ sigma = sqrt{ frac{6}{in + out}}$$ . Supondo que a camada atual tem 5 neurônios e a camada anterior tinha 3, a implementação em Python ficaria dessa forma: . np.random.seed(42) # para que o seu resultado seja igual ao meu :) sigma = np.sqrt(6 / (5 + 3)) pesos = 2 * sigma * np.random.rand(5, 3) - sigma print(pesos) . [[-0.21730289 0.78066008 0.40182529] [ 0.17088151 -0.59579319 -0.59583497] [-0.76542164 0.63423569 0.17513634] [ 0.36039228 -0.83037201 0.81390774] [ 0.57580754 -0.49824328 -0.55109532]] . Inicializa&#231;&#227;o Glorot Normal . A Glorot Normal, também conhecida como Xavier Normal, é bem parecida com a sua irmã uniforme. A diferença, na verdade, é que agora vamos fazer a amostragem dos pesos baseado numa distribuição normal (não diga!), também levando em consideração a quantidade de neurônios da camada atual e anterior: . $$w = sigma * randN()$$ . também teremos uma pequena diferençazinha no $ sigma$: . $$ sigma = sqrt{ frac{2}{in + out}}$$ . Em Python, considerando a mesma rede com 5 e 3 neurônios: . np.random.seed(42) # mesma semente do exemplo anterior para gente comparar os pesos sigma = np.sqrt(2 / (5 + 3)) pesos = sigma * np.random.randn(5, 3) print(pesos) . [[ 0.24835708 -0.06913215 0.32384427] [ 0.76151493 -0.11707669 -0.11706848] [ 0.78960641 0.38371736 -0.23473719] [ 0.27128002 -0.23170885 -0.23286488] [ 0.12098114 -0.95664012 -0.86245892]] . Pronto! Agora você sabe tudo sobre inicialização de pesos em redes neurais. Provavelmente, a sua única dúvida na cabeça agora é: da onde vem o $2$ e $6$ nas fórmulas do $ sigma$ das distribuições Glorot? A verdade é que, no artigo original, os autores testaram diversos valores em diversas redes e simplesmente esses valores levaram aos melhores resultados. É, meus amigos e amigas, esse é o mundo da redes neurais: tentativa e erro! .",
            "url": "https://mari-linhares.github.io/codando-deep-learning-jax/2021/01/21/1.7.inicializacao_dos_pesos.html",
            "relUrl": "/2021/01/21/1.7.inicializacao_dos_pesos.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "1.4 Funções de ativação",
            "content": "Depend&#234;ncias . import matplotlib.pyplot as plt # Matemática + manipulação de vetores import math import numpy as np # &quot;Fixar&quot; números aleatórios gerados np.random.seed(0) # Trabalhar com os dados import pandas as pd from sklearn.datasets import fetch_mldata from sklearn.model_selection import train_test_split # Utilidades import utils # Recarregar automaticamente dependências caso elas mudem %load_ext autoreload %autoreload 2 # Ignorar warnings import warnings warnings.filterwarnings(&#39;ignore&#39;) np.seterr(all=&#39;raise&#39;) . {&#39;divide&#39;: &#39;warn&#39;, &#39;over&#39;: &#39;warn&#39;, &#39;under&#39;: &#39;ignore&#39;, &#39;invalid&#39;: &#39;warn&#39;} . x = np.linspace(-10, 10) . def plot_line(ax, x, y, *args, **kwargs): ax.plot(x, y, *args, **kwargs) def plot_activation_vis(f, x, name): fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4.5), tight_layout=True) # função de activação y = f(x) ax1.set_title(&#39;Funcao de ativacao: &#39; + name) plot_line(ax1, x, y) # derivada dy = f(x, derivative=True) ax2.set_title(&#39;Derivada da Funcao de ativacao: &#39; + name) plot_line(ax2, x, dy) plt.show() . Sem ativa&#231;&#227;o (Linear) . def linear(x, derivative=False): if derivative: return np.ones(len(x)) return x plot_activation_vis(linear, x, &#39;linear&#39;) . ReLU . def relu(x, derivative=False): if derivative: return np.where(relu(x) &gt; 0, 1, 0) return np.where(x &gt; 0, x, 0) plot_activation_vis(relu, x, &#39;relu&#39;) . SeLU . def selu(x, alpha=3, derivative=False): if derivative: return np.where(selu(x) &gt; 0, 1, alpha * np.exp(x)) return np.where(x &gt; 0, x, alpha * np.exp(x) - alpha) plot_activation_vis(selu, x, &#39;SeLU&#39;) . Sigmoid . def sigmoid(x, alpha=3, derivative=False): if derivative: return sigmoid(x) * (1 - sigmoid(x)) return 1 / (1 + np.e ** (-x)) plot_activation_vis(sigmoid, x, &#39;Sigmoid&#39;) . Softmax . Tanh . Imagem de: https://medium.com/machine-learning-world/how-to-debug-neural-networks-manual-dc2a200f10f2 .",
            "url": "https://mari-linhares.github.io/codando-deep-learning-jax/2021/01/21/1.4.funcoes_de_ativacao.html",
            "relUrl": "/2021/01/21/1.4.funcoes_de_ativacao.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "1.1 Introdução a Redes Neurais",
            "content": "",
            "url": "https://mari-linhares.github.io/codando-deep-learning-jax/2021/01/21/1.1.o_que_sao.html",
            "relUrl": "/2021/01/21/1.1.o_que_sao.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://mari-linhares.github.io/codando-deep-learning-jax/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://mari-linhares.github.io/codando-deep-learning-jax/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mari-linhares.github.io/codando-deep-learning-jax/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}